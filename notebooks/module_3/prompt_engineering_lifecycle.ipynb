{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering Lifecycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can set them inline\n",
    "import os\n",
    "os.environ[\"MISTRAL_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy-mistral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or you can use a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log a trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The speed of light in a vacuum, denoted as **c**, is a fundamental constant of nature approximately equal to **299,792,458 meters per second** (≈ 3 × 10⁸ m/s).\\n\\nIn physics, it represents the **maximum speed at which all energy, matter, and information can travel** in the universe, as described by Einstein’s theory of relativity.\\nIt also serves as a **cosmic speed limit**, linking space and time in the spacetime continuum.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from app import langsmith_rag\n",
    "\n",
    "question = \"What iis the meaning of speed of light as physics major\"\n",
    "langsmith_rag(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dataset to evaluate this particular step of our application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example_ids': ['1f1818a5-5b0e-43df-8b14-d13f9a13ba43',\n",
       "  '7fe712f6-d049-4cbc-b217-3a9c1148edb9',\n",
       "  'a7e1c594-18d2-4be8-a7d9-2fc9f7f22d08'],\n",
       " 'count': 3}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "example_dataset = [\n",
    "    (\n",
    "        \"What is the speed of light in a vacuum?\",\n",
    "        \"\"\"The speed of light in a vacuum is a fundamental physical constant denoted by c. It is exactly 299,792,458 meters per second (approximately 3.00 × 10^8 m/s). This speed represents the maximum speed at which all energy, matter, and physical information in the universe can travel. According to Einstein's special theory of relativity, nothing with mass can reach or exceed this speed.\"\"\",\n",
    "        \"The speed of light in a vacuum is exactly 299,792,458 meters per second (approximately 3.00 × 10^8 m/s), which is a fundamental physical constant denoted by c.\"\n",
    "    ),\n",
    "    (\n",
    "        \"What is Newton's second law of motion?\",\n",
    "        \"\"\"Newton's second law of motion states that the acceleration of an object is directly proportional to the net force acting on it and inversely proportional to its mass. Mathematically, this is expressed as F = ma, where F is the net force, m is the mass, and a is the acceleration. This law explains how the velocity of an object changes when it is subjected to an external force.\"\"\",\n",
    "        \"Newton's second law states that F = ma, where the force (F) equals mass (m) times acceleration (a). This means acceleration is directly proportional to force and inversely proportional to mass.\"\n",
    "    ),\n",
    "    (\n",
    "        \"What is quantum entanglement?\",\n",
    "        \"\"\"Quantum entanglement is a phenomenon in quantum physics where two or more particles become interconnected in such a way that the quantum state of each particle cannot be described independently. When particles are entangled, measuring the state of one particle instantly affects the state of the other, regardless of the distance separating them. Einstein famously called this 'spooky action at a distance' and it forms the basis for quantum computing and quantum communication technologies.\"\"\",\n",
    "        \"Quantum entanglement is a quantum physics phenomenon where particles become interconnected so that measuring one instantly affects the other, regardless of distance. Einstein called this 'spooky action at a distance'.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"Physics Questions\"\n",
    "\n",
    "# Create dataset\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name, description=\"Physics questions and concepts\"\n",
    ")\n",
    "\n",
    "# Prepare inputs and outputs\n",
    "inputs = [{\"question\": q, \"context\": c} for q, c, _ in example_dataset]\n",
    "outputs = [{\"output\": o} for _, _, o in example_dataset]\n",
    "\n",
    "# Create examples in the dataset\n",
    "client.create_examples(\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update our Application to use Prompt Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to pretty much define the same RAG application as before - with one crucial improvement.\n",
    "\n",
    "Instead of pulling our `RAG_PROMPT` from utils.py, we're going to connect to the Prompt Hub in LangSmith.\n",
    "\n",
    "Let's add the code snippet that will pull down our prompt that we just iterated on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://smith.langchain.com/prompts/rag-prompt/70ece6d7?organizationId=bd531ccf-4286-4467-99ba-7eab707122af'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langsmith import Client\n",
    "\n",
    "client=Client()\n",
    "\n",
    "prompt = \"\"\"You are a polymath who knows everything about different subjects .\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "template = ChatPromptTemplate.from_template(prompt)\n",
    "client.push_prompt(\"rag-prompt\", object=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pulled prompt from Prompt Hub:\n",
      "input_variables=['context', 'question'] input_types={} partial_variables={} metadata={'lc_hub_owner': '-', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '70ece6d7877d2fdac5a8e2143d8baf05bce53f1e9b5fe50f63d48cbf47f63d99'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='You are a polymath who knows everything about different subjects .\\n\\nContext: {context}\\nQuestion: {question}\\nAnswer:'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# Pull a prompt from the LangSmith Prompt Hub\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# Physics-oriented RAG prompt for answering physics questions\n",
    "prompt = client.pull_prompt(\"rag-prompt\")\n",
    "\n",
    "print(\"Successfully pulled prompt from Prompt Hub:\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /mistralai/Mixtral-8x7B-v0.1/resolve/main/tokenizer.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7f0849782cf0>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 880b5998-c521-4509-a290-6a87725d967e)')' thrown while requesting HEAD https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/resolve/main/tokenizer.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /mistralai/Mixtral-8x7B-v0.1/resolve/main/tokenizer.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7f083347d450>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 18dbe88e-af72-4031-a7bf-efd3ee3555cb)')' thrown while requesting HEAD https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/resolve/main/tokenizer.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /mistralai/Mixtral-8x7B-v0.1/resolve/main/tokenizer.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7f083347d950>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 91c02eb2-67ac-4707-bc01-db65ea868759)')' thrown while requesting HEAD https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/resolve/main/tokenizer.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "Fetching pages: 100%|##########| 197/197 [01:01<00:00,  3.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nlangsmith_rag\\n- Calls `retrieve_documents` to fetch documents\\n- Calls `generate_response` to generate a response based on the fetched documents\\n- Returns the model response\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders.sitemap import SitemapLoader\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_mistralai import ChatMistralAI, MistralAIEmbeddings\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langsmith import traceable\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "\n",
    "MODEL_NAME = \"mistral-large-latest\"\n",
    "MODEL_PROVIDER = \"mistral\"\n",
    "APP_VERSION = 1.0\n",
    "\n",
    "# Physics-oriented system prompt\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are a physics expert assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer physics questions accurately. \n",
    "Provide clear explanations of physical concepts and include relevant equations when appropriate.\n",
    "If you don't know the answer, just say that you don't know. \n",
    "Keep your answers concise but informative.\n",
    "\"\"\"\n",
    "\n",
    "mistral_client = ChatMistralAI(model=MODEL_NAME)\n",
    "\n",
    "def get_vector_db_retriever():\n",
    "    persist_path = os.path.join(tempfile.gettempdir(), \"physics_docs.parquet\")\n",
    "    embd = MistralAIEmbeddings(model=\"mistral-embed\")\n",
    "\n",
    "    # If vector store exists, then load it\n",
    "    if os.path.exists(persist_path):\n",
    "        vectorstore = SKLearnVectorStore(\n",
    "            embedding=embd,\n",
    "            persist_path=persist_path,\n",
    "            serializer=\"parquet\"\n",
    "        )\n",
    "        return vectorstore.as_retriever(lambda_mult=0)\n",
    "\n",
    "    # Otherwise, index LangSmith documents and create new vector store\n",
    "    ls_docs_sitemap_loader = SitemapLoader(web_path=\"https://docs.smith.langchain.com/sitemap.xml\", continue_on_failure=True)\n",
    "    ls_docs = ls_docs_sitemap_loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=500, chunk_overlap=0\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(ls_docs)\n",
    "\n",
    "    vectorstore = SKLearnVectorStore.from_documents(\n",
    "        documents=doc_splits,\n",
    "        embedding=embd,\n",
    "        persist_path=persist_path,\n",
    "        serializer=\"parquet\"\n",
    "    )\n",
    "    vectorstore.persist()\n",
    "    return vectorstore.as_retriever(lambda_mult=0)\n",
    "\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "\"\"\"\n",
    "retrieve_documents\n",
    "- Returns documents fetched from a vectorstore based on the user's question\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "\"\"\"\n",
    "generate_response\n",
    "- Calls `call_mistral` to generate a physics-oriented model response using Prompt Hub\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    \n",
    "    # Use the physics prompt pulled from Prompt Hub\n",
    "    try:\n",
    "        formatted_prompt = prompt.invoke({\"context\": formatted_docs, \"question\": question})\n",
    "        \n",
    "        # Convert to LangChain message format\n",
    "        messages = []\n",
    "        if isinstance(formatted_prompt, dict) and \"messages\" in formatted_prompt:\n",
    "            for msg in formatted_prompt[\"messages\"]:\n",
    "                if msg[\"role\"] == \"system\":\n",
    "                    messages.append(SystemMessage(content=msg[\"content\"]))\n",
    "                elif msg[\"role\"] == \"user\":\n",
    "                    messages.append(HumanMessage(content=msg[\"content\"]))\n",
    "                elif msg[\"role\"] == \"assistant\":\n",
    "                    messages.append(AIMessage(content=msg[\"content\"]))\n",
    "        else:\n",
    "            # Fallback to physics-oriented manual formatting\n",
    "            messages = [\n",
    "                SystemMessage(content=RAG_SYSTEM_PROMPT),\n",
    "                HumanMessage(content=f\"Context: {formatted_docs}\\n\\nPhysics Question: {question}\")\n",
    "            ]\n",
    "    except Exception as e:\n",
    "        print(f\"Error using Prompt Hub: {e}\")\n",
    "        # Fallback to physics-oriented manual formatting\n",
    "        messages = [\n",
    "            SystemMessage(content=RAG_SYSTEM_PROMPT),\n",
    "            HumanMessage(content=f\"Context: {formatted_docs}\\n\\nPhysics Question: {question}\")\n",
    "        ]\n",
    "    \n",
    "    return call_mistral(messages)\n",
    "\n",
    "\"\"\"\n",
    "call_mistral\n",
    "- Returns the chat completion output from Mistral AI\n",
    "\"\"\"\n",
    "@traceable(\n",
    "    run_type=\"llm\",\n",
    "    metadata={\n",
    "        \"ls_provider\": MODEL_PROVIDER,\n",
    "        \"ls_model_name\": MODEL_NAME\n",
    "    }\n",
    ")\n",
    "def call_mistral(messages: List[dict]) -> str:\n",
    "    return mistral_client.invoke(messages)\n",
    "\n",
    "\"\"\"\n",
    "langsmith_rag\n",
    "- Calls `retrieve_documents` to fetch documents\n",
    "- Calls `generate_response` to generate a response based on the fetched documents\n",
    "- Returns the model response\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"According to Einstein's theory of relativity, energy and mass are related by the equation **E=mc²**, where **E** is energy, **m** is mass, and **c** is the speed of light. This means mass can be converted into energy and vice versa. The equation shows that a small amount of mass can produce an enormous amount of energy.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is the relationship between energy and mass according to Einstein?\"\n",
    "langsmith_rag(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro-to-langsmith (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
