{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairwise Experiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can set them inline\n",
    "import os\n",
    "os.environ[\"MISTRAL_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy-mistral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or you can use a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up a new task! Here, we have a salesperson named Bob. Bob has a lot of deals, so he wants to summarize what happened in his deals based off of some meeting transcripts.\n",
    "\n",
    "Bob is iterating on a few different prompts using Mistral AI, that will give him nice, concise transcripts for his deals.\n",
    "\n",
    "Bob has curated a dataset of his deal transcripts, let's go ahead and load that in. You can take a look at the dataset as well if you're curious! Note that this is not a golden dataset, there is no reference output here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloned dataset: Meeting Transcripts\n",
      "Dataset contains 5 examples\n",
      "Dataset contains 5 examples\n"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "dataset = client.clone_public_dataset(\n",
    "  \"https://smith.langchain.com/public/9078d2f1-7bef-4ba7-b795-210a17682ef9/d\"\n",
    ")\n",
    "print(f\"Cloned dataset: {dataset.name}\")\n",
    "print(f\"Dataset contains {len(list(client.list_examples(dataset_name=dataset.name)))} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run some experiments on this dataset using two different prompts. Let's add an evaluator that tries to score how good our summaries are!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "import json\n",
    "\n",
    "mistral_client = ChatMistralAI(model=\"mistral-small-latest\", temperature=0.1)\n",
    "\n",
    "SUMMARIZATION_SYSTEM_PROMPT = \"\"\"You are a judge, aiming to score how well a summary summarizes the content of a transcript. \n",
    "Respond with a JSON object containing 'score' (integer 1-5) and 'reasoning' (string explanation).\"\"\"\n",
    "\n",
    "SUMMARIZATION_HUMAN_PROMPT = \"\"\"\n",
    "[The Meeting Transcript] {transcript}\n",
    "[The Start of Summarization] {summary} [The End of Summarization]\n",
    "\n",
    "Please evaluate this summary on a scale of 1-5, where 1 is a bad summary and 5 is a great summary.\n",
    "Consider completeness, accuracy, and conciseness.\"\"\"\n",
    "\n",
    "class SummarizationScore(BaseModel):\n",
    "    score: int = Field(description=\"\"\"A score from 1-5 ranking how good the summarization is for the provided transcript, with 1 being a bad summary, and 5 being a great summary\"\"\")\n",
    "    reasoning: str = Field(description=\"Brief explanation for the score\")\n",
    "    \n",
    "def summary_score_evaluator(inputs: dict, outputs: dict) -> list:\n",
    "    messages = [\n",
    "        SystemMessage(content=SUMMARIZATION_SYSTEM_PROMPT),\n",
    "        HumanMessage(content=SUMMARIZATION_HUMAN_PROMPT.format(\n",
    "            transcript=inputs[\"transcript\"],\n",
    "            summary=outputs.get(\"output\", \"N/A\")\n",
    "        ))\n",
    "    ]\n",
    "    \n",
    "    response = mistral_client.invoke(messages)\n",
    "    \n",
    "    try:\n",
    "        # Parse JSON response from Mistral AI\n",
    "        result = json.loads(response.content)\n",
    "        summary_score = result.get(\"score\", 3)\n",
    "        reasoning = result.get(\"reasoning\", \"No reasoning provided\")\n",
    "        \n",
    "        return {\n",
    "            \"key\": \"summary_score\", \n",
    "            \"score\": summary_score,\n",
    "            \"comment\": reasoning\n",
    "        }\n",
    "    except json.JSONDecodeError:\n",
    "        # Fallback if JSON parsing fails\n",
    "        return {\n",
    "            \"key\": \"summary_score\", \n",
    "            \"score\": 3,\n",
    "            \"comment\": \"Could not parse Mistral AI response\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll run our experiment with a good version of our prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Good Mistral Summarizer-64321797' at:\n",
      "https://smith.langchain.com/o/bd531ccf-4286-4467-99ba-7eab707122af/datasets/fe28ab41-9df8-47e9-a675-858ad8feeeb1/compare?selectedSessions=a01ab955-7b6d-4a2d-955b-60e18ba30270\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5973dee7b8cb4d598fa1aa6feec57b5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.transcript</th>\n",
       "      <th>outputs.output</th>\n",
       "      <th>error</th>\n",
       "      <th>feedback.summary_score</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bob and Mr. Carter (CLOSED DEAL): Bob: Welcome...</td>\n",
       "      <td>Bob met with Mr. Carter to discuss trading in ...</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>0.805242</td>\n",
       "      <td>33286ea7-9126-476e-8ea4-b36c7928fce8</td>\n",
       "      <td>177671d0-236d-458f-9abe-423d4b8715be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob and Mr. Patel (CLOSED DEAL): Bob: Hello, M...</td>\n",
       "      <td>Mr. Patel sought a midsize hybrid sedan for hi...</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>0.774518</td>\n",
       "      <td>75ce83fc-ac58-4335-9a6d-6c3aac4aa990</td>\n",
       "      <td>27342517-3a01-41b0-b3fd-58fa69a841fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bob and Ms. Thompson (NO DEAL): Bob: Hi, Ms. T...</td>\n",
       "      <td>Bob welcomed Ms. Thompson to Ford Motors, disc...</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>0.971985</td>\n",
       "      <td>80f0443e-2fbf-4a54-9bd5-e2969fd32e40</td>\n",
       "      <td>a73aaee5-4296-4847-a7bd-b3237677f66a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bob and Ms. Nguyen (NO DEAL): Bob: Good aftern...</td>\n",
       "      <td>1. **Discussion**: Bob presented Ms. Nguyen wi...</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>1.358458</td>\n",
       "      <td>c199cb17-647e-4582-95a3-24dfe092124f</td>\n",
       "      <td>19d17428-f50b-4b08-a8cd-6c3a376e07bc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bob and Mr. Johnson (CLOSED DEAL): Bob: Good m...</td>\n",
       "      <td>Bob met with Mr. Johnson to discuss purchasing...</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>0.794056</td>\n",
       "      <td>e9730901-3533-4c8a-baa5-0ee5c867990d</td>\n",
       "      <td>cfd871c8-2cb2-456e-bff0-325777e105a4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults Good Mistral Summarizer-64321797>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt One: Good Prompt with Mistral AI!\n",
    "def good_summarizer(inputs: dict):\n",
    "    messages = [\n",
    "        HumanMessage(content=f\"Concisely summarize this meeting in 3 sentences. Make sure to include all of the important events, key decisions, and action items. Meeting: {inputs['transcript']}\")\n",
    "    ]\n",
    "    \n",
    "    response = mistral_client.invoke(messages)\n",
    "    return response.content\n",
    "\n",
    "client.evaluate(\n",
    "    good_summarizer,\n",
    "    data=dataset,\n",
    "    evaluators=[summary_score_evaluator],\n",
    "    experiment_prefix=\"Good Mistral Summarizer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll run an experiment with a worse version of our prompt, to highlight the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Bad Mistral Summarizer-068a34eb' at:\n",
      "https://smith.langchain.com/o/bd531ccf-4286-4467-99ba-7eab707122af/datasets/fe28ab41-9df8-47e9-a675-858ad8feeeb1/compare?selectedSessions=d3eeb829-b77f-4985-8f52-73cb03c86d49\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "655b1e558b1e45afb8bc6886b4ad5938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.transcript</th>\n",
       "      <th>outputs.output</th>\n",
       "      <th>error</th>\n",
       "      <th>feedback.summary_score</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bob and Mr. Carter (CLOSED DEAL): Bob: Welcome...</td>\n",
       "      <td>Bob successfully helped Mr. Carter trade in hi...</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>0.587522</td>\n",
       "      <td>33286ea7-9126-476e-8ea4-b36c7928fce8</td>\n",
       "      <td>57bd76ae-ea0d-43d7-8e19-82f44dc0efb7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob and Mr. Patel (CLOSED DEAL): Bob: Hello, M...</td>\n",
       "      <td>Bob successfully helped Mr. Patel purchase a F...</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>0.732520</td>\n",
       "      <td>75ce83fc-ac58-4335-9a6d-6c3aac4aa990</td>\n",
       "      <td>020e4a99-693c-4be8-a4d6-c134be8f1af8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bob and Ms. Thompson (NO DEAL): Bob: Hi, Ms. T...</td>\n",
       "      <td>Bob and Ms. Thompson discussed SUV options, wi...</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>0.544668</td>\n",
       "      <td>80f0443e-2fbf-4a54-9bd5-e2969fd32e40</td>\n",
       "      <td>14c58278-34be-46f5-9146-238671876d50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bob and Ms. Nguyen (NO DEAL): Bob: Good aftern...</td>\n",
       "      <td>Bob and Ms. Nguyen discussed various car optio...</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>0.538156</td>\n",
       "      <td>c199cb17-647e-4582-95a3-24dfe092124f</td>\n",
       "      <td>91695ced-f95e-43fe-a854-714ac126d4a3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bob and Mr. Johnson (CLOSED DEAL): Bob: Good m...</td>\n",
       "      <td>Bob successfully convinced Mr. Johnson to purc...</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>0.612687</td>\n",
       "      <td>e9730901-3533-4c8a-baa5-0ee5c867990d</td>\n",
       "      <td>765b7068-e34d-428d-9a45-efee8c22cb70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults Bad Mistral Summarizer-068a34eb>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt Two: Worse Prompt with Mistral AI!\n",
    "def bad_summarizer(inputs: dict):\n",
    "    messages = [\n",
    "        HumanMessage(content=f\"Summarize this in one sentence. {inputs['transcript']}\")\n",
    "    ]\n",
    "    \n",
    "    response = mistral_client.invoke(messages)\n",
    "    return response.content\n",
    "\n",
    "client.evaluate(\n",
    "    bad_summarizer,\n",
    "    data=dataset,\n",
    "    evaluators=[summary_score_evaluator],\n",
    "    experiment_prefix=\"Bad Mistral Summarizer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function that will compare our two experiments. These are the fields that pairwise evaluator functions get access to:\n",
    "- `inputs: dict`: A dictionary of the inputs corresponding to a single example in a dataset.\n",
    "- `outputs: list[dict]`: A list of the dict outputs produced by each experiment on the given inputs.\n",
    "- `reference_outputs: dict`: A dictionary of the reference outputs associated with the example, if available.\n",
    "- `runs: list[Run]`: A list of the full Run objects generated by the experiments on the given example. Use this if you need access to intermediate steps or metadata about each run.\n",
    "- `example: Example`: The full dataset Example, including the example inputs, outputs (if available), and metdata (if available)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's give our LLM-as-Judge some instructions. In our case, we're just going to directly use LLM-as-judge to grade which of the summarizers is the most helpful.\n",
    "\n",
    "It might be hard to grade our summarizers without a ground truth reference, but here, comparing different prompts head to head will give us a sense of which is better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGE_SYSTEM_PROMPT = \"\"\"\n",
    "Please act as an impartial judge and evaluate the quality of the summarizations provided by two AI summarizers to the meeting transcript below.\n",
    "Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their summarizations. \n",
    "Begin your evaluation by comparing the two summarizations and provide a short explanation. \n",
    "Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. \n",
    "Do not favor certain names of the assistants. \n",
    "Be as objective as possible.\n",
    "Respond with a JSON object containing 'preference' (integer: 1 for Assistant A, 2 for Assistant B, 0 for tie) and 'reasoning' (string explanation).\"\"\"\n",
    "\n",
    "JUDGE_HUMAN_PROMPT = \"\"\"\n",
    "[The Meeting Transcript] {transcript}\n",
    "\n",
    "[The Start of Assistant A's Summarization] {answer_a} [The End of Assistant A's Summarization]\n",
    "\n",
    "[The Start of Assistant B's Summarization] {answer_b} [The End of Assistant B's Summarization]\n",
    "\n",
    "Which summarization is better? Consider completeness, accuracy, conciseness, and usefulness for a business context.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our function will take in an `inputs` dictionary, and a list of `outputs` dictionaries for the different experiments that we want to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "import json\n",
    "\n",
    "class Preference(BaseModel):\n",
    "    preference: int = Field(description=\"\"\"1 if Assistant A answer is better based upon the factors above.\n",
    "2 if Assistant B answer is better based upon the factors above.\n",
    "Output 0 if it is a tie.\"\"\")\n",
    "    reasoning: str = Field(description=\"Brief explanation for the preference choice\")\n",
    "    \n",
    "def ranked_preference(inputs: dict, outputs: list[dict]) -> list:\n",
    "    messages = [\n",
    "        SystemMessage(content=JUDGE_SYSTEM_PROMPT),\n",
    "        HumanMessage(content=JUDGE_HUMAN_PROMPT.format(\n",
    "            transcript=inputs[\"transcript\"],\n",
    "            answer_a=outputs[0].get(\"output\", \"N/A\"),\n",
    "            answer_b=outputs[1].get(\"output\", \"N/A\")\n",
    "        ))\n",
    "    ]\n",
    "    \n",
    "    response = mistral_client.invoke(messages)\n",
    "    \n",
    "    try:\n",
    "        # Parse JSON response from Mistral AI\n",
    "        result = json.loads(response.content)\n",
    "        preference_score = result.get(\"preference\", 0)\n",
    "        reasoning = result.get(\"reasoning\", \"No reasoning provided\")\n",
    "        \n",
    "        if preference_score == 1:\n",
    "            scores = [1, 0]\n",
    "        elif preference_score == 2:\n",
    "            scores = [0, 1]\n",
    "        else:\n",
    "            scores = [0, 0]\n",
    "            \n",
    "        return scores\n",
    "        \n",
    "    except json.JSONDecodeError:\n",
    "        # Fallback if JSON parsing fails - return tie\n",
    "        return [0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run our pairwise experiment with `evaluate()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the pairwise evaluation results at:\n",
      "https://smith.langchain.com/o/bd531ccf-4286-4467-99ba-7eab707122af/datasets/fe28ab41-9df8-47e9-a675-858ad8feeeb1/compare?selectedSessions=4dc84cae-b5e5-4886-a959-0f398eab84f1%2Cfafaa715-c983-417e-910d-1a5bd804a745&comparativeExperiment=f969df8a-15c0-4759-9569-9569ddcee74b\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f4ffc707514973adcceca5a4c54ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<langsmith.evaluation._runner.ComparativeExperimentResults at 0x789460b841a0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import evaluate\n",
    "\n",
    "# Run pairwise experiment comparing the two Mistral AI summarizers\n",
    "# Use the full experiment names with their generated suffixes\n",
    "evaluate(\n",
    "    (\"Good Mistral Summarizer-54e57cbf\", \"Bad Mistral Summarizer-70151d73\"),  # Use actual experiment names from LangSmith\n",
    "    evaluators=[ranked_preference],\n",
    "    experiment_prefix=\"Mistral Pairwise Comparison\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learnings\n",
    "\n",
    "### Key Changes Made:\n",
    "1. **Model Provider**: Migrated from OpenAI to Mistral AI using `ChatMistralAI`\n",
    "2. **Environment Variables**: Changed from `OPENAI_API_KEY` to `MISTRAL_API_KEY`\n",
    "3. **Message Format**: Updated to use LangChain message objects (SystemMessage, HumanMessage)\n",
    "4. **Response Parsing**: Implemented JSON parsing for Mistral AI responses instead of OpenAI's structured output\n",
    "5. **Project Name**: Updated to `langsmith-academy-mistral`\n",
    "6. **Experiment Names**: Updated to `Good Mistral Summarizer` and `Bad Mistral Summarizer`\n",
    "\n",
    "### Custom Tweakings:\n",
    "1. **Enhanced Prompts**: Improved summarization prompts to include key decisions and action items\n",
    "2. **JSON Response Handling**: Added robust JSON parsing with fallback mechanisms for both evaluator and preference functions\n",
    "3. **Reasoning Addition**: Extended both evaluators to include reasoning explanations alongside scores\n",
    "4. **Business Context**: Enhanced judge prompts to consider business-specific usefulness criteria\n",
    "5. **Dataset Info**: Added logging to show dataset cloning information and example count\n",
    "6. **Temperature Control**: Set temperature to 0.1 for more consistent evaluation results\n",
    "7. **Error Resilience**: Implemented fallback responses for parsing failures\n",
    "\n",
    "### Pairwise Comparison Enhancements:\n",
    "1. **Enhanced Judge Instructions**: Improved judge prompts to consider business context and completeness\n",
    "2. **Reasoning Capture**: Extended preference evaluation to capture reasoning for decisions\n",
    "3. **Robust Scoring**: Implemented fallback mechanisms to handle JSON parsing errors gracefully\n",
    "4. **Clear Experiment Naming**: Used descriptive names for better experiment tracking\n",
    "\n",
    "### What I Learned:\n",
    "1. **Pairwise Evaluation Migration**: Successfully adapted pairwise comparison systems from OpenAI to Mistral AI\n",
    "2. **JSON Response Handling**: Learned to handle structured responses from Mistral AI using custom JSON parsing\n",
    "3. **Multi-Stage Evaluation**: Understood how to chain individual evaluators with pairwise comparisons\n",
    "4. **Business Application**: Applied AI evaluation to real-world business scenarios (sales meeting summarization)\n",
    "5. **Robust Error Handling**: Implemented comprehensive error handling for production-ready evaluation systems\n",
    "\n",
    "\n",
    "### Evaluation Methodology Learned:\n",
    "1. **Individual vs Pairwise**: Understanding when to use individual scoring vs comparative evaluation\n",
    "2. **Reference-Free Evaluation**: Techniques for evaluating quality without ground truth\n",
    "3. **Multi-Criteria Assessment**: Considering multiple factors (completeness, accuracy, conciseness, business value)\n",
    "4. **Consistency Testing**: Using temperature control and error handling for reliable results\n",
    "5. **Experiment Tracking**: Proper naming and metadata for experiment management\n",
    "\n",
    "This migration demonstrates how to adapt complex evaluation workflows including pairwise comparisons when changing language model providers while maintaining evaluation quality and adding robustness improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mat496",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
