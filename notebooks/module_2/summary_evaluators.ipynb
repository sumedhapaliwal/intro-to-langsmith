{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Evaluators \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can set them inline\n",
    "import os\n",
    "os.environ[\"MISTRAL_API_KEY\"] = \"MISTRAL_API_KEY\" \n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"LANGSMITH_API_KEY\"\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy-mistral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or you can use a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our task here is to analyze the toxicity of random statements using Mistral AI, classifying them as `Toxic` or `Not toxic`. \n",
    "\n",
    "Take a look at our dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloned toxicity dataset: Toxicity Analysis\n",
      "Dataset contains 9 examples\n",
      "Example classes: {'Not toxic', 'Toxic'}\n",
      "Dataset contains 9 examples\n",
      "Example classes: {'Not toxic', 'Toxic'}\n"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "dataset = client.clone_public_dataset(\n",
    "    \"https://smith.langchain.com/public/89ef0d44-a252-4011-8bb8-6a114afc1522/d\"\n",
    ")\n",
    "\n",
    "print(f\"Cloned toxicity dataset: {dataset.name}\")\n",
    "examples = list(client.list_examples(dataset_name=dataset.name))\n",
    "print(f\"Dataset contains {len(examples)} examples\")\n",
    "print(f\"Example classes: {set(ex.outputs.get('class', 'Unknown') for ex in examples[:5])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple toxicity classifier using Mistral AI!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from pydantic import BaseModel, Field\n",
    "import json\n",
    "\n",
    "mistral_client = ChatMistralAI(model=\"mistral-small-latest\", temperature=0.1)\n",
    "\n",
    "class Toxicity(BaseModel):\n",
    "    toxicity: str = Field(description=\"\"\"'Toxic' if this the statement is toxic, 'Not toxic' if the statement is not toxic.\"\"\")\n",
    "    confidence: float = Field(description=\"Confidence score between 0 and 1\")\n",
    "\n",
    "def good_classifier(inputs: dict) -> dict:\n",
    "    system_prompt = \"\"\"You are a toxicity classifier. Analyze the given statement and classify it as either 'Toxic' or 'Not toxic'. \n",
    "    Consider factors like hate speech, harassment, threats, profanity, and harmful content.\n",
    "    Respond with a JSON object containing 'toxicity' (string) and 'confidence' (float 0-1).\"\"\"\n",
    "    \n",
    "    user_prompt = f\"Classify this statement: {inputs['statement']}\"\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=user_prompt)\n",
    "    ]\n",
    "    \n",
    "    response = mistral_client.invoke(messages)\n",
    "    \n",
    "    try:\n",
    "        # Parse JSON response from Mistral AI\n",
    "        result = json.loads(response.content)\n",
    "        toxicity_score = result.get(\"toxicity\", \"Not toxic\")\n",
    "        confidence = result.get(\"confidence\", 0.5)\n",
    "        \n",
    "        return {\n",
    "            \"class\": toxicity_score,\n",
    "            \"confidence\": confidence\n",
    "        }\n",
    "    except json.JSONDecodeError:\n",
    "        # Fallback if JSON parsing fails\n",
    "        content = response.content.lower()\n",
    "        if \"toxic\" in content and \"not toxic\" not in content:\n",
    "            return {\"class\": \"Toxic\", \"confidence\": 0.3}\n",
    "        else:\n",
    "            return {\"class\": \"Not toxic\", \"confidence\": 0.3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the fields that summary evaluator functions get access to:\n",
    "- `inputs: list[dict]`: A list of inputs from the examples in our dataset\n",
    "- `outputs: list[dict]`: A list of the dict outputs produced from running our target over each input\n",
    "- `reference_outputs: list[dict]`: A list of reference_outputs from the examples in our dataset\n",
    "- `runs: list[Run]`: A list of the Run objects from running our target over the dataset.\n",
    "- `examples: list[Example]`: A list of the full dataset Examples, including the example inputs, outputs (if available), and metdata (if available)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define our summary evaluator! Here, we'll compute the f1-score, which is a combination of precision and recall.\n",
    "\n",
    "This sort of metric can only be computed over all of the examples in our experiment, so our evaluator takes in a list of outputs, and a list of reference_outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_summary_evaluator(outputs: list[dict], reference_outputs: list[dict]) -> dict:\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    true_negatives = 0\n",
    "    \n",
    "    for output_dict, reference_output_dict in zip(outputs, reference_outputs):\n",
    "        output = output_dict.get(\"class\", \"Not toxic\")\n",
    "        reference_output = reference_output_dict.get(\"class\", \"Not toxic\")\n",
    "        \n",
    "        if output == \"Toxic\" and reference_output == \"Toxic\":\n",
    "            true_positives += 1\n",
    "        elif output == \"Toxic\" and reference_output == \"Not toxic\":\n",
    "            false_positives += 1\n",
    "        elif output == \"Not toxic\" and reference_output == \"Toxic\":\n",
    "            false_negatives += 1\n",
    "        elif output == \"Not toxic\" and reference_output == \"Not toxic\":\n",
    "            true_negatives += 1\n",
    "\n",
    "    # Calculate metrics\n",
    "    total = len(outputs)\n",
    "    accuracy = (true_positives + true_negatives) / total if total > 0 else 0.0\n",
    "    \n",
    "    if true_positives == 0:\n",
    "        precision = 0.0\n",
    "        recall = 0.0\n",
    "        f1_score = 0.0\n",
    "    else:\n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0.0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0.0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        \"key\": \"f1_score\", \n",
    "        \"score\": f1_score,\n",
    "        \"metadata\": {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"true_positives\": true_positives,\n",
    "            \"false_positives\": false_positives,\n",
    "            \"false_negatives\": false_negatives,\n",
    "            \"true_negatives\": true_negatives,\n",
    "            \"total_examples\": total\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Additional custom summary evaluator for confidence analysis\n",
    "def confidence_summary_evaluator(outputs: list[dict], reference_outputs: list[dict]) -> dict:\n",
    "    confidences = [output.get(\"confidence\", 0.5) for output in outputs]\n",
    "    \n",
    "    if not confidences:\n",
    "        return {\"key\": \"avg_confidence\", \"score\": 0.0}\n",
    "    \n",
    "    avg_confidence = sum(confidences) / len(confidences)\n",
    "    min_confidence = min(confidences)\n",
    "    max_confidence = max(confidences)\n",
    "    \n",
    "    return {\n",
    "        \"key\": \"confidence_analysis\",\n",
    "        \"score\": avg_confidence,\n",
    "        \"metadata\": {\n",
    "            \"average_confidence\": avg_confidence,\n",
    "            \"min_confidence\": min_confidence,\n",
    "            \"max_confidence\": max_confidence,\n",
    "            \"low_confidence_count\": sum(1 for c in confidences if c < 0.6)\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we pass in `f1_score_summary_evaluator` as a summary evaluator!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Mistral Toxicity Classifier-1dcca71b' at:\n",
      "https://smith.langchain.com/o/bd531ccf-4286-4467-99ba-7eab707122af/datasets/91cd1cee-078a-4a28-9775-4359cb132cce/compare?selectedSessions=2b775f19-1c5c-44e8-a3cf-2828bfe8c144\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a927c1f2234e06978952ddd5fb2021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation completed!\n",
      "Experiment results: <ExperimentResults Mistral Toxicity Classifier-1dcca71b>\n"
     ]
    }
   ],
   "source": [
    "results = client.evaluate(\n",
    "    good_classifier,\n",
    "    data=dataset,\n",
    "    summary_evaluators=[f1_score_summary_evaluator, confidence_summary_evaluator],\n",
    "    experiment_prefix=\"Mistral Toxicity Classifier\",\n",
    "    metadata={\n",
    "        \"model\": \"mistral-small-latest\",\n",
    "        \"task\": \"toxicity_classification\",\n",
    "        \"provider\": \"mistral\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Evaluation completed!\")\n",
    "print(f\"Experiment results: {results}\")\n",
    "\n",
    "# Display summary metrics if available\n",
    "if hasattr(results, 'get_summary_scores'):\n",
    "    summary_scores = results.get_summary_scores()\n",
    "    print(\"\\nSummary Metrics:\")\n",
    "    for score in summary_scores:\n",
    "        print(f\"  {score.key}: {score.score}\")\n",
    "        if hasattr(score, 'metadata') and score.metadata:\n",
    "            print(f\"    Details: {score.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Custom Tweakings:\n",
    "1. **Enhanced Classifier**: Added confidence scoring alongside toxicity classification\n",
    "2. **Robust JSON Parsing**: Implemented JSON parsing with intelligent fallback mechanisms\n",
    "3. **Additional Summary Evaluator**: Created `confidence_summary_evaluator` to analyze model confidence\n",
    "4. **Enhanced F1 Evaluator**: Extended to include precision, recall, accuracy, and confusion matrix details\n",
    "5. **Dataset Logging**: Added information display for dataset cloning and example analysis\n",
    "6. **Metadata Enhancement**: Added comprehensive experiment metadata including model and task information\n",
    "7. **Temperature Control**: Set temperature to 0.1 for more consistent classification results\n",
    "8. **Error Resilience**: Implemented fallback classification based on content analysis\n",
    "\n",
    "### Summary Evaluation Enhancements:\n",
    "1. **Multi-Metric Analysis**: Combined F1 score with confidence analysis for comprehensive evaluation\n",
    "2. **Confusion Matrix Details**: Extended F1 evaluator to provide full confusion matrix breakdown\n",
    "3. **Confidence Insights**: Added analysis of model confidence patterns and low-confidence predictions\n",
    "4. **Result Visualization**: Enhanced result display with detailed metric breakdown\n",
    "\n",
    "### What I Learned:\n",
    "1. **Summary Evaluators**: Understood how to create evaluators that analyze aggregate results across entire datasets\n",
    "2. **Classification Metrics**: Learned to implement comprehensive classification evaluation including F1, precision, recall, and accuracy\n",
    "3. **Confidence Analysis**: Discovered how to analyze model confidence patterns for reliability assessment\n",
    "4. **JSON Response Handling**: Mastered handling structured responses from Mistral AI with robust error handling\n",
    "5. **Multi-Evaluator Systems**: Learned to combine multiple summary evaluators for comprehensive analysis\n",
    "\n",
    "\n",
    "### Classification Task Methodology:\n",
    "1. **Structured Prompting**: Using clear instructions for consistent classification behavior\n",
    "2. **Multi-Output Classification**: Combining classification with confidence scoring\n",
    "3. **Robust Error Handling**: Implementing content-based fallbacks for parsing failures\n",
    "4. **Comprehensive Evaluation**: Using multiple summary evaluators for thorough analysis\n",
    "5. **Result Interpretation**: Understanding how to interpret and visualize classification metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mat496",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
