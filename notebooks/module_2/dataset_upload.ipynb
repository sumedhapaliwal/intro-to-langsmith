{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Upload \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Dataset Management with LangSmith SDK\n",
    "\n",
    "In addition to creating and editing Datasets in the LangSmith UI, you can also create and edit datasets with the LangSmith SDK.\n",
    "\n",
    "This version has been updated to use Mistral AI instead of OpenAI for better performance and cost-effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Upload Process\n",
    "\n",
    "Let's go ahead and upload a list of examples that we have from our RAG application to LangSmith as a new dataset.\n",
    "\n",
    "The examples have been updated to reflect Mistral AI usage instead of OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Enhanced environment configuration loaded\n",
      " Model Provider: mistral\n",
      "✓ Embedding Provider: huggingface\n",
      " Dataset analytics system initialized\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Environment Configuration for Mistral AI and LangSmith\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "\n",
    "# Set environment variables for Mistral AI and LangSmith integration\n",
    "os.environ[\"MISTRAL_API_KEY\"] = \"MISTRAL_API_KEY\"  # Replace with your Mistral AI API key\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"LANGSMITH_API_KEY\"  # Your LangSmith API key\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy-mistral\"\n",
    "\n",
    "# Enhanced configuration settings\n",
    "DATASET_CONFIG = {\n",
    "    \"model_provider\": \"mistral\",\n",
    "    \"model_name\": \"mistral-small-latest\",\n",
    "    \"embedding_provider\": \"huggingface\",\n",
    "    \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"batch_size\": 10,\n",
    "    \"validation_enabled\": True,\n",
    "    \"analytics_enabled\": True\n",
    "}\n",
    "\n",
    "print(\" Enhanced environment configuration loaded\")\n",
    "print(f\" Model Provider: {DATASET_CONFIG['model_provider']}\")\n",
    "print(f\"✓ Embedding Provider: {DATASET_CONFIG['embedding_provider']}\")\n",
    "\n",
    "# Dataset Analytics Class\n",
    "class DatasetAnalytics:\n",
    "    def __init__(self):\n",
    "        self.operations = []\n",
    "        self.start_time = datetime.now()\n",
    "    \n",
    "    def log_operation(self, operation_type: str, success: bool, details: Dict = None):\n",
    "        entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"operation_type\": operation_type,\n",
    "            \"success\": success,\n",
    "            \"details\": details or {}\n",
    "        }\n",
    "        self.operations.append(entry)\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        if not self.operations:\n",
    "            return {\"message\": \"No operations recorded yet\"}\n",
    "        \n",
    "        successful_ops = sum(1 for op in self.operations if op[\"success\"])\n",
    "        total_ops = len(self.operations)\n",
    "        \n",
    "        return {\n",
    "            \"total_operations\": total_ops,\n",
    "            \"successful_operations\": successful_ops,\n",
    "            \"success_rate\": round(successful_ops / total_ops, 3) if total_ops > 0 else 0,\n",
    "            \"session_duration\": str(datetime.now() - self.start_time)\n",
    "        }\n",
    "\n",
    "# Initialize analytics\n",
    "dataset_analytics = DatasetAnalytics()\n",
    "print(\" Dataset analytics system initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or you can use a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset 'Mistral AI RAG Questions and Answers'...\n",
      "Created dataset 'Mistral AI RAG Questions and Answers' with ID: e7c57d4d-2f87-474a-a1d3-44424e1e661f\n",
      "Created dataset 'Mistral AI RAG Questions and Answers' with ID: e7c57d4d-2f87-474a-a1d3-44424e1e661f\n",
      "Successfully created 10 examples in dataset 'Mistral AI RAG Questions and Answers'\n",
      "\n",
      "=== Dataset Upload Analytics ===\n",
      "Total Operations: 2\n",
      "Successful Operations: 2\n",
      "Success Rate: 1.0\n",
      "Session Duration: 0:00:02.447809\n",
      "Successfully created 10 examples in dataset 'Mistral AI RAG Questions and Answers'\n",
      "\n",
      "=== Dataset Upload Analytics ===\n",
      "Total Operations: 2\n",
      "Successful Operations: 2\n",
      "Success Rate: 1.0\n",
      "Session Duration: 0:00:02.447809\n"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "# Custom examples for Mistral AI with LangSmith\n",
    "example_inputs = [\n",
    "(\"How do I configure Mistral AI with LangChain for my RAG application?\", \"To configure Mistral AI with LangChain, install langchain-mistralai and use ChatMistralAI class. Set your MISTRAL_API_KEY environment variable and initialize the model with your desired parameters like temperature and max_tokens. The integration works seamlessly with LangChain's standard interfaces.\"),\n",
    "(\"What are the advantages of using Mistral AI over other language models?\", \"Mistral AI offers strong performance with efficient computation, multilingual capabilities, and competitive pricing. The models are designed for both cloud and on-premise deployment, with good instruction following and reasoning capabilities. They also provide fine-tuning options for specialized use cases.\"),\n",
    "(\"How can I trace my Mistral AI application calls in LangSmith?\", \"Enable tracing by setting LANGSMITH_TRACING=true and LANGSMITH_API_KEY in your environment. Use the @traceable decorator on functions that call Mistral AI models. LangSmith will automatically capture inputs, outputs, token usage, and performance metrics for analysis.\"),\n",
    "(\"What evaluation strategies work best for Mistral AI applications?\", \"Effective evaluation strategies include automated metrics like BLEU or ROUGE for text generation, human evaluation for quality assessment, and A/B testing for comparing model variants. LangSmith provides tools to create evaluation datasets and run systematic comparisons.\"),\n",
    "(\"How do I optimize token usage when working with Mistral AI models?\", \"Optimize token usage by crafting concise prompts, using appropriate context lengths, implementing prompt templates, and choosing the right model size for your task. Monitor token consumption through LangSmith to identify optimization opportunities.\"),\n",
    "(\"Can I use Mistral AI for multilingual applications?\", \"Yes, Mistral AI models have strong multilingual capabilities supporting major European languages and more. They can handle translation, multilingual question answering, and cross-lingual tasks effectively while maintaining good performance across languages.\"),\n",
    "(\"How do I handle errors and implement retry logic with Mistral AI?\", \"Implement robust error handling by catching API exceptions, using exponential backoff for retries, and setting appropriate timeouts. Consider implement fallback strategies and monitoring error rates through LangSmith for production applications.\"),\n",
    "(\"What are the best practices for prompt engineering with Mistral AI?\", \"Best practices include being specific and clear in instructions, providing examples when needed, using consistent formatting, and iterating based on output quality. Mistral AI responds well to structured prompts and explicit instructions.\"),\n",
    "(\"How can I fine-tune Mistral AI models for my specific domain?\", \"Fine-tuning involves preparing domain-specific training data, choosing appropriate hyperparameters, and using Mistral AI's fine-tuning API. Start with a base model, prepare high-quality examples, and evaluate performance on validation sets.\"),\n",
    "(\"What monitoring and analytics should I track for Mistral AI applications?\", \"Track key metrics including response time, token usage, error rates, user satisfaction, and model performance. LangSmith provides comprehensive monitoring tools to analyze these metrics and identify areas for improvement in your Mistral AI applications.\"),\n",
    "]\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"Mistral AI RAG Questions and Answers\"\n",
    "\n",
    "# First, create the dataset if it doesn't exist\n",
    "try:\n",
    "    # Try to get the dataset to see if it exists\n",
    "    dataset = client.read_dataset(dataset_name=dataset_name)\n",
    "    print(f\"Dataset '{dataset_name}' already exists with {dataset.example_count} examples\")\n",
    "    dataset_analytics.log_operation(\"dataset_read\", True, {\"name\": dataset_name})\n",
    "except Exception:\n",
    "    # Dataset doesn't exist, let's create it\n",
    "    print(f\"Creating dataset '{dataset_name}'...\")\n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        description=\"Custom dataset for testing Mistral AI RAG applications with LangSmith integration. Contains questions about Mistral AI configuration, optimization, multilingual capabilities, and best practices.\"\n",
    "    )\n",
    "    print(f\"Created dataset '{dataset_name}' with ID: {dataset.id}\")\n",
    "    dataset_analytics.log_operation(\"dataset_create\", True, {\"name\": dataset_name, \"id\": str(dataset.id)})\n",
    "\n",
    "# Prepare inputs and outputs for bulk creation\n",
    "inputs = [{\"question\": input_prompt} for input_prompt, _ in example_inputs]\n",
    "outputs = [{\"output\": output_answer} for _, output_answer in example_inputs]\n",
    "\n",
    "# Create examples using dataset_name instead of dataset_id\n",
    "try:\n",
    "    examples_created = client.create_examples(\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        dataset_name=dataset_name,\n",
    "    )\n",
    "    print(f\"Successfully created {len(example_inputs)} examples in dataset '{dataset_name}'\")\n",
    "    dataset_analytics.log_operation(\"examples_create\", True, {\"count\": len(example_inputs)})\n",
    "    \n",
    "    # Display analytics\n",
    "    print(\"\\n=== Dataset Upload Analytics ===\")\n",
    "    stats = dataset_analytics.get_stats()\n",
    "    for key, value in stats.items():\n",
    "        print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error creating examples: {e}\")\n",
    "    dataset_analytics.log_operation(\"examples_create\", False, {\"error\": str(e)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting another Trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration with Enhanced RAG Application\n",
    "\n",
    "I've moved our RAG application definition to `app.py` so we can quickly import it. The app has been updated to use Mistral AI instead of OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector store from LangSmith documentation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 197/197 [00:34<00:00,  5.73it/s]\n",
      "Fetching pages: 100%|##########| 197/197 [00:34<00:00,  5.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 197 documents from LangSmith documentation\n",
      "Split documents into 3810 chunks\n",
      "Vector store created and saved to /tmp/langsmith_docs.parquet\n",
      "Vector store created and saved to /tmp/langsmith_docs.parquet\n"
     ]
    }
   ],
   "source": [
    "from app import langsmith_rag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ask another question to create a new trace!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The provided context does not include specific information on configuring Mistral AI with LangChain for a RAG application. For detailed guidance, please refer to the LangChain documentation or relevant tutorials.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How do I configure Mistral AI with LangChain for my RAG application?\"\n",
    "langsmith_rag(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUMMARY\n",
    "\n",
    "### Key Changes I Made:\n",
    "1. **Environment Variables**: Changed from OPENAI_API_KEY to MISTRAL_API_KEY\n",
    "2. **Dataset Examples**: Created 10 new custom examples focused on Mistral AI usage, covering configuration, optimization, multilingual capabilities, and best practices\n",
    "3. **Integration**: Updated RAG application to use Mistral AI and HuggingFace embeddings\n",
    "4. **Project Name**: Updated to langsmith-academy-mistral\n",
    "\n",
    "### Custom Tweaks:\n",
    "- Added examples about multilingual capabilities specific to Mistral AI\n",
    "- Included token optimization strategies\n",
    "- Added fine-tuning and domain adaptation examples\n",
    "- Focused on practical implementation questions\n",
    "\n",
    "### Learning Outcomes:\n",
    "- Successfully migrated dataset creation from OpenAI to Mistral AI\n",
    "- Created custom examples showcasing Mistral AI advantages\n",
    "- Maintained LangSmith functionality with new model provider\n",
    "- Learned about dataset management with different language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro-to-langsmith (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
