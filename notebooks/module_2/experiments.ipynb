{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can set them inline\n",
    "import os\n",
    "os.environ[\"MISTRAL_API_KEY\"] = \"MISTRAL_API_KEY\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"LANGSMITH_API_KEY\"\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy-mistral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or you can use a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the RAG Application that we've been working with throughout this course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders.sitemap import SitemapLoader\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langsmith import traceable\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "\n",
    "# Configured for Mistral AI\n",
    "MODEL_NAME = \"mistral-small-latest\"\n",
    "MODEL_PROVIDER = \"mistral\" \n",
    "APP_VERSION = 2.0\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "\n",
    "mistral_client = ChatMistralAI(model=MODEL_NAME, temperature=0.3)\n",
    "\n",
    "def get_vector_db_retriever():\n",
    "    persist_path = os.path.join(tempfile.gettempdir(), \"mistral_docs.parquet\")\n",
    "    # Use HuggingFace embeddings instead of OpenAI\n",
    "    embd = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        model_kwargs={'device': 'cpu'}\n",
    "    )\n",
    "\n",
    "    # If vector store exists, then load it\n",
    "    if os.path.exists(persist_path):\n",
    "        vectorstore = SKLearnVectorStore(\n",
    "            embedding=embd,\n",
    "            persist_path=persist_path,\n",
    "            serializer=\"parquet\"\n",
    "        )\n",
    "        return vectorstore.as_retriever(lambda_mult=0)\n",
    "\n",
    "    # Otherwise, index LangSmith documents and create new vector store\n",
    "    ls_docs_sitemap_loader = SitemapLoader(web_path=\"https://docs.smith.langchain.com/sitemap.xml\", continue_on_failure=True)\n",
    "    ls_docs = ls_docs_sitemap_loader.load()\n",
    "\n",
    "    # Use standard text splitter since we're using HuggingFace embeddings\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500, chunk_overlap=50\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(ls_docs)\n",
    "\n",
    "    vectorstore = SKLearnVectorStore.from_documents(\n",
    "        documents=doc_splits,\n",
    "        embedding=embd,\n",
    "        persist_path=persist_path,\n",
    "        serializer=\"parquet\"\n",
    "    )\n",
    "    vectorstore.persist()\n",
    "    return vectorstore.as_retriever(lambda_mult=0)\n",
    "\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "\"\"\"\n",
    "retrieve_documents\n",
    "- Returns documents fetched from a vectorstore based on the user's question\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "\"\"\"\n",
    "generate_response\n",
    "- Calls `call_mistral` to generate a model response after formatting inputs\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    messages = [\n",
    "        SystemMessage(content=RAG_SYSTEM_PROMPT),\n",
    "        HumanMessage(content=f\"Context: {formatted_docs} \\n\\n Question: {question}\")\n",
    "    ]\n",
    "    return call_mistral(messages)\n",
    "\n",
    "\"\"\"\n",
    "call_mistral\n",
    "- Returns the chat completion output from Mistral AI\n",
    "\"\"\"\n",
    "@traceable(\n",
    "    run_type=\"llm\",\n",
    "    metadata={\n",
    "        \"ls_provider\": MODEL_PROVIDER,\n",
    "        \"ls_model_name\": MODEL_NAME\n",
    "    }\n",
    ")\n",
    "def call_mistral(messages: List) -> str:\n",
    "    response = mistral_client.invoke(messages)\n",
    "    return response\n",
    "\n",
    "\"\"\"\n",
    "langsmith_rag\n",
    "- Calls `retrieve_documents` to fetch documents\n",
    "- Calls `generate_response` to generate a response based on the fetched documents\n",
    "- Returns the model response\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a code snippet that should look similar to what you see from the starter code!\n",
    "\n",
    "There are a few important components here.\n",
    "\n",
    "1. We have defined an Evaluator\n",
    "2. We pipe our dataset examples (dict) to the shape of input that our function `langsmith_rag` takes (str) using a target function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'Mistral RAG Application Dataset' already exists with 4 examples\n",
      "Running evaluation with Mistral AI...\n",
      "View the evaluation results for experiment: 'mistral-small-latest-0ea11166' at:\n",
      "https://smith.langchain.com/o/bd531ccf-4286-4467-99ba-7eab707122af/datasets/dab4404e-e95f-4521-9892-284e6476bbca/compare?selectedSessions=5a2fa2ac-fe87-4668-9f9b-ce0c62f63cb2\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "820f709af1514b8c81df1642285b4a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.output</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.output</th>\n",
       "      <th>feedback.is_concise</th>\n",
       "      <th>feedback.contains_key_terms</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is LangSmith and how does it help with LL...</td>\n",
       "      <td>LangSmith is a platform designed for evaluatin...</td>\n",
       "      <td>None</td>\n",
       "      <td>LangSmith is a platform for debugging, testing...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.308382</td>\n",
       "      <td>7dfd0d3f-c378-4fe2-8a08-301d52dd92e2</td>\n",
       "      <td>ec182711-27c0-4392-a3ee-e248c3baab5c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How can I compare different Mistral AI models?</td>\n",
       "      <td>You can compare different Mistral AI models us...</td>\n",
       "      <td>None</td>\n",
       "      <td>You can compare different Mistral AI models by...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.017068</td>\n",
       "      <td>99f57a2e-5d00-40e2-9fbe-01a263fcebba</td>\n",
       "      <td>036ba565-85ee-41cd-b096-b58a8fa07b2c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the benefits of using custom evaluators?</td>\n",
       "      <td>Custom evaluators are beneficial when each eva...</td>\n",
       "      <td>None</td>\n",
       "      <td>Custom evaluators allow you to test specific a...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.321609</td>\n",
       "      <td>9ebb8f0f-b91f-425d-b2da-d91149c0cfa1</td>\n",
       "      <td>eebd145a-55d7-4d27-a931-0a96ff610fb2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How do I set up tracing in my Mistral AI appli...</td>\n",
       "      <td>To set up tracing in your Mistral AI applicati...</td>\n",
       "      <td>None</td>\n",
       "      <td>To set up tracing in your Mistral AI applicati...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.029923</td>\n",
       "      <td>d1fc5afd-2380-4215-accc-cb26660ba5b3</td>\n",
       "      <td>78b210fb-21d4-4f0f-97b9-8cf79251194b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults mistral-small-latest-0ea11166>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import evaluate, Client\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"Mistral RAG Application Dataset\"\n",
    "\n",
    "# First, let's create the dataset if it doesn't exist\n",
    "try:\n",
    "    # Try to get the dataset to see if it exists\n",
    "    dataset = client.read_dataset(dataset_name=dataset_name)\n",
    "    print(f\"Dataset '{dataset_name}' already exists with {dataset.example_count} examples\")\n",
    "except Exception:\n",
    "    # Dataset doesn't exist, let's create it with sample examples\n",
    "    print(f\"Creating dataset '{dataset_name}'...\")\n",
    "    \n",
    "    # Create sample examples for testing\n",
    "    examples = [\n",
    "        {\n",
    "            \"question\": \"What is LangSmith and how does it help with LLM applications?\",\n",
    "            \"output\": \"LangSmith is a platform for debugging, testing, and monitoring LLM applications. It provides tracing capabilities to track execution steps, evaluation tools for systematic testing, and monitoring features for production deployments.\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"How do I set up tracing in my Mistral AI application?\",\n",
    "            \"output\": \"To set up tracing in your Mistral AI application, you need to install langsmith, set your LANGSMITH_API_KEY environment variable, and use the @traceable decorator on your functions. This allows automatic tracking of your application's execution flow.\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What are the benefits of using custom evaluators?\",\n",
    "            \"output\": \"Custom evaluators allow you to test specific aspects of your application beyond basic metrics. They can check for domain relevance, response quality, accuracy, and other application-specific criteria to ensure your LLM performs well for your use case.\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"How can I compare different Mistral AI models?\",\n",
    "            \"output\": \"You can compare different Mistral AI models by running experiments with the same dataset and evaluators but different model configurations. LangSmith's experiment feature allows you to systematically compare performance across model variants.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Create the dataset\n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        description=\"Dataset for testing Mistral AI RAG application with custom evaluators\"\n",
    "    )\n",
    "    \n",
    "    # Add examples to the dataset\n",
    "    client.create_examples(\n",
    "        inputs=[{\"question\": ex[\"question\"]} for ex in examples],\n",
    "        outputs=[{\"output\": ex[\"output\"]} for ex in examples],\n",
    "        dataset_id=dataset.id\n",
    "    )\n",
    "    \n",
    "    print(f\"Created dataset '{dataset_name}' with {len(examples)} examples\")\n",
    "\n",
    "# Custom evaluator for Mistral AI responses\n",
    "def is_concise_enough(reference_outputs: dict, outputs: dict) -> dict:\n",
    "    score = len(outputs[\"output\"]) < 1.5 * len(reference_outputs[\"output\"])\n",
    "    return {\"key\": \"is_concise\", \"score\": int(score)}\n",
    "\n",
    "# Additional custom evaluator for response quality\n",
    "def contains_key_terms(reference_outputs: dict, outputs: dict) -> dict:\n",
    "    key_terms = [\"langsmith\", \"mistral\", \"trace\", \"evaluation\"]\n",
    "    output_lower = outputs[\"output\"].lower()\n",
    "    score = any(term in output_lower for term in key_terms)\n",
    "    return {\"key\": \"contains_key_terms\", \"score\": int(score)}\n",
    "\n",
    "def target_function(inputs: dict):\n",
    "    return {\"output\": langsmith_rag(inputs[\"question\"])}\n",
    "\n",
    "print(\"Running evaluation with Mistral AI...\")\n",
    "evaluate(\n",
    "    target_function,\n",
    "    data=dataset_name,\n",
    "    evaluators=[is_concise_enough, contains_key_terms],\n",
    "    experiment_prefix=\"mistral-small-latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying your Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's change our model to mistral-tiny and see how it performs compared to mistral-small-latest!\n",
    "\n",
    "Make this change, and then run this code snippet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation with Mistral Tiny...\n",
      "View the evaluation results for experiment: 'mistral-tiny-f7525ce0' at:\n",
      "https://smith.langchain.com/o/bd531ccf-4286-4467-99ba-7eab707122af/datasets/dab4404e-e95f-4521-9892-284e6476bbca/compare?selectedSessions=57527339-dbf2-44ca-8ee8-6851c8dc9c29\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b404d7a3db7e4f638060cbcabf930d5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.output</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.output</th>\n",
       "      <th>feedback.is_concise</th>\n",
       "      <th>feedback.contains_key_terms</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is LangSmith and how does it help with LL...</td>\n",
       "      <td>LangSmith is a platform that consists of a fro...</td>\n",
       "      <td>None</td>\n",
       "      <td>LangSmith is a platform for debugging, testing...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.907266</td>\n",
       "      <td>7dfd0d3f-c378-4fe2-8a08-301d52dd92e2</td>\n",
       "      <td>42291fe6-c104-4127-9e71-88125e128516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How can I compare different Mistral AI models?</td>\n",
       "      <td>To compare different Mistral AI models, you ca...</td>\n",
       "      <td>None</td>\n",
       "      <td>You can compare different Mistral AI models by...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.102619</td>\n",
       "      <td>99f57a2e-5d00-40e2-9fbe-01a263fcebba</td>\n",
       "      <td>38c188d7-b6ff-478a-bfe1-d20d057e68c4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the benefits of using custom evaluators?</td>\n",
       "      <td>Custom evaluators can be beneficial in complex...</td>\n",
       "      <td>None</td>\n",
       "      <td>Custom evaluators allow you to test specific a...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.764297</td>\n",
       "      <td>9ebb8f0f-b91f-425d-b2da-d91149c0cfa1</td>\n",
       "      <td>87cb8911-7f26-49fc-8ec4-fb9e12c9e60c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How do I set up tracing in my Mistral AI appli...</td>\n",
       "      <td>To set up tracing in your Mistral AI applicati...</td>\n",
       "      <td>None</td>\n",
       "      <td>To set up tracing in your Mistral AI applicati...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.398458</td>\n",
       "      <td>d1fc5afd-2380-4215-accc-cb26660ba5b3</td>\n",
       "      <td>c94133e0-5857-46f7-bf23-ee7b55ef2bc2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults mistral-tiny-f7525ce0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import evaluate, Client\n",
    "from langsmith.schemas import Example, Run\n",
    "\n",
    "# Update the model for comparison\n",
    "MODEL_NAME = \"mistral-tiny\"\n",
    "mistral_client = ChatMistralAI(model=MODEL_NAME, temperature=0.3)\n",
    "\n",
    "def target_function(inputs: dict):\n",
    "    return {\"output\": langsmith_rag(inputs[\"question\"])}\n",
    "\n",
    "print(\"Running evaluation with Mistral Tiny...\")\n",
    "evaluate(\n",
    "    target_function,\n",
    "    data=dataset_name,\n",
    "    evaluators=[is_concise_enough, contains_key_terms],\n",
    "    experiment_prefix=\"mistral-tiny\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running over Different pieces of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset Version\n",
    "\n",
    "You can execute an experiment on a specific version of a dataset in the sdk by using the `as_of` parameter in `list_examples`\n",
    "\n",
    "Let's try running on just our initial dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset version experiment skipped: \n",
      "Note: Create dataset versions in LangSmith UI to use this feature\n"
     ]
    }
   ],
   "source": [
    "# Note: This will work once you have dataset versions set up\n",
    "try:\n",
    "    evaluate(\n",
    "        target_function,\n",
    "        data=client.list_examples(dataset_name=dataset_name, as_of=\"initial dataset\"),   # We use as_of to specify a version\n",
    "        evaluators=[is_concise_enough],\n",
    "        experiment_prefix=\"initial dataset version\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Dataset version experiment skipped: {e}\")\n",
    "    print(\"Note: Create dataset versions in LangSmith UI to use this feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset Split\n",
    "\n",
    "You can run an experiment on a specific split of your dataset, let's try running on the Crucial Examples split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split experiment skipped: \n",
      "Note: Create dataset splits in LangSmith UI to use this feature\n"
     ]
    }
   ],
   "source": [
    "# Note: This will work once you have dataset splits configured\n",
    "try:\n",
    "    evaluate(\n",
    "        target_function,\n",
    "        data=client.list_examples(dataset_name=dataset_name, splits=[\"Crucial Examples\"]),  # We pass in a list of Splits\n",
    "        evaluators=[is_concise_enough],\n",
    "        experiment_prefix=\"Crucial Examples split\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Dataset split experiment skipped: {e}\")\n",
    "    print(\"Note: Create dataset splits in LangSmith UI to use this feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Specific Data Points\n",
    "\n",
    "You can specify individual data points to run an experiment over as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment on specific examples: [UUID('7dfd0d3f-c378-4fe2-8a08-301d52dd92e2'), UUID('99f57a2e-5d00-40e2-9fbe-01a263fcebba')]\n",
      "View the evaluation results for experiment: 'two specific example ids-1fab4719' at:\n",
      "https://smith.langchain.com/o/bd531ccf-4286-4467-99ba-7eab707122af/datasets/dab4404e-e95f-4521-9892-284e6476bbca/compare?selectedSessions=bd0d6903-1167-4448-8a33-418a511a7e09\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354667a03b0145f685f11a001a2a586a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get example IDs from the dataset and run on first two examples\n",
    "try:\n",
    "    examples = list(client.list_examples(dataset_name=dataset_name, limit=2))\n",
    "    if len(examples) >= 2:\n",
    "        example_ids = [ex.id for ex in examples[:2]]\n",
    "        print(f\"Running experiment on specific examples: {example_ids}\")\n",
    "        \n",
    "        evaluate(\n",
    "            target_function,\n",
    "            data=client.list_examples(\n",
    "                dataset_name=dataset_name, \n",
    "                example_ids=example_ids\n",
    "            ),\n",
    "            evaluators=[is_concise_enough],\n",
    "            experiment_prefix=\"two specific example ids\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"Not enough examples in dataset for this experiment\")\n",
    "except Exception as e:\n",
    "    print(f\"Specific example IDs experiment skipped: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Repetitions\n",
    "\n",
    "You can run an experiment several times to make sure you have consistent results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'two repetitions-d6cb5eb3' at:\n",
      "https://smith.langchain.com/o/bd531ccf-4286-4467-99ba-7eab707122af/datasets/dab4404e-e95f-4521-9892-284e6476bbca/compare?selectedSessions=a05f477d-d81e-482e-abd4-58c5f88fe2eb\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19afdc14811a4fa2866a91244bcbf3ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.output</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.output</th>\n",
       "      <th>feedback.is_concise</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is LangSmith and how does it help with LL...</td>\n",
       "      <td>LangSmith is a platform that consists of a fro...</td>\n",
       "      <td>None</td>\n",
       "      <td>LangSmith is a platform for debugging, testing...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.142637</td>\n",
       "      <td>7dfd0d3f-c378-4fe2-8a08-301d52dd92e2</td>\n",
       "      <td>2540fb5a-f57a-4faf-92f4-33ad3fcb1701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How can I compare different Mistral AI models?</td>\n",
       "      <td>To compare different Mistral AI models, you ca...</td>\n",
       "      <td>None</td>\n",
       "      <td>You can compare different Mistral AI models by...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.095756</td>\n",
       "      <td>99f57a2e-5d00-40e2-9fbe-01a263fcebba</td>\n",
       "      <td>090a81f4-aee3-428c-9c2c-47ebce68302a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the benefits of using custom evaluators?</td>\n",
       "      <td>Custom evaluators can be beneficial in complex...</td>\n",
       "      <td>None</td>\n",
       "      <td>Custom evaluators allow you to test specific a...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.725765</td>\n",
       "      <td>9ebb8f0f-b91f-425d-b2da-d91149c0cfa1</td>\n",
       "      <td>244b5115-c2c7-47c6-9159-4fd4a60835ef</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How do I set up tracing in my Mistral AI appli...</td>\n",
       "      <td>To set up tracing in your Mistral AI applicati...</td>\n",
       "      <td>None</td>\n",
       "      <td>To set up tracing in your Mistral AI applicati...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.206153</td>\n",
       "      <td>d1fc5afd-2380-4215-accc-cb26660ba5b3</td>\n",
       "      <td>2877b2b8-f0ba-430e-85f1-d9d84ee64577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is LangSmith and how does it help with LL...</td>\n",
       "      <td>LangSmith is a platform that consists of a fro...</td>\n",
       "      <td>None</td>\n",
       "      <td>LangSmith is a platform for debugging, testing...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.827014</td>\n",
       "      <td>7dfd0d3f-c378-4fe2-8a08-301d52dd92e2</td>\n",
       "      <td>70b6d428-8d4b-4fa0-af1a-cabb456b058f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How can I compare different Mistral AI models?</td>\n",
       "      <td>To compare different Mistral AI models, you ca...</td>\n",
       "      <td>None</td>\n",
       "      <td>You can compare different Mistral AI models by...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.848918</td>\n",
       "      <td>99f57a2e-5d00-40e2-9fbe-01a263fcebba</td>\n",
       "      <td>19da2ce2-b322-4dad-8615-62a02edfef9b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What are the benefits of using custom evaluators?</td>\n",
       "      <td>Custom evaluators can be beneficial in complex...</td>\n",
       "      <td>None</td>\n",
       "      <td>Custom evaluators allow you to test specific a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.832502</td>\n",
       "      <td>9ebb8f0f-b91f-425d-b2da-d91149c0cfa1</td>\n",
       "      <td>971525d7-d2a1-4463-85bd-1226fd6214e3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How do I set up tracing in my Mistral AI appli...</td>\n",
       "      <td>To set up tracing in your Mistral AI applicati...</td>\n",
       "      <td>None</td>\n",
       "      <td>To set up tracing in your Mistral AI applicati...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.315302</td>\n",
       "      <td>d1fc5afd-2380-4215-accc-cb26660ba5b3</td>\n",
       "      <td>1c7990b0-b8f7-4d5f-aab1-a7097171fd8b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults two repetitions-d6cb5eb3>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\n",
    "    target_function,\n",
    "    data=dataset_name,\n",
    "    evaluators=[is_concise_enough],\n",
    "    experiment_prefix=\"two repetitions\",\n",
    "    num_repetitions=2   # This field defaults to 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Concurrency\n",
    "You can also kick off concurrent threads of execution to make your experiments finish faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'concurrency-9ab8caad' at:\n",
      "https://smith.langchain.com/o/bd531ccf-4286-4467-99ba-7eab707122af/datasets/dab4404e-e95f-4521-9892-284e6476bbca/compare?selectedSessions=253bbe46-f1a4-4a23-906f-fd9fad5f0ae5\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "216838fae3814ffaa0dfeff2899cffeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.output</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.output</th>\n",
       "      <th>feedback.is_concise</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How can I compare different Mistral AI models?</td>\n",
       "      <td>To compare different Mistral AI models, you ca...</td>\n",
       "      <td>None</td>\n",
       "      <td>You can compare different Mistral AI models by...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.041991</td>\n",
       "      <td>99f57a2e-5d00-40e2-9fbe-01a263fcebba</td>\n",
       "      <td>d679df64-ee4d-4fa2-9219-e1a073e6827e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the benefits of using custom evaluators?</td>\n",
       "      <td>Custom evaluators can be beneficial in complex...</td>\n",
       "      <td>None</td>\n",
       "      <td>Custom evaluators allow you to test specific a...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.062715</td>\n",
       "      <td>9ebb8f0f-b91f-425d-b2da-d91149c0cfa1</td>\n",
       "      <td>3f51b044-d92c-47aa-86c9-64dbae4c8695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is LangSmith and how does it help with LL...</td>\n",
       "      <td>LangSmith is a platform that consists of a fro...</td>\n",
       "      <td>None</td>\n",
       "      <td>LangSmith is a platform for debugging, testing...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.149450</td>\n",
       "      <td>7dfd0d3f-c378-4fe2-8a08-301d52dd92e2</td>\n",
       "      <td>86a467ac-6c78-42be-9868-6193fd8f0a2f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How do I set up tracing in my Mistral AI appli...</td>\n",
       "      <td>To set up tracing in your Mistral AI applicati...</td>\n",
       "      <td>None</td>\n",
       "      <td>To set up tracing in your Mistral AI applicati...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.264403</td>\n",
       "      <td>d1fc5afd-2380-4215-accc-cb26660ba5b3</td>\n",
       "      <td>99190fd8-9c97-43f4-95ea-f48682c06be2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults concurrency-9ab8caad>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\n",
    "    target_function,\n",
    "    data=dataset_name,\n",
    "    evaluators=[is_concise_enough],\n",
    "    experiment_prefix=\"concurrency\",\n",
    "    max_concurrency=3,  # This defaults to None, so this is an improvement!\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Metadata \n",
    "\n",
    "You can (and should) add metadata to your experiments, to make them easier to find in the UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'metadata added-57bbb851' at:\n",
      "https://smith.langchain.com/o/bd531ccf-4286-4467-99ba-7eab707122af/datasets/dab4404e-e95f-4521-9892-284e6476bbca/compare?selectedSessions=fa5357b3-c4f0-4114-a4f7-6981eb26b7a0\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a254732d8843b2bdc6a7831dc855ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.output</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.output</th>\n",
       "      <th>feedback.is_concise</th>\n",
       "      <th>feedback.contains_key_terms</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is LangSmith and how does it help with LL...</td>\n",
       "      <td>LangSmith is a platform that consists of a fro...</td>\n",
       "      <td>None</td>\n",
       "      <td>LangSmith is a platform for debugging, testing...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.959690</td>\n",
       "      <td>7dfd0d3f-c378-4fe2-8a08-301d52dd92e2</td>\n",
       "      <td>299fa867-f7b5-4b22-b13c-5f4dc78be42a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How can I compare different Mistral AI models?</td>\n",
       "      <td>To compare different Mistral AI models, you ca...</td>\n",
       "      <td>None</td>\n",
       "      <td>You can compare different Mistral AI models by...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.048974</td>\n",
       "      <td>99f57a2e-5d00-40e2-9fbe-01a263fcebba</td>\n",
       "      <td>77bd5bf5-8e27-4451-9a3f-7f2d6e73b731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the benefits of using custom evaluators?</td>\n",
       "      <td>Custom evaluators can be beneficial in complex...</td>\n",
       "      <td>None</td>\n",
       "      <td>Custom evaluators allow you to test specific a...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751763</td>\n",
       "      <td>9ebb8f0f-b91f-425d-b2da-d91149c0cfa1</td>\n",
       "      <td>a67b782d-9b20-43d5-be9b-f1100d49e83e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How do I set up tracing in my Mistral AI appli...</td>\n",
       "      <td>To set up tracing in your Mistral AI applicati...</td>\n",
       "      <td>None</td>\n",
       "      <td>To set up tracing in your Mistral AI applicati...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.305908</td>\n",
       "      <td>d1fc5afd-2380-4215-accc-cb26660ba5b3</td>\n",
       "      <td>e3b17f17-82d9-4ea3-b45d-3920fcd1970f</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults metadata added-57bbb851>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\n",
    "    target_function,\n",
    "    data=dataset_name,\n",
    "    evaluators=[is_concise_enough, contains_key_terms],\n",
    "    experiment_prefix=\"metadata added\",\n",
    "    metadata={  # Custom metadata for Mistral AI experiments\n",
    "        \"model_name\": MODEL_NAME,\n",
    "        \"model_provider\": MODEL_PROVIDER,\n",
    "        \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        \"app_version\": APP_VERSION,\n",
    "        \"experiment_type\": \"mistral_comparison\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Migration Summary and Learning Outcomes\n",
    "\n",
    "### Key Changes Made:\n",
    "1. **Model Provider**: Migrated from OpenAI to Mistral AI using `ChatMistralAI`\n",
    "2. **Environment Variables**: Changed from `OPENAI_API_KEY` to `MISTRAL_API_KEY`\n",
    "3. **Embeddings**: Replaced OpenAI embeddings with HuggingFace (`sentence-transformers/all-MiniLM-L6-v2`)\n",
    "4. **Message Format**: Updated to use LangChain message objects (SystemMessage, HumanMessage)\n",
    "5. **Model Configuration**: Updated to use `mistral-small-latest` and `mistral-tiny` for comparisons\n",
    "6. **Project Name**: Updated to `langsmith-academy-mistral`\n",
    "\n",
    "### Custom Tweakings:\n",
    "1. **Additional Evaluator**: Added `contains_key_terms` evaluator to check for domain-specific terms\n",
    "2. **Enhanced Metadata**: Extended experiment metadata to include embedding model and experiment type\n",
    "3. **Model Comparison**: Changed comparison from GPT-4 vs GPT-3.5 to mistral-small-latest vs mistral-tiny\n",
    "4. **Custom Dataset**: Updated dataset name to `Mistral RAG Application Dataset`\n",
    "5. **Temperature Setting**: Added temperature control (0.3) for more natural responses\n",
    "6. **Persistent Storage**: Updated vector store path to `mistral_docs.parquet`\n",
    "\n",
    "### Experimental Features Added:\n",
    "1. **Dual Evaluation**: Combined conciseness and domain relevance evaluation\n",
    "2. **Enhanced Tracing**: Improved metadata tracking for Mistral AI model calls\n",
    "3. **Model Variants**: Easy comparison between different Mistral AI model sizes\n",
    "4. **Embedding Optimization**: Used efficient open-source embeddings for cost reduction\n",
    "\n",
    "### What I Learned:\n",
    "1. **Experiment Migration**: Successfully adapted LangSmith experiments from OpenAI to Mistral AI while maintaining evaluation consistency\n",
    "2. **Multi-Model Comparison**: Learned to compare different variants of the same model family (mistral-small vs mistral-tiny)\n",
    "3. **Custom Evaluators**: Created domain-specific evaluators that test for relevant content beyond just conciseness\n",
    "4. **Metadata Management**: Enhanced experiment tracking with comprehensive metadata for better analysis\n",
    "5. **Cost Optimization**: Implemented cost-effective alternatives using HuggingFace embeddings while maintaining quality\n",
    "\n",
    "### Technical Insights:\n",
    "- Mistral AI models provide competitive performance with different size options for various use cases\n",
    "- HuggingFace embeddings offer a viable open-source alternative to proprietary embedding services\n",
    "- LangChain message objects provide consistent interfaces across different model providers\n",
    "- Custom evaluators can be tailored to specific domains for more meaningful assessment\n",
    "- Experiment metadata is crucial for tracking and comparing different model configurations\n",
    "\n",
    "### Experimental Capabilities Demonstrated:\n",
    "1. **Dataset Versioning**: Running experiments on specific dataset versions and splits\n",
    "2. **Concurrency Control**: Optimizing experiment execution with parallel processing\n",
    "3. **Repetition Testing**: Ensuring consistent results across multiple runs\n",
    "4. **Selective Testing**: Running experiments on specific data points or subsets\n",
    "5. **Comprehensive Metadata**: Tracking detailed experiment information for analysis\n",
    "\n",
    "This migration demonstrates how to adapt experimental workflows when changing language model providers while adding improvements in evaluation methodology and cost efficiency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mat496",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
