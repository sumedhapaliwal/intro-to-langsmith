{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracing Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you set your environment variables, including your Mistral API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can set them inline\n",
    "import os\n",
    "os.environ[\"MISTRAL_API_KEY\"] = \"MISTRAL_API_KEY\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"LANGSMITH_API_KEY\"\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or you can use a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracing with @traceable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The @traceable decorator is a simple way to log traces from the LangSmith Python SDK. Simply decorate any function with @traceable.\n",
    "\n",
    "The decorator works by creating a run tree for you each time the function is called and inserting it within the current trace. The function inputs, name, and other information is then streamed to LangSmith. If the function raises an error or if it returns a response, that information is also added to the tree, and updates are patched to LangSmith so you can detect and diagnose sources of errors. This is all done on a background thread to avoid blocking your app's execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import traceable decorator for LangSmith tracing\n",
    "from langsmith import traceable\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "from utils import get_vector_db_retriever\n",
    "\n",
    "MODEL_PROVIDER = \"mistral\"\n",
    "MODEL_NAME = \"mistral-small-latest\"\n",
    "APP_VERSION = 1.0\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "\n",
    "mistral_client = ChatMistralAI(model=MODEL_NAME)\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "# Set up tracing for each function using @traceable decorator\n",
    "@traceable(run_type=\"retriever\")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)   # NOTE: This is a LangChain vector db retriever, so this .invoke() call will be traced automatically\n",
    "\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": RAG_SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    return call_mistral(messages)\n",
    "\n",
    "\n",
    "@traceable(run_type=\"llm\")\n",
    "def call_mistral(\n",
    "    messages: List[dict], model: str = MODEL_NAME, temperature: float = 0.0\n",
    ") -> str:\n",
    "    # Convert dict messages to LangChain message objects\n",
    "    langchain_messages = []\n",
    "    for msg in messages:\n",
    "        if msg[\"role\"] == \"system\":\n",
    "            langchain_messages.append(SystemMessage(content=msg[\"content\"]))\n",
    "        elif msg[\"role\"] == \"user\":\n",
    "            langchain_messages.append(HumanMessage(content=msg[\"content\"]))\n",
    "    \n",
    "    return mistral_client.invoke(langchain_messages)\n",
    "\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@traceable handles the RunTree lifecycle for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the key benefits of using @traceable decorator for ML model monitoring?\n",
      "Answer: The @traceable decorator helps in monitoring traces by automatically logging and tracking runs related to a single operation. It provides detailed information on trace count, latency, and error rates. Additionally, it allows for indefinite data retention when traces are added to datasets, ensuring that the data is never deleted.\n",
      "Model used: mistral-small-latest\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the key benefits of using @traceable decorator for ML model monitoring?\"\n",
    "ai_answer = langsmith_rag(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {ai_answer}\")\n",
    "print(f\"Model used: {MODEL_NAME}\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's take a look in LangSmith!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangSmith supports sending arbitrary metadata along with traces.\n",
    "\n",
    "Metadata is a collection of key-value pairs that can be attached to runs. Metadata can be used to store additional information about a run, such as the version of the application that generated the run, the environment in which the run was generated, or any other information that you want to associate with a run. Similar to tags, you can use metadata to filter runs in the LangSmith UI, and can be used to group runs together for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "\n",
    "@traceable(\n",
    "    run_type=\"retriever\",\n",
    "    metadata={\"vectordb\": \"sklearn\", \"embedding_model\": \"huggingface\", \"retrieval_method\": \"similarity_search\"}\n",
    ")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "@traceable(\n",
    "    run_type=\"chain\",\n",
    "    metadata={\"processing_step\": \"document_formatting\", \"max_context_length\": 4000}\n",
    ")\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": RAG_SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    return call_mistral(messages)\n",
    "\n",
    "@traceable(\n",
    "    run_type=\"llm\",\n",
    "    metadata={\"model_name\": MODEL_NAME, \"model_provider\": MODEL_PROVIDER, \"temperature\": 0.0, \"max_tokens\": 150}\n",
    ")\n",
    "def call_mistral(\n",
    "    messages: List[dict], model: str = MODEL_NAME, temperature: float = 0.0\n",
    ") -> str:\n",
    "    # Convert dict messages to LangChain message objects\n",
    "    langchain_messages = []\n",
    "    for msg in messages:\n",
    "        if msg[\"role\"] == \"system\":\n",
    "            langchain_messages.append(SystemMessage(content=msg[\"content\"]))\n",
    "        elif msg[\"role\"] == \"user\":\n",
    "            langchain_messages.append(HumanMessage(content=msg[\"content\"]))\n",
    "    \n",
    "    return mistral_client.invoke(langchain_messages)\n",
    "\n",
    "@traceable(\n",
    "    run_type=\"chain\",\n",
    "    metadata={\"pipeline_version\": APP_VERSION, \"system_type\": \"RAG\", \"user_id\": \"rakshit\"}\n",
    ")\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata Test Question: How can metadata help in debugging and monitoring ML production systems?\n",
      "Answer: Metadata can help in debugging and monitoring ML production systems by providing additional context and information about each run. It allows for filtering and grouping runs based on specific criteria, such as the application version or environment. This makes it easier to identify and analyze issues within the system.\n",
      "Metadata includes: vectordb, model_provider, pipeline_version, etc.\n"
     ]
    }
   ],
   "source": [
    "question = \"How can metadata help in debugging and monitoring ML production systems?\"\n",
    "ai_answer = langsmith_rag(question)\n",
    "print(f\"Metadata Test Question: {question}\")\n",
    "print(f\"Answer: {ai_answer}\")\n",
    "print(f\"Metadata includes: vectordb, model_provider, pipeline_version, etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also add metadata at runtime!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime Metadata Test:\n",
      "Question: What are best practices for implementing real-time model performance monitoring?\n",
      "Answer: To implement real-time model performance monitoring, consider using online evaluation to assess your application's outputs in near real-time. This involves running evaluators on real inputs and outputs as they are produced, allowing you to monitor your application and flag unintended behavior. Additionally, you can use automation rules to send specific traces to annotation queues for human review, helping to spot check for issues and gather valuable feedback.\n",
      "Execution time: 1.08 seconds\n",
      "Runtime metadata prepared: ['runtime_metadata', 'execution_time', 'experiment_id', 'user_session', 'performance_mode']\n",
      "Note: Runtime metadata would be added via langsmith_extra in production\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "question = \"What are best practices for implementing real-time model performance monitoring?\"\n",
    "runtime_metadata = {\n",
    "    \"runtime_metadata\": \"production_test\",\n",
    "    \"execution_time\": datetime.now().isoformat(),\n",
    "    \"experiment_id\": \"exp_001\",\n",
    "    \"user_session\": \"rakshit_session_123\",\n",
    "    \"performance_mode\": \"optimized\"\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "# Note: langsmith_extra parameter needs to be handled by the @traceable decorator\n",
    "# For now, we'll call the function normally and add metadata handling in a future version\n",
    "ai_answer = langsmith_rag(question)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Runtime Metadata Test:\")\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {ai_answer}\")\n",
    "print(f\"Execution time: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Runtime metadata prepared: {list(runtime_metadata.keys())}\")\n",
    "print(\"Note: Runtime metadata would be added via langsmith_extra in production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's take a look in LangSmith!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Tracing Experiments\n",
    "\n",
    "Let's explore advanced tracing scenarios with error handling, performance monitoring, and batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robust RAG System Test Results:\n",
      "==================================================\n",
      "\n",
      "Test 1: 'How do I implement fault-tolerant ML systems?'\n",
      "Status: SUCCESS after 1 attempts\n",
      "Response: To implement fault-tolerant ML systems, you can use libraries like tenacity or backoff in Python to ...\n",
      "Execution time: 1.03s\n",
      "\n",
      "Test 2: ''\n",
      "Attempt 1 failed: Empty question provided. Retrying...\n",
      "Attempt 2 failed: Empty question provided. Retrying...\n",
      "Status: FAILED after 3 attempts\n",
      "Error: Empty question provided\n",
      "Total time: 0.00s\n",
      "\n",
      "Test 3: 'What are the key principles of reliable software architecture?'\n",
      "Status: SUCCESS after 1 attempts\n",
      "Response: To implement fault-tolerant ML systems, you can use libraries like tenacity or backoff in Python to ...\n",
      "Execution time: 1.03s\n",
      "\n",
      "Test 2: ''\n",
      "Attempt 1 failed: Empty question provided. Retrying...\n",
      "Attempt 2 failed: Empty question provided. Retrying...\n",
      "Status: FAILED after 3 attempts\n",
      "Error: Empty question provided\n",
      "Total time: 0.00s\n",
      "\n",
      "Test 3: 'What are the key principles of reliable software architecture?'\n",
      "Status: SUCCESS after 1 attempts\n",
      "Response: The context provided does not contain information about the key principles of reliable software arch...\n",
      "Execution time: 0.59s\n",
      "Status: SUCCESS after 1 attempts\n",
      "Response: The context provided does not contain information about the key principles of reliable software arch...\n",
      "Execution time: 0.59s\n"
     ]
    }
   ],
   "source": [
    "# Custom Experiment 1: Error Handling and Performance Monitoring\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "@traceable(\n",
    "    run_type=\"chain\",\n",
    "    metadata={\"experiment\": \"error_handling\", \"version\": \"1.0\", \"author\": \"rakshit\"}\n",
    ")\n",
    "def robust_rag_with_fallback(question: str, max_retries: int = 2):\n",
    "    \"\"\"Enhanced RAG system with error handling and performance monitoring\"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    attempt_count = 0\n",
    "    \n",
    "    for attempt in range(max_retries + 1):\n",
    "        attempt_count += 1\n",
    "        try:\n",
    "            # Add attempt-specific metadata\n",
    "            attempt_metadata = {\n",
    "                \"attempt_number\": attempt_count,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"max_retries\": max_retries\n",
    "            }\n",
    "            \n",
    "            if len(question.strip()) == 0:\n",
    "                raise ValueError(\"Empty question provided\")\n",
    "            \n",
    "            # Simulate potential network delays\n",
    "            if attempt > 0:\n",
    "                time.sleep(0.1 * attempt)\n",
    "            \n",
    "            documents = retrieve_documents(question)\n",
    "            if not documents:\n",
    "                raise RuntimeError(\"No documents retrieved\")\n",
    "            \n",
    "            response = generate_response(question, documents)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            execution_time = end_time - start_time\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"response\": response.content,  # Extract content from AIMessage\n",
    "                \"attempts\": attempt_count,\n",
    "                \"execution_time\": execution_time,\n",
    "                \"metadata\": attempt_metadata\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            if attempt == max_retries:\n",
    "                # Final attempt failed\n",
    "                end_time = time.time()\n",
    "                return {\n",
    "                    \"success\": False,\n",
    "                    \"error\": str(e),\n",
    "                    \"attempts\": attempt_count,\n",
    "                    \"execution_time\": end_time - start_time,\n",
    "                    \"metadata\": attempt_metadata\n",
    "                }\n",
    "            else:\n",
    "                print(f\"Attempt {attempt_count} failed: {e}. Retrying...\")\n",
    "                continue\n",
    "\n",
    "# Test the robust RAG system\n",
    "test_questions = [\n",
    "    \"How do I implement fault-tolerant ML systems?\",\n",
    "    \"\",  # This will trigger error handling\n",
    "    \"What are the key principles of reliable software architecture?\"\n",
    "]\n",
    "\n",
    "print(\"Robust RAG System Test Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, test_q in enumerate(test_questions, 1):\n",
    "    print(f\"\\nTest {i}: '{test_q}'\")\n",
    "    result = robust_rag_with_fallback(test_q)\n",
    "    \n",
    "    if result[\"success\"]:\n",
    "        print(f\"Status: SUCCESS after {result['attempts']} attempts\")\n",
    "        print(f\"Response: {result['response'][:100]}{'...' if len(result['response']) > 100 else ''}\")\n",
    "        print(f\"Execution time: {result['execution_time']:.2f}s\")\n",
    "    else:\n",
    "        print(f\"Status: FAILED after {result['attempts']} attempts\")\n",
    "        print(f\"Error: {result['error']}\")\n",
    "        print(f\"Total time: {result['execution_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Processing Experiment:\n",
      "============================================================\n",
      "Processing batch: ml_concepts_batch\n",
      "Total questions: 5\n",
      "----------------------------------------\n",
      "Q1: What is the difference between machine learning and deep lea...\n",
      "     Processed in 0.51s\n",
      "Q1: What is the difference between machine learning and deep lea...\n",
      "     Processed in 0.51s\n",
      "Q2: How do I choose the right algorithm for my dataset?\n",
      "     Processed in 1.63s\n",
      "Q2: How do I choose the right algorithm for my dataset?\n",
      "     Processed in 1.63s\n",
      "Q3: What are the key metrics for evaluating classification model...\n",
      "     Processed in 1.02s\n",
      "Q3: What are the key metrics for evaluating classification model...\n",
      "     Processed in 1.02s\n",
      "Q4: Explain the concept of overfitting and how to prevent it\n",
      "     Processed in 0.62s\n",
      "Q4: Explain the concept of overfitting and how to prevent it\n",
      "     Processed in 0.62s\n",
      "Q5: How do I implement model versioning in production?\n",
      "     Processed in 1.31s\n",
      "\n",
      "Batch Analytics Summary:\n",
      "Batch ID: ml_concepts_batch\n",
      "Success Rate: 100.0% (5/5)\n",
      "Total Processing Time: 5.10s\n",
      "Average Question Time: 1.02s\n",
      "Fastest Question: 0.51s\n",
      "Slowest Question: 1.63s\n",
      "Q5: How do I implement model versioning in production?\n",
      "     Processed in 1.31s\n",
      "\n",
      "Batch Analytics Summary:\n",
      "Batch ID: ml_concepts_batch\n",
      "Success Rate: 100.0% (5/5)\n",
      "Total Processing Time: 5.10s\n",
      "Average Question Time: 1.02s\n",
      "Fastest Question: 0.51s\n",
      "Slowest Question: 1.63s\n"
     ]
    }
   ],
   "source": [
    "# Custom Experiment 2: Batch Processing with Tracing Analytics\n",
    "from typing import Dict, Any\n",
    "import statistics\n",
    "\n",
    "@traceable(\n",
    "    run_type=\"chain\",\n",
    "    metadata={\"experiment\": \"batch_processing\", \"processing_mode\": \"parallel_simulation\"}\n",
    ")\n",
    "def batch_rag_processor(questions: List[str], batch_id: str = \"batch_001\"):\n",
    "    \"\"\"Process multiple questions with detailed tracing and analytics\"\"\"\n",
    "    \n",
    "    batch_start_time = time.time()\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Processing batch: {batch_id}\")\n",
    "    print(f\"Total questions: {len(questions)}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for i, question in enumerate(questions, 1):\n",
    "        question_start_time = time.time()\n",
    "        \n",
    "        # Create question-specific metadata\n",
    "        question_metadata = {\n",
    "            \"batch_id\": batch_id,\n",
    "            \"question_index\": i,\n",
    "            \"total_questions\": len(questions),\n",
    "            \"question_length\": len(question),\n",
    "            \"complexity_score\": len(question.split()) / 10.0  # Simple complexity metric\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Process question (metadata would be added via langsmith_extra in production)\n",
    "            response = langsmith_rag(question)\n",
    "            \n",
    "            question_end_time = time.time()\n",
    "            processing_time = question_end_time - question_start_time\n",
    "            \n",
    "            result = {\n",
    "                \"index\": i,\n",
    "                \"question\": question,\n",
    "                \"response\": response,\n",
    "                \"processing_time\": processing_time,\n",
    "                \"success\": True,\n",
    "                \"metadata\": question_metadata\n",
    "            }\n",
    "            \n",
    "            print(f\"Q{i}: {question[:60]}{'...' if len(question) > 60 else ''}\")\n",
    "            print(f\"     Processed in {processing_time:.2f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            result = {\n",
    "                \"index\": i,\n",
    "                \"question\": question,\n",
    "                \"error\": str(e),\n",
    "                \"success\": False,\n",
    "                \"metadata\": question_metadata\n",
    "            }\n",
    "            print(f\"Q{i}: ERROR - {e}\")\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    batch_end_time = time.time()\n",
    "    total_batch_time = batch_end_time - batch_start_time\n",
    "    \n",
    "    # Generate batch analytics\n",
    "    successful_results = [r for r in results if r[\"success\"]]\n",
    "    failed_results = [r for r in results if not r[\"success\"]]\n",
    "    \n",
    "    analytics = {\n",
    "        \"batch_id\": batch_id,\n",
    "        \"total_questions\": len(questions),\n",
    "        \"successful\": len(successful_results),\n",
    "        \"failed\": len(failed_results),\n",
    "        \"success_rate\": len(successful_results) / len(questions) * 100,\n",
    "        \"total_batch_time\": total_batch_time,\n",
    "        \"average_processing_time\": statistics.mean([r[\"processing_time\"] for r in successful_results]) if successful_results else 0,\n",
    "        \"min_processing_time\": min([r[\"processing_time\"] for r in successful_results]) if successful_results else 0,\n",
    "        \"max_processing_time\": max([r[\"processing_time\"] for r in successful_results]) if successful_results else 0\n",
    "    }\n",
    "    \n",
    "    return results, analytics\n",
    "\n",
    "# Test batch processing\n",
    "batch_questions = [\n",
    "    \"What is the difference between machine learning and deep learning?\",\n",
    "    \"How do I choose the right algorithm for my dataset?\",\n",
    "    \"What are the key metrics for evaluating classification models?\",\n",
    "    \"Explain the concept of overfitting and how to prevent it\",\n",
    "    \"How do I implement model versioning in production?\"\n",
    "]\n",
    "\n",
    "print(\"Batch Processing Experiment:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "batch_results, batch_analytics = batch_rag_processor(batch_questions, \"ml_concepts_batch\")\n",
    "\n",
    "print(f\"\\nBatch Analytics Summary:\")\n",
    "print(f\"Batch ID: {batch_analytics['batch_id']}\")\n",
    "print(f\"Success Rate: {batch_analytics['success_rate']:.1f}% ({batch_analytics['successful']}/{batch_analytics['total_questions']})\")\n",
    "print(f\"Total Processing Time: {batch_analytics['total_batch_time']:.2f}s\")\n",
    "print(f\"Average Question Time: {batch_analytics['average_processing_time']:.2f}s\")\n",
    "print(f\"Fastest Question: {batch_analytics['min_processing_time']:.2f}s\")\n",
    "print(f\"Slowest Question: {batch_analytics['max_processing_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A/B Testing Experiment: Prompt Variations\n",
      "=======================================================\n",
      "Test Question: How do I optimize the performance of my machine learning model?\n",
      "\n",
      "Prompt Variation: CONCISE\n",
      "------------------------------\n",
      "Response: To optimize your classifier's performance based on user feedback, follow the tutorial on LangChain's documentation. It guides you through building a GitHub issue classifier and improving it using collected feedback.\n",
      "Processing time: 0.67s\n",
      "Response length: 215 chars\n",
      "Word count: 30 words\n",
      "\n",
      "Prompt Variation: DETAILED\n",
      "------------------------------\n",
      "Response: To optimize the performance of your machine learning model, you can follow these steps:\n",
      "\n",
      "1. **Collect and Utilize User Feedback**: Gather user feedback to create few-shot examples that can help refine your classifier. For instance, if you're classifying GitHub issues based on their titles, user feedback can provide the desired outputs, making it easier to improve the classifier's accuracy.\n",
      "\n",
      "2. **Implement Rate Limit Handling**: When running large evaluation jobs, handle model rate limits by using rate limiters or retry mechanisms with exponential backoff. This ensures that your model doesn't get overwhelmed by too many requests at once, maintaining smooth performance.\n",
      "\n",
      "3. **Continuous Evaluation and Improvement**: Regularly evaluate your model's performance using datasets and feedback criteria. For example, you can use evaluation techniques to assess the model's correctness and make necessary adjustments based on the results.\n",
      "\n",
      "By following these steps, you can enhance the performance and reliability of your machine learning model.\n",
      "Processing time: 2.44s\n",
      "Response length: 1046 chars\n",
      "Word count: 151 words\n",
      "\n",
      "Prompt Variation: TECHNICAL\n",
      "------------------------------\n",
      "Response: To optimize the performance of your machine learning model, particularly in the context of a classifier as described in the LangChain documentation, you can follow these steps:\n",
      "\n",
      "### 1. **Set Up Environment and Initial Application**\n",
      "First, ensure you have the necessary environment variables and initial setup:\n",
      "\n",
      "```python\n",
      "import os\n",
      "os.environ[\"LANGSMITH_PROJECT\"] = \"classifier\"\n",
      "```\n",
      "\n",
      "### 2. **Create the Initial Classifier Function**\n",
      "Implement a simple function to classify GitHub issues based on their titles:\n",
      "\n",
      "```python\n",
      "import openai\n",
      "from langsmith import traceable, Client\n",
      "import uuid\n",
      "\n",
      "client = openai.Client()\n",
      "\n",
      "@traceable\n",
      "def classify_issue(title):\n",
      "    response = client.completions.create(\n",
      "        model=\"text-davinci-002\",\n",
      "        prompt=f\"Classify the following GitHub issue title into one of the following categories: bug, feature, question, documentation, or other.\\n\\nTitle: {title}\\n\\nCategory:\",\n",
      "        max_tokens=10\n",
      "    )\n",
      "    return response.choices[0].text.strip()\n",
      "```\n",
      "\n",
      "### 3. **Collect User Feedback**\n",
      "Collect feedback from users to improve the classifier. This can be done by logging the correct classifications and comparing them with the model's predictions.\n",
      "\n",
      "### 4. **Use Few-Shot Learning**\n",
      "Create few-shot examples based on user feedback to improve the classifier's performance. Few-shot learning involves providing the model with a few examples of input-output pairs to help it understand the task better.\n",
      "\n",
      "```python\n",
      "def classify_with_few_shot(title, examples):\n",
      "    few_shot_prompt = \"\\n\\n\".join([f\"Title: {example['title']}\\nCategory: {example['category']}\" for example in examples])\n",
      "    prompt = f\"{few_shot_prompt}\\n\\nTitle: {title}\\nCategory:\"\n",
      "    response = client.completions.create(\n",
      "        model=\"text-davinci-002\",\n",
      "        prompt=prompt,\n",
      "        max_tokens=10\n",
      "    )\n",
      "    return response.choices[0].text.strip()\n",
      "```\n",
      "\n",
      "### 5. **Implement Feedback Loop**\n",
      "Create a feedback loop to continuously update the classifier based on user feedback. This can be done by storing the feedback in a database and using it to retrain the model periodically.\n",
      "\n",
      "### 6. **Handle Rate Limits**\n",
      "To handle model rate limits, you can use LangChain's `RateLimiters` to manage the number of requests sent to the model API:\n",
      "\n",
      "```python\n",
      "from langchain.utilities import RateLimiter\n",
      "\n",
      "rate_limiter = RateLimiter(max_calls=10, period=1)  # 10 calls per second\n",
      "\n",
      "@rate_limiter\n",
      "def classify_issue_with_rate_limit(title):\n",
      "    return classify_issue(title)\n",
      "```\n",
      "\n",
      "### 7. **Evaluate and Optimize**\n",
      "Use evaluation techniques to assess the classifier's performance and make necessary adjustments. This can include using metrics like accuracy, precision, recall, and F1-score to evaluate the model's performance.\n",
      "\n",
      "```python\n",
      "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
      "\n",
      "def evaluate_classifier(predictions, true_labels):\n",
      "    accuracy = accuracy_score(true_labels, predictions)\n",
      "    precision = precision_score(true_labels, predictions, average='weighted')\n",
      "    recall = recall_score(true_labels, predictions, average='weighted')\n",
      "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
      "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
      "```\n",
      "\n",
      "By following these steps, you can optimize the performance of your machine learning model and ensure it continues to improve based on user feedback.\n",
      "Processing time: 6.07s\n",
      "Response length: 3351 chars\n",
      "Word count: 389 words\n",
      "\n",
      "Performance Comparison:\n",
      "-------------------------\n",
      "Fastest response: concise (0.67s)\n",
      "Most detailed: technical (389 words)\n",
      "Average processing time: 3.06s\n",
      "Average response length: 190.0 words\n",
      "Prompt Variation: CONCISE\n",
      "------------------------------\n",
      "Response: To optimize your classifier's performance based on user feedback, follow the tutorial on LangChain's documentation. It guides you through building a GitHub issue classifier and improving it using collected feedback.\n",
      "Processing time: 0.67s\n",
      "Response length: 215 chars\n",
      "Word count: 30 words\n",
      "\n",
      "Prompt Variation: DETAILED\n",
      "------------------------------\n",
      "Response: To optimize the performance of your machine learning model, you can follow these steps:\n",
      "\n",
      "1. **Collect and Utilize User Feedback**: Gather user feedback to create few-shot examples that can help refine your classifier. For instance, if you're classifying GitHub issues based on their titles, user feedback can provide the desired outputs, making it easier to improve the classifier's accuracy.\n",
      "\n",
      "2. **Implement Rate Limit Handling**: When running large evaluation jobs, handle model rate limits by using rate limiters or retry mechanisms with exponential backoff. This ensures that your model doesn't get overwhelmed by too many requests at once, maintaining smooth performance.\n",
      "\n",
      "3. **Continuous Evaluation and Improvement**: Regularly evaluate your model's performance using datasets and feedback criteria. For example, you can use evaluation techniques to assess the model's correctness and make necessary adjustments based on the results.\n",
      "\n",
      "By following these steps, you can enhance the performance and reliability of your machine learning model.\n",
      "Processing time: 2.44s\n",
      "Response length: 1046 chars\n",
      "Word count: 151 words\n",
      "\n",
      "Prompt Variation: TECHNICAL\n",
      "------------------------------\n",
      "Response: To optimize the performance of your machine learning model, particularly in the context of a classifier as described in the LangChain documentation, you can follow these steps:\n",
      "\n",
      "### 1. **Set Up Environment and Initial Application**\n",
      "First, ensure you have the necessary environment variables and initial setup:\n",
      "\n",
      "```python\n",
      "import os\n",
      "os.environ[\"LANGSMITH_PROJECT\"] = \"classifier\"\n",
      "```\n",
      "\n",
      "### 2. **Create the Initial Classifier Function**\n",
      "Implement a simple function to classify GitHub issues based on their titles:\n",
      "\n",
      "```python\n",
      "import openai\n",
      "from langsmith import traceable, Client\n",
      "import uuid\n",
      "\n",
      "client = openai.Client()\n",
      "\n",
      "@traceable\n",
      "def classify_issue(title):\n",
      "    response = client.completions.create(\n",
      "        model=\"text-davinci-002\",\n",
      "        prompt=f\"Classify the following GitHub issue title into one of the following categories: bug, feature, question, documentation, or other.\\n\\nTitle: {title}\\n\\nCategory:\",\n",
      "        max_tokens=10\n",
      "    )\n",
      "    return response.choices[0].text.strip()\n",
      "```\n",
      "\n",
      "### 3. **Collect User Feedback**\n",
      "Collect feedback from users to improve the classifier. This can be done by logging the correct classifications and comparing them with the model's predictions.\n",
      "\n",
      "### 4. **Use Few-Shot Learning**\n",
      "Create few-shot examples based on user feedback to improve the classifier's performance. Few-shot learning involves providing the model with a few examples of input-output pairs to help it understand the task better.\n",
      "\n",
      "```python\n",
      "def classify_with_few_shot(title, examples):\n",
      "    few_shot_prompt = \"\\n\\n\".join([f\"Title: {example['title']}\\nCategory: {example['category']}\" for example in examples])\n",
      "    prompt = f\"{few_shot_prompt}\\n\\nTitle: {title}\\nCategory:\"\n",
      "    response = client.completions.create(\n",
      "        model=\"text-davinci-002\",\n",
      "        prompt=prompt,\n",
      "        max_tokens=10\n",
      "    )\n",
      "    return response.choices[0].text.strip()\n",
      "```\n",
      "\n",
      "### 5. **Implement Feedback Loop**\n",
      "Create a feedback loop to continuously update the classifier based on user feedback. This can be done by storing the feedback in a database and using it to retrain the model periodically.\n",
      "\n",
      "### 6. **Handle Rate Limits**\n",
      "To handle model rate limits, you can use LangChain's `RateLimiters` to manage the number of requests sent to the model API:\n",
      "\n",
      "```python\n",
      "from langchain.utilities import RateLimiter\n",
      "\n",
      "rate_limiter = RateLimiter(max_calls=10, period=1)  # 10 calls per second\n",
      "\n",
      "@rate_limiter\n",
      "def classify_issue_with_rate_limit(title):\n",
      "    return classify_issue(title)\n",
      "```\n",
      "\n",
      "### 7. **Evaluate and Optimize**\n",
      "Use evaluation techniques to assess the classifier's performance and make necessary adjustments. This can include using metrics like accuracy, precision, recall, and F1-score to evaluate the model's performance.\n",
      "\n",
      "```python\n",
      "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
      "\n",
      "def evaluate_classifier(predictions, true_labels):\n",
      "    accuracy = accuracy_score(true_labels, predictions)\n",
      "    precision = precision_score(true_labels, predictions, average='weighted')\n",
      "    recall = recall_score(true_labels, predictions, average='weighted')\n",
      "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
      "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
      "```\n",
      "\n",
      "By following these steps, you can optimize the performance of your machine learning model and ensure it continues to improve based on user feedback.\n",
      "Processing time: 6.07s\n",
      "Response length: 3351 chars\n",
      "Word count: 389 words\n",
      "\n",
      "Performance Comparison:\n",
      "-------------------------\n",
      "Fastest response: concise (0.67s)\n",
      "Most detailed: technical (389 words)\n",
      "Average processing time: 3.06s\n",
      "Average response length: 190.0 words\n"
     ]
    }
   ],
   "source": [
    "# Custom Experiment 3: A/B Testing Different Prompts with Tracing\n",
    "import random\n",
    "\n",
    "@traceable(run_type=\"chain\", metadata={\"experiment\": \"ab_testing\", \"test_type\": \"prompt_optimization\"})\n",
    "def ab_test_prompts(question: str, test_id: str = \"ab_test_001\"):\n",
    "    \"\"\"Test different prompt variations with tracing\"\"\"\n",
    "    \n",
    "    # Define prompt variations\n",
    "    prompt_variations = {\n",
    "        \"concise\": \"\"\"You are a concise assistant. Answer briefly and directly. \n",
    "        Use the context provided. Maximum 2 sentences.\"\"\",\n",
    "        \n",
    "        \"detailed\": \"\"\"You are a detailed assistant for question-answering tasks. \n",
    "        Use the following pieces of retrieved context to provide comprehensive answers. \n",
    "        Include examples when relevant. Use 3-4 sentences maximum.\"\"\",\n",
    "        \n",
    "        \"technical\": \"\"\"You are a technical expert assistant. \n",
    "        Use the provided context to give precise, technical answers. \n",
    "        Include specific terminology and implementation details. Keep responses focused.\"\"\"\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for prompt_name, system_prompt in prompt_variations.items():\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create variation-specific metadata\n",
    "        variation_metadata = {\n",
    "            \"test_id\": test_id,\n",
    "            \"prompt_variation\": prompt_name,\n",
    "            \"prompt_length\": len(system_prompt),\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Get documents (same for all variations)\n",
    "            documents = retrieve_documents(question)\n",
    "            formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "            \n",
    "            # Create messages with the specific prompt variation\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"}\n",
    "            ]\n",
    "            \n",
    "            # Call Mistral with tracing\n",
    "            response = call_mistral(messages)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            processing_time = end_time - start_time\n",
    "            \n",
    "            # Extract content from AIMessage object\n",
    "            response_content = response.content if hasattr(response, 'content') else str(response)\n",
    "            \n",
    "            results[prompt_name] = {\n",
    "                \"response\": response_content,\n",
    "                \"processing_time\": processing_time,\n",
    "                \"response_length\": len(response_content),\n",
    "                \"word_count\": len(response_content.split()),\n",
    "                \"success\": True,\n",
    "                \"metadata\": variation_metadata\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            results[prompt_name] = {\n",
    "                \"error\": str(e),\n",
    "                \"success\": False,\n",
    "                \"metadata\": variation_metadata\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test A/B testing\n",
    "test_question = \"How do I optimize the performance of my machine learning model?\"\n",
    "\n",
    "print(\"A/B Testing Experiment: Prompt Variations\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"Test Question: {test_question}\")\n",
    "print()\n",
    "\n",
    "ab_results = ab_test_prompts(test_question, \"prompt_optimization_v1\")\n",
    "\n",
    "# Analyze results\n",
    "for prompt_name, result in ab_results.items():\n",
    "    print(f\"Prompt Variation: {prompt_name.upper()}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    if result[\"success\"]:\n",
    "        print(f\"Response: {result['response']}\")\n",
    "        print(f\"Processing time: {result['processing_time']:.2f}s\")\n",
    "        print(f\"Response length: {result['response_length']} chars\")\n",
    "        print(f\"Word count: {result['word_count']} words\")\n",
    "    else:\n",
    "        print(f\"Error: {result['error']}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "# Summary comparison\n",
    "successful_results = {k: v for k, v in ab_results.items() if v[\"success\"]}\n",
    "\n",
    "if len(successful_results) > 1:\n",
    "    print(\"Performance Comparison:\")\n",
    "    print(\"-\" * 25)\n",
    "    \n",
    "    # Find fastest and most detailed responses\n",
    "    fastest = min(successful_results.items(), key=lambda x: x[1][\"processing_time\"])\n",
    "    most_detailed = max(successful_results.items(), key=lambda x: x[1][\"word_count\"])\n",
    "    \n",
    "    print(f\"Fastest response: {fastest[0]} ({fastest[1]['processing_time']:.2f}s)\")\n",
    "    print(f\"Most detailed: {most_detailed[0]} ({most_detailed[1]['word_count']} words)\")\n",
    "    \n",
    "    avg_time = sum(r[\"processing_time\"] for r in successful_results.values()) / len(successful_results)\n",
    "    avg_length = sum(r[\"word_count\"] for r in successful_results.values()) / len(successful_results)\n",
    "    \n",
    "    print(f\"Average processing time: {avg_time:.2f}s\")\n",
    "    print(f\"Average response length: {avg_length:.1f} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Working through this notebook was really eye-opening! I discovered that the @traceable decorator is like having a smart assistant that automatically logs everything your AI functions do - it creates these neat run trees and sends all the details to LangSmith without you having to think about it. What really clicked for me was seeing how you can add metadata to make debugging so much easier, and it all happens in the background without slowing anything down.\n",
    "\n",
    "**What I Changed:**\n",
    "- Switched everything from OpenAI to Mistral AI (using their mistral-small-latest model) but kept all the cool LangSmith tracing features working perfectly\n",
    "- Updated the API keys - bye bye OPENAI_API_KEY, hello MISTRAL_API_KEY\n",
    "- Fixed up the RAG system to work nicely with LangChain's ChatMistralAI - had to convert message formats properly\n",
    "- Added tons of useful metadata to each function so I can track things like which model I'm using, performance metrics, and version info\n",
    "- Built some fun experiments to test error handling, batch processing multiple questions at once, and A/B testing different prompts\n",
    "- Created a robust system that can retry when things go wrong and monitors how long everything takes\n",
    "- Played around with adding metadata while the code is running, which is pretty neat for tracking dynamic information\n",
    "\n",
    "The coolest thing I learned is that LangSmith's tracing doesn't care which AI model you use - switching from OpenAI to Mistral was seamless. Plus, all those experiments showed me how powerful tracing can be for monitoring real production systems, optimizing performance, and systematically testing different approaches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mat496",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
