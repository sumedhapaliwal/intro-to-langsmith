{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Tracing Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AWTT](../../images/alternative_ways_to_trace_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far in this module, we've taken a look at the traceable decorator, and how we can use it to set up tracing.\n",
    "\n",
    "In this lesson, we're going to look at alternative ways in which we can set up tracing, and when you should think about using these different approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain and LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are using LangChain or LangGraph, all we need to do to set up tracing is to set a few environment variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AWTT](../../images/alternative_ways_to_trace_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can set them inline\n",
    "import os\n",
    "os.environ[\"MISTRAL_API_KEY\"] = \"MISTRAL_API_KEY\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"LANGSMITH_API_KEY\"\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy\"  # If you don't set this, traces will go to the Default project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or you can use a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry too much about our graph implementation here, you can learn more about LangGraph through our LangGraph Academy course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import operator\n",
    "from langchain.schema import Document\n",
    "from langchain_core.messages import HumanMessage, AnyMessage, get_buffer_string\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "from utils import get_vector_db_retriever, RAG_PROMPT\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "retriever = get_vector_db_retriever()\n",
    "llm = ChatMistralAI(model=\"mistral-small-latest\", temperature=0)\n",
    "\n",
    "# Define Graph state\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    messages: Annotated[List[AnyMessage], operator.add]\n",
    "    documents: List[Document]\n",
    "\n",
    "# Define Nodes\n",
    "def retrieve_documents(state: GraphState):\n",
    "    messages = state.get(\"messages\", [])\n",
    "    question = state[\"question\"]\n",
    "    documents = retriever.invoke(f\"{get_buffer_string(messages)} {question}\")\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "def generate_response(state: GraphState):\n",
    "    question = state[\"question\"]\n",
    "    messages = state[\"messages\"]\n",
    "    documents = state[\"documents\"]\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    \n",
    "    rag_prompt_formatted = RAG_PROMPT.format(context=formatted_docs, conversation=messages, question=question)\n",
    "    generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "    return {\"documents\": documents, \"messages\": [HumanMessage(question), generation]}\n",
    "\n",
    "# Define Graph\n",
    "graph_builder = StateGraph(GraphState)\n",
    "graph_builder.add_node(\"retrieve_documents\", retrieve_documents)\n",
    "graph_builder.add_node(\"generate_response\", generate_response)\n",
    "graph_builder.add_edge(START, \"retrieve_documents\")\n",
    "graph_builder.add_edge(\"retrieve_documents\", \"generate_response\")\n",
    "graph_builder.add_edge(\"generate_response\", END)\n",
    "\n",
    "simple_rag_graph = graph_builder.compile()\n",
    "display(Image(simple_rag_graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're setting up a simple graph in LangGraph. If you want to learn more about LangGraph, I would highly recommend taking a look at our LangGraph Academy course.\n",
    "\n",
    "You can also pass in metadata or other fields through an optional config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"How can I use LangSmith to evaluate my ML model's performance?\",\n",
       " 'messages': [HumanMessage(content=\"How can I use LangSmith to evaluate my ML model's performance?\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"To evaluate your ML model's performance using LangSmith, you can use prebuilt LLM-as-judge evaluators from the open-source openevals package or define custom evaluators. First, install the required dependencies like `langsmith` and `openevals`, then set up environment variables for tracing. Create a dataset with input and reference output pairs to test your model's responses.\", additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 1195, 'total_tokens': 1274, 'completion_tokens': 79}, 'model_name': 'mistral-small-latest', 'model': 'mistral-small-latest', 'finish_reason': 'stop'}, id='run--03ec59f5-ce34-4ccc-8c8e-0df340a9b9c3-0', usage_metadata={'input_tokens': 1195, 'output_tokens': 79, 'total_tokens': 1274})],\n",
       " 'documents': [Document(metadata={'id': '844d25ee-02c6-4f40-8466-952842503968', 'changefreq': 'weekly', 'loc': 'https://docs.smith.langchain.com/evaluation/tutorials/evaluation', 'priority': '0.5', 'source': 'https://docs.smith.langchain.com/evaluation/tutorials/evaluation'}, page_content='For more information on the evaluation workflows LangSmith supports, check out the how-to guides, or see the reference docs for evaluate and its asynchronous aevaluate counterpart.\\nLots to cover, let’s dive in!\\n\\u200bSetup\\nFirst install the required dependencies for this tutorial. We happen to use OpenAI, but LangSmith can be used with any model:\\npipuvCopypip install -U langsmith openai\\n\\nAnd set environment variables to enable LangSmith tracing:\\nCopyexport LANGSMITH_TRACING=\"true\"\\nexport LANGSMITH_API_KEY=\"<Your LangSmith API key>\"\\nexport OPENAI_API_KEY=\"<Your OpenAI API key>\"\\n\\n\\u200bCreate a dataset\\nThe first step when getting ready to test and evaluate your application is to define the datapoints you want to evaluate. There are a few aspects to consider here:\\n\\nWhat should the schema of each datapoint be?\\nHow many datapoints should I gather?\\nHow should I gather those datapoints?'),\n",
       "  Document(metadata={'id': 'b9daa630-9bcb-4520-a734-9dbb3a22b2bb', 'changefreq': 'weekly', 'loc': 'https://docs.smith.langchain.com/evaluation/tutorials', 'priority': '0.5', 'source': 'https://docs.smith.langchain.com/evaluation/tutorials'}, page_content='This quick start guides you through running a simple evaluation to test the correctness of LLM responses with the LangSmith SDK or UI.\\nThis guide uses prebuilt LLM-as-judge evaluators from the open-source openevals package. OpenEvals includes a set of commonly used evaluators and is a great starting point if you’re new to evaluations. If you want greater flexibility in how you evaluate your apps, you can also define completely custom evaluators using your own code.\\n\\u200bLangSmith SDK\\n\\u200b1. Install dependencies\\nPythonTypeScriptCopypip install -U langsmith openevals openai\\n\\nIf you are using yarn as your package manager, you will also need to manually install @langchain/core as a peer dependency of openevals. This is not required for LangSmith evals in general - you may define evaluators using arbitrary custom code.\\n\\u200b2. Create a LangSmith API key\\nTo create an API key, head to the Settings page. Then click + API Key.\\n\\u200b3. Set up environment variables\\nThis guide uses OpenAI, but you can adapt it to use any LLM provider.\\nIf you’re using Anthropic, use the Anthropic wrapper to trace your calls. For other providers, use the traceable wrapper.\\nCopyexport LANGSMITH_TRACING=true\\nexport LANGSMITH_API_KEY=\"<your-langchain-api-key>\"\\nexport OPENAI_API_KEY=\"<your-openai-api-key>\"\\n\\n\\u200b4. Create a dataset\\nNext, define example input and reference output pairs that you’ll use to evaluate your app:\\nPythonTypeScriptCopyfrom langsmith import Client\\nclient = Client()\\n\\n# Programmatically create a dataset in LangSmith\\n# For other dataset creation methods, see:\\n# https://docs.smith.langchain.com/langsmith/manage-datasets-programmatically\\n# https://docs.smith.langchain.com/langsmith/manage-datasets-in-application\\ndataset = client.create_dataset(\\n    dataset_name=\"Sample dataset\", description=\"A sample dataset in LangSmith.\"\\n)'),\n",
       "  Document(metadata={'id': '19b16057-ad62-4da6-9ed5-afb5a5c8afd7', 'changefreq': 'weekly', 'loc': 'https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators', 'priority': '0.5', 'source': 'https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators'}, page_content=\"Skip to main contentLangSmith docs have moved! Find the LangSmith docs at the new LangChain Docs site.API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceLangChain off-the-shelf evaluatorsReferencesdk_referenceLangChain off-the-shelf evaluatorsLangChain off-the-shelf evaluators\\nLangChain's evaluation module provides evaluators you can use as-is for common evaluation scenarios.\\nTo learn how to use these evaluators, please refer to the following guide.\\nnoteWe currently support off-the-shelf evaluators in LangChain for Python only.\\nnoteMost of these evaluators are useful but imperfect! We recommend against blind trust of any single automated metric and to always incorporate them as a part of a holistic testing and evaluation strategy.\\nMany of the LLM-based evaluators return a binary score for a given datapoint, so measuring differences in prompt or model performance are most reliable in aggregate over a larger dataset.\\nThe following table enumerates the off-the-shelf evaluators available in LangSmith, along with their output keys and a simple code sample.\"),\n",
       "  Document(metadata={'id': 'e6e14dd7-a0ed-4415-a6b6-c83c182e3af3', 'changefreq': 'weekly', 'loc': 'https://docs.smith.langchain.com/reference/data_formats/example_data_format', 'priority': '0.5', 'source': 'https://docs.smith.langchain.com/reference/data_formats/example_data_format'}, page_content='LangSmith stores examples in datasets as follows:\\nField NameTypeDescriptionidUUIDUnique identifier for the example.namestringThe name of the example.created_atdatetimeThe time this example was createdmodified_atdatetimeThe last time this example was modifiedinputsobjectA map of inputs for the example.outputsobjectA map or set of outputs generated by the run.dataset_idUUIDThe dataset the example belongs tosource_run_idUUIDIf this example was created from a LangSmith Run, the ID of said runmetadataobjectA map of additional, user or SDK defined information that can be stored on an example.\\nTo learn more about how examples are used in evaluation, read our how-to guide on evaluating LLM applications.Was this page helpful?YesNoSuggest editsAudit evaluator scoresDataset prebuilt JSON schema types⌘IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How can I use LangSmith to evaluate my ML model's performance?\"\n",
    "simple_rag_graph.invoke({\"question\": question}, config={\"metadata\": {\"custom_experiment\": \"model_eval\", \"user\": \"rakshit\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's take a look in LangSmith!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracing Context Manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, you can use the trace context manager to log traces to LangSmith. This is useful in situations where:\n",
    "\n",
    "You want to log traces for a specific block of code.\n",
    "You want control over the inputs, outputs, and other attributes of the trace.\n",
    "It is not feasible to use a decorator or wrapper.\n",
    "Any or all of the above.\n",
    "The context manager integrates seamlessly with the traceable decorator and wrap_openai wrapper, so you can use them together in the same application.\n",
    "\n",
    "You still need to set your `LANGSMITH_API_KEY` and `LANGSMITH_TRACING`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AWTT](../../images/alternative_ways_to_trace_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable, trace\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "from utils import get_vector_db_retriever\n",
    "\n",
    "MODEL_PROVIDER = \"mistral\"\n",
    "MODEL_NAME = \"mistral-small-latest\"\n",
    "APP_VERSION = 1.0\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "\n",
    "mistral_client = ChatMistralAI(model=MODEL_NAME)\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "\"\"\"\n",
    "retrieve_documents\n",
    "- Returns documents fetched from a vectorstore based on the user's question\n",
    "\"\"\"\n",
    "@traceable\n",
    "def retrieve_documents(question: str):\n",
    "    documents = retriever.invoke(question)\n",
    "    return documents\n",
    "\n",
    "\"\"\"\n",
    "generate_response\n",
    "- Calls `call_mistral` to generate a model response after formatting inputs\n",
    "\"\"\"\n",
    "# TODO: Remove traceable, and use with trace()\n",
    "@traceable\n",
    "def generate_response(question: str, documents):\n",
    "    # NOTE: Our documents came in as a list of objects, but we just want to log a string\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "\n",
    "    # TODO: Use with trace()\n",
    "    # with trace(\n",
    "    #     name=\"Generate Response\",\n",
    "    #     run_type=\"chain\", \n",
    "    #     inputs={\"question\": question, \"formatted_docs\": formatted_docs},\n",
    "    #     metadata={\"foo\": \"bar\"},\n",
    "    # ) as ls_trace:\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": RAG_SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    response = call_mistral(messages)\n",
    "    # TODO: End your trace and write outputs to LangSmith\n",
    "    # ls_trace.end(outputs={\"output\": response})\n",
    "    return response\n",
    "\n",
    "\"\"\"\n",
    "call_mistral\n",
    "- Returns the chat completion output from Mistral AI\n",
    "\"\"\"\n",
    "@traceable\n",
    "def call_mistral(\n",
    "    messages: List[dict], model: str = MODEL_NAME, temperature: float = 0.0\n",
    ") -> str:\n",
    "    from langchain_core.messages import HumanMessage, SystemMessage\n",
    "    \n",
    "    # Convert dict messages to LangChain message objects\n",
    "    langchain_messages = []\n",
    "    for msg in messages:\n",
    "        if msg[\"role\"] == \"system\":\n",
    "            langchain_messages.append(SystemMessage(content=msg[\"content\"]))\n",
    "        elif msg[\"role\"] == \"user\":\n",
    "            langchain_messages.append(HumanMessage(content=msg[\"content\"]))\n",
    "    \n",
    "    response = mistral_client.invoke(langchain_messages)\n",
    "    return response\n",
    "\n",
    "\"\"\"\n",
    "langsmith_rag\n",
    "- Calls `retrieve_documents` to fetch documents\n",
    "- Calls `generate_response` to generate a response based on the fetched documents\n",
    "- Returns the model response\n",
    "\"\"\"\n",
    "@traceable\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Response: To debug LLM applications using tracing, you can attach feedback to traces, which can come from users, annotators, or automated evaluators. This feedback is crucial for monitoring and evaluating applications. You can use the `create_feedback()` or `createFeedback()` function to log feedback using the SDK. Additionally, you can attach user feedback to any child run of a trace, not just the root run, which is useful for critiquing specific steps of the LLM application.\n",
      "Question asked: What are the best practices for debugging LLM applications using tracing?\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the best practices for debugging LLM applications using tracing?\"\n",
    "ai_answer = langsmith_rag(question)\n",
    "print(f\"AI Response: {ai_answer}\")\n",
    "print(f\"Question asked: {question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral AI Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we demonstrate how to integrate Mistral AI with LangSmith tracing using the @traceable decorator. The decorator works seamlessly with Mistral AI clients and allows you to automatically log traces without requiring function wrapping.\n",
    "\n",
    "You still need to set your `LANGSMITH_API_KEY` and `LANGSMITH_TRACING`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AWTT](../../images/alternative_ways_to_trace_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mistral AI Integration with LangSmith Tracing\n",
    "# Using @traceable decorator with Mistral AI ChatMistralAI client\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "from utils import get_vector_db_retriever\n",
    "\n",
    "MODEL_PROVIDER = \"mistral\"\n",
    "MODEL_NAME = \"mistral-small-latest\"\n",
    "APP_VERSION = 1.0\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "\n",
    "# Use Mistral AI client directly\n",
    "mistral_client = ChatMistralAI(model=MODEL_NAME)\n",
    "\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": RAG_SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    return call_mistral(messages)\n",
    "\n",
    "@traceable\n",
    "def call_mistral(\n",
    "    messages: List[dict],\n",
    ") -> str:\n",
    "    from langchain_core.messages import HumanMessage, SystemMessage\n",
    "    \n",
    "    # Convert dict messages to LangChain message objects\n",
    "    langchain_messages = []\n",
    "    for msg in messages:\n",
    "        if msg[\"role\"] == \"system\":\n",
    "            langchain_messages.append(SystemMessage(content=msg[\"content\"]))\n",
    "        elif msg[\"role\"] == \"user\":\n",
    "            langchain_messages.append(HumanMessage(content=msg[\"content\"]))\n",
    "    \n",
    "    return mistral_client.invoke(langchain_messages)\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def langsmith_rag_with_mistral(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How can I optimize costs when using Mistral AI in production?\n",
      "Answer: To optimize costs when using Mistral AI in production, you can start by setting limits on production usage, particularly focusing on the total traces limit based on expected load and potential growth. Additionally, you can cut maximum spend by setting a limit on the maximum high retention traces you want to keep for more than 14 days, such as keeping only 10% of traces.\n",
      "Response time: 1.19 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "question = \"How can I optimize costs when using Mistral AI in production?\"\n",
    "start_time = time.time()\n",
    "ai_answer = langsmith_rag_with_mistral(question)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {ai_answer}\")\n",
    "print(f\"Response time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mistral AI client with @traceable decorator accepts all the same langsmith_extra parameters as other @traceable decorated functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing multiple queries with Mistral AI:\n",
      "\n",
      "1. Q: What is the weather like today?\n",
      "   A: I can't provide real-time weather updates, but you can check the current weather in your location us...\n",
      "\n",
      "1. Q: What is the weather like today?\n",
      "   A: I can't provide real-time weather updates, but you can check the current weather in your location us...\n",
      "\n",
      "2. Q: Explain quantum computing in simple terms\n",
      "   A: Quantum computing is like a super-powered version of regular computers, but instead of using bits (w...\n",
      "\n",
      "2. Q: Explain quantum computing in simple terms\n",
      "   A: Quantum computing is like a super-powered version of regular computers, but instead of using bits (w...\n",
      "\n",
      "3. Q: How do neural networks learn?\n",
      "   A: Neural networks learn through a process called **training**, which involves adjusting their internal...\n",
      "\n",
      "3. Q: How do neural networks learn?\n",
      "   A: Neural networks learn through a process called **training**, which involves adjusting their internal...\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Test multiple custom questions\n",
    "custom_questions = [\n",
    "    \"What is the weather like today?\",\n",
    "    \"Explain quantum computing in simple terms\",\n",
    "    \"How do neural networks learn?\"\n",
    "]\n",
    "\n",
    "print(\"Testing multiple queries with Mistral AI:\")\n",
    "for i, question_text in enumerate(custom_questions, 1):\n",
    "    message = HumanMessage(content=question_text)\n",
    "    response = mistral_client.invoke([message])\n",
    "    print(f\"\\n{i}. Q: {question_text}\")\n",
    "    print(f\"   A: {response.content[:100]}{'...' if len(response.content) > 100 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## [Advanced] RunTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another, more explicit way to log traces to LangSmith is via the RunTree API. This API allows you more control over your tracing - you can manually create runs and children runs to assemble your trace. You still need to set your `LANGSMITH_API_KEY`, but `LANGSMITH_TRACING` is not necessary for this method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AWTT](../../images/alternative_ways_to_trace_4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MISTRAL_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "# I have my env variables defined in a .env file\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and set `LANGSMITH_TRACING` to false, as we are using RunTree to manually create runs in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable automatic tracing for RunTree demonstration\n",
    "import os\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have rewritten our RAG application, except this time we pass a RunTree argument through our function calls, and create child runs at each layer. This gives our RunTree the same hierarchy that we were automatically able to establish with @traceable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import RunTree\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "from utils import get_vector_db_retriever\n",
    "\n",
    "mistral_client = ChatMistralAI(model=\"mistral-small-latest\")\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "def retrieve_documents(parent_run: RunTree, question: str):\n",
    "    # Create a child run\n",
    "    child_run = parent_run.create_child(\n",
    "        name=\"Retrieve Documents\",\n",
    "        run_type=\"retriever\",\n",
    "        inputs={\"question\": question},\n",
    "    )\n",
    "    documents = retriever.invoke(question)\n",
    "    # Post the output of our child run\n",
    "    child_run.end(outputs={\"documents\": documents})\n",
    "    child_run.post()\n",
    "    return documents\n",
    "\n",
    "def generate_response(parent_run: RunTree, question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    rag_system_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise.\n",
    "    \"\"\"\n",
    "    # Create a child run\n",
    "    child_run = parent_run.create_child(\n",
    "        name=\"Generate Response\",\n",
    "        run_type=\"chain\",\n",
    "        inputs={\"question\": question, \"documents\": documents},\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": rag_system_prompt\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    mistral_response = call_mistral(child_run, messages)\n",
    "    # Post the output of our child run\n",
    "    child_run.end(outputs={\"mistral_response\": mistral_response})\n",
    "    child_run.post()\n",
    "    return mistral_response\n",
    "\n",
    "def call_mistral(\n",
    "    parent_run: RunTree, messages: List[dict], model: str = \"mistral-small-latest\", temperature: float = 0.0\n",
    ") -> str:\n",
    "    # Create a child run\n",
    "    child_run = parent_run.create_child(\n",
    "        name=\"Mistral AI Call\",\n",
    "        run_type=\"llm\",\n",
    "        inputs={\"messages\": messages},\n",
    "    )\n",
    "    \n",
    "    # Convert dict messages to LangChain message objects\n",
    "    langchain_messages = []\n",
    "    for msg in messages:\n",
    "        if msg[\"role\"] == \"system\":\n",
    "            langchain_messages.append(SystemMessage(content=msg[\"content\"]))\n",
    "        elif msg[\"role\"] == \"user\":\n",
    "            langchain_messages.append(HumanMessage(content=msg[\"content\"]))\n",
    "    \n",
    "    mistral_response = mistral_client.invoke(langchain_messages)\n",
    "    # Post the output of our child run\n",
    "    child_run.end(outputs={\"mistral_response\": mistral_response})\n",
    "    child_run.post()\n",
    "    return mistral_response\n",
    "\n",
    "def langsmith_rag(question: str):\n",
    "    # Create a root RunTree\n",
    "    root_run_tree = RunTree(\n",
    "        name=\"Chat Pipeline\",\n",
    "        run_type=\"chain\",\n",
    "        inputs={\"question\": question}\n",
    "    )\n",
    "\n",
    "    # Pass our RunTree into the nested function calls\n",
    "    documents = retrieve_documents(root_run_tree, question)\n",
    "    response = generate_response(root_run_tree, question, documents)\n",
    "    output = response.content\n",
    "\n",
    "    # Post our final output\n",
    "    root_run_tree.end(outputs={\"generation\": output})\n",
    "    root_run_tree.post()\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can convert LangChain’s RunnableConfig to an equivalent RunTree object using `RunTree.fromRunnableConfig`. Alternatively, you can pass the RunnableConfig as the first argument of the traceable-wrapped function.\n",
      "Custom RunTree Demo - Question: What are the key components of a scalable RAG system architecture?\n",
      "Answer: The key components of a scalable RAG system architecture include:\n",
      "\n",
      "- **ClickHouse** for data warehousing, with recommendations for resource allocation and replication.\n",
      "- **Redis** for caching, with a minimum size of 200 GB for optimal performance.\n",
      "- **Autoscaling** capabilities for various services like platformBackend, queue, and backend to handle varying loads efficiently.\n",
      "\n",
      "Second question: Explain the difference between vector databases and traditional databases\n",
      "Answer: The key components of a scalable RAG system architecture include:\n",
      "\n",
      "- **ClickHouse** for data warehousing, with recommendations for resource allocation and replication.\n",
      "- **Redis** for caching, with a minimum size of 200 GB for optimal performance.\n",
      "- **Autoscaling** capabilities for various services like platformBackend, queue, and backend to handle varying loads efficiently.\n",
      "\n",
      "Second question: Explain the difference between vector databases and traditional databases\n",
      "Answer: Vector databases store data as vectors, which are mathematical representations of information, allowing for similarity searches and machine learning tasks. Traditional databases store data in tables with rows and columns, optimized for structured query operations. Vector databases are particularly useful for tasks like recommendation systems and image recognition, while traditional databases are better suited for transactional and analytical processing.\n",
      "Answer: Vector databases store data as vectors, which are mathematical representations of information, allowing for similarity searches and machine learning tasks. Traditional databases store data in tables with rows and columns, optimized for structured query operations. Vector databases are particularly useful for tasks like recommendation systems and image recognition, while traditional databases are better suited for transactional and analytical processing.\n"
     ]
    }
   ],
   "source": [
    "question = \"How can I trace with RunTree?\"\n",
    "ai_answer = langsmith_rag(question)\n",
    "print(ai_answer)\n",
    "\n",
    "question = \"What are the key components of a scalable RAG system architecture?\"\n",
    "print(f\"Custom RunTree Demo - Question: {question}\")\n",
    "ai_answer = langsmith_rag(question)\n",
    "print(f\"Answer: {ai_answer}\")\n",
    "\n",
    "# Additional custom example with error handling\n",
    "try:\n",
    "    question2 = \"Explain the difference between vector databases and traditional databases\"\n",
    "    print(f\"\\nSecond question: {question2}\")\n",
    "    ai_answer2 = langsmith_rag(question2)\n",
    "    print(f\"Answer: {ai_answer2}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook was such a deep dive into the different ways you can set up tracing - it's like having multiple tools in your toolbox for different situations! I learned that there's no one-size-fits-all approach to tracing, and each method has its sweet spot depending on what you're building.\n",
    "\n",
    "**What I Discovered and Changed:**\n",
    "- Migrated everything from OpenAI to Mistral AI across all the different tracing methods - @traceable, LangGraph, context managers, and RunTree\n",
    "- Updated all the API keys (MISTRAL_API_KEY replaced OPENAI_API_KEY everywhere) and made sure the environment setup worked smoothly\n",
    "- Played around with LangGraph integration and saw how it automatically traces everything when you just set environment variables - that's pretty magical!\n",
    "- Explored the context manager approach with `with trace()` which gives you super fine-grained control over what gets logged and when\n",
    "- Dove deep into the RunTree API and manually created parent-child run relationships - it's like being the architect of your own trace structure\n",
    "- Added custom experiments showing how Mistral AI works seamlessly with all these different tracing approaches\n",
    "- Tested multiple scenarios including error handling and performance monitoring across different tracing methods\n",
    "\n",
    "I also realized that each tracing method serves different needs. LangGraph is perfect when you want zero-hassle automatic tracing, @traceable is great for most everyday use cases, context managers give you surgical precision, and RunTree lets you build exactly the trace structure you want. Plus, switching from OpenAI to Mistral AI was smooth as butter across all methods - the tracing infrastructure just doesn't care which AI provider you use, which is incredibly powerful for flexibility!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mat496",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
