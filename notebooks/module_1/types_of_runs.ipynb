{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracing for Different Types of Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangSmith supports many different types of Runs - you can specify what type your Run is in the @traceable decorator. The types of runs are:\n",
    "\n",
    "- LLM: Invokes an LLM\n",
    "- Retriever: Retrieves documents from databases or other sources\n",
    "- Tool: Executes actions with function calls\n",
    "- Chain: Default type; combines multiple Runs into a larger process\n",
    "- Prompt: Hydrates a prompt to be used with an LLM\n",
    "- Parser: Extracts structured data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can set them inline!\n",
    "import os\n",
    "os.environ[\"MISTRAL_API_KEY\"] = \"MISTRAL_API_KEY\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"LANGSMITH_API_KEY\"\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or you can use a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Runs for Chat Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangSmith provides special rendering and processing for LLM traces. In order to make the most of this feature, you must log your LLM traces in a specific format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For chat-style models, inputs must be a list of messages in OpenAI-compatible format, represented as Python dictionaries or TypeScript object. Each message must contain the key role and content.\n",
    "\n",
    "The output is accepted in any of the following formats:\n",
    "\n",
    "- A dictionary/object that contains the key choices with a value that is a list of dictionaries/objects. Each dictionary/object must contain the key message, which maps to a message object with the keys role and content.\n",
    "- A dictionary/object that contains the key message with a value that is a message object with the keys role and content.\n",
    "- A tuple/array of two elements, where the first element is the role and the second element is the content.\n",
    "- A dictionary/object that contains the key role and content.\n",
    "The input to your function should be named messages.\n",
    "\n",
    "You can also provide the following metadata fields to help LangSmith identify the model and calculate costs. If using LangChain or OpenAI wrapper, these fields will be automatically populated correctly.\n",
    "- ls_provider: The provider of the model, eg \"openai\", \"anthropic\", etc.\n",
    "- ls_model_name: The name of the model, eg \"gpt-4o-mini\", \"claude-3-opus-20240307\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2 messages with Mistral AI\n",
      "Chat model result: I'd be happy to help you with a reservation! What time would you prefer for your table for two tonight? We have availability between 6:00 PM and 9:00 PM.\n"
     ]
    }
   ],
   "source": [
    "from langsmith import traceable\n",
    "\n",
    "inputs = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a helpful AI assistant specializing in customer service.\"},\n",
    "  {\"role\": \"user\", \"content\": \"I'd like to book a table for two at your restaurant for tonight.\"},\n",
    "]\n",
    "\n",
    "output = {\n",
    "  \"choices\": [\n",
    "      {\n",
    "          \"message\": {\n",
    "              \"role\": \"assistant\",\n",
    "              \"content\": \"I'd be happy to help you with a reservation! What time would you prefer for your table for two tonight? We have availability between 6:00 PM and 9:00 PM.\"\n",
    "          }\n",
    "      }\n",
    "  ]\n",
    "}\n",
    "\n",
    "# Can also use one of these formats:\n",
    "# output = {\n",
    "#     \"message\": {\n",
    "#         \"role\": \"assistant\",\n",
    "#         \"content\": \"I'd be happy to help you with a reservation! What time would you prefer?\"\n",
    "#     }\n",
    "# }\n",
    "#\n",
    "# output = {\n",
    "#     \"role\": \"assistant\",\n",
    "#     \"content\": \"I'd be happy to help you with a reservation! What time would you prefer?\"\n",
    "# }\n",
    "#\n",
    "# output = [\"assistant\", \"I'd be happy to help you with a reservation! What time would you prefer?\"]\n",
    "\n",
    "@traceable(\n",
    "    run_type=\"llm\",\n",
    "    metadata={\n",
    "        \"ls_provider\": \"mistral\",\n",
    "        \"ls_model_name\": \"mistral-small-latest\",\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens\": 150,\n",
    "        \"use_case\": \"customer_service\"\n",
    "    }\n",
    ")\n",
    "def chat_model(messages: list):\n",
    "    # Simulate Mistral AI chat model response\n",
    "    print(f\"Processing {len(messages)} messages with Mistral AI\")\n",
    "    return output\n",
    "\n",
    "result = chat_model(inputs)\n",
    "print(f\"Chat model result: {result['choices'][0]['message']['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Streaming LLM Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For streaming, you can \"reduce\" the outputs into the same format as the non-streaming version. This is currently only supported in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming Mistral AI Response:\n",
      "Generated 5 chunks\n",
      "Final combined response: Hello there, Data Scientist! I'm Mistral AI, and I'm here to help you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "def _reduce_chunks(chunks: list):\n",
    "    \"\"\"Combine streaming chunks into final response format\"\"\"\n",
    "    all_text = \"\".join([chunk[\"choices\"][0][\"message\"][\"content\"] for chunk in chunks])\n",
    "    return {\"choices\": [{\"message\": {\"content\": all_text, \"role\": \"assistant\"}}]}\n",
    "\n",
    "@traceable(\n",
    "    run_type=\"llm\",\n",
    "    metadata={\n",
    "        \"ls_provider\": \"mistral\", \n",
    "        \"ls_model_name\": \"mistral-small-latest\",\n",
    "        \"streaming\": True,\n",
    "        \"chunk_size\": \"dynamic\"\n",
    "    },\n",
    "    reduce_fn=_reduce_chunks\n",
    ")\n",
    "def my_streaming_mistral_model(messages: list):\n",
    "    \"\"\"Simulate Mistral AI streaming response\"\"\"\n",
    "    user_name = messages[1][\"content\"] if len(messages) > 1 else \"friend\"\n",
    "    \n",
    "    # Simulate streaming chunks\n",
    "    chunks = [\n",
    "        \"Hello there, \",\n",
    "        f\"{user_name}! \",\n",
    "        \"I'm Mistral AI, \",\n",
    "        \"and I'm here to help you. \",\n",
    "        \"How can I assist you today?\"\n",
    "    ]\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        yield {\n",
    "            \"choices\": [\n",
    "                {\n",
    "                    \"message\": {\n",
    "                        \"content\": chunk,\n",
    "                        \"role\": \"assistant\",\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "# Test streaming with custom input\n",
    "streaming_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Mistral AI, a helpful assistant. Please greet the user warmly.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Data Scientist\"},\n",
    "]\n",
    "\n",
    "print(\"Streaming Mistral AI Response:\")\n",
    "streaming_result = list(my_streaming_mistral_model(streaming_messages))\n",
    "print(f\"Generated {len(streaming_result)} chunks\")\n",
    "print(f\"Final combined response: {_reduce_chunks(streaming_result)['choices'][0]['message']['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever Runs + Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many LLM applications require looking up documents from vector databases, knowledge graphs, or other types of indexes. Retriever traces are a way to log the documents that are retrieved by the retriever. LangSmith provides special rendering for retrieval steps in traces to make it easier to understand and diagnose retrieval issues. In order for retrieval steps to be rendered correctly, a few small steps need to be taken.\n",
    "\n",
    "1. Annotate the retriever step with run_type=\"retriever\".\n",
    "2. Return a list of Python dictionaries or TypeScript objects from the retriever step. Each dictionary should contain the following keys:\n",
    "    - page_content: The text of the document.\n",
    "    - type: This should always be \"Document\".\n",
    "    - metadata: A python dictionary or TypeScript object containing metadata about the document. This metadata will be displayed in the trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing query: 'model evaluation techniques' ---\n",
      "Retrieving documents for query: 'model evaluation techniques'\n",
      "Retrieved 3 documents\n",
      "Doc 1: Machine learning models require careful evaluation using met...\n",
      "Doc 2: Model deployment involves containerization, API creation, an...\n",
      "Doc 3: Model versioning is crucial for tracking experiments and mai...\n",
      "Metadata for first doc: {'source': 'knowledge_base_doc_1', 'relevance_score': 0.95, 'document_type': 'technical_guide', 'query_used': 'model evaluation techniques', 'retrieval_timestamp': '2025-10-04T12:00:00Z'}\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Testing query: 'data preprocessing steps' ---\n",
      "Retrieving documents for query: 'data preprocessing steps'\n",
      "Retrieved 3 documents\n",
      "Doc 1: Data preprocessing includes cleaning, normalization, and fea...\n",
      "Doc 2: Data quality assessment involves checking for missing values...\n",
      "Doc 3: Data pipeline automation ensures consistent and reliable dat...\n",
      "Metadata for first doc: {'source': 'knowledge_base_doc_1', 'relevance_score': 0.95, 'document_type': 'technical_guide', 'query_used': 'data preprocessing steps', 'retrieval_timestamp': '2025-10-04T12:00:00Z'}\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Testing query: 'production deployment' ---\n",
      "Retrieving documents for query: 'production deployment'\n",
      "Retrieved 3 documents\n",
      "Doc 1: General ML documentation covering best practices and impleme...\n",
      "Doc 2: Production ML systems require monitoring, logging, and autom...\n",
      "Doc 3: MLOps practices integrate development and operations for eff...\n",
      "Metadata for first doc: {'source': 'knowledge_base_doc_1', 'relevance_score': 0.95, 'document_type': 'technical_guide', 'query_used': 'production deployment', 'retrieval_timestamp': '2025-10-04T12:00:00Z'}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langsmith import traceable\n",
    "\n",
    "def _convert_docs(results, query_metadata=None):\n",
    "    \"\"\"Convert retrieved results to proper document format\"\"\"\n",
    "    return [\n",
    "        {\n",
    "            \"page_content\": r,\n",
    "            \"type\": \"Document\",  # Fixed: This should be \"type\", not \"Document\"\n",
    "            \"metadata\": {\n",
    "                \"source\": f\"knowledge_base_doc_{i+1}\",\n",
    "                \"relevance_score\": 0.95 - (i * 0.1),\n",
    "                \"document_type\": \"technical_guide\",\n",
    "                \"query_used\": query_metadata.get(\"query\", \"\") if query_metadata else \"\",\n",
    "                \"retrieval_timestamp\": \"2025-10-04T12:00:00Z\"\n",
    "            }\n",
    "        }\n",
    "        for i, r in enumerate(results)\n",
    "    ]\n",
    "\n",
    "@traceable(\n",
    "    run_type=\"retriever\",\n",
    "    metadata={\n",
    "        \"retriever_type\": \"vector_similarity\",\n",
    "        \"embedding_model\": \"huggingface\",\n",
    "        \"top_k\": 3,\n",
    "        \"similarity_threshold\": 0.8\n",
    "    }\n",
    ")\n",
    "def retrieve_ml_docs(query):\n",
    "    \"\"\"Enhanced retriever for ML-related documents\"\"\"\n",
    "    print(f\"Retrieving documents for query: '{query}'\")\n",
    "    \n",
    "    # Simulate ML-focused document retrieval\n",
    "    if \"model\" in query.lower():\n",
    "        contents = [\n",
    "            \"Machine learning models require careful evaluation using metrics like accuracy, precision, and recall.\",\n",
    "            \"Model deployment involves containerization, API creation, and monitoring systems for production.\",\n",
    "            \"Model versioning is crucial for tracking experiments and maintaining reproducible results.\"\n",
    "        ]\n",
    "    elif \"data\" in query.lower():\n",
    "        contents = [\n",
    "            \"Data preprocessing includes cleaning, normalization, and feature engineering steps.\",\n",
    "            \"Data quality assessment involves checking for missing values, outliers, and data consistency.\",\n",
    "            \"Data pipeline automation ensures consistent and reliable data flow for ML systems.\"\n",
    "        ]\n",
    "    else:\n",
    "        contents = [\n",
    "            \"General ML documentation covering best practices and implementation guidelines.\",\n",
    "            \"Production ML systems require monitoring, logging, and automated testing capabilities.\",\n",
    "            \"MLOps practices integrate development and operations for efficient ML lifecycle management.\"\n",
    "        ]\n",
    "    \n",
    "    query_metadata = {\"query\": query, \"total_results\": len(contents)}\n",
    "    retrieved_docs = _convert_docs(contents, query_metadata)\n",
    "    \n",
    "    print(f\"Retrieved {len(retrieved_docs)} documents\")\n",
    "    for i, doc in enumerate(retrieved_docs, 1):\n",
    "        print(f\"Doc {i}: {doc['page_content'][:60]}...\")\n",
    "    \n",
    "    return retrieved_docs\n",
    "\n",
    "# Test retriever with different queries\n",
    "test_queries = [\"model evaluation techniques\", \"data preprocessing steps\", \"production deployment\"]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n--- Testing query: '{query}' ---\")\n",
    "    docs = retrieve_ml_docs(query)\n",
    "    print(f\"Metadata for first doc: {docs[0]['metadata']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangSmith has custom rendering for Tool Calls made by the model to make it clear when provided tools are being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Testing: Current Weather Query\n",
      "============================================================\n",
      "Starting enhanced weather assistant...\n",
      "Calling Mistral AI with 2 messages and 2 tools\n",
      "Error occurred: 'dict' object has no attribute 'choices'\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "Testing: Detailed Weather Query\n",
      "============================================================\n",
      "Starting enhanced weather assistant...\n",
      "Calling Mistral AI with 2 messages and 2 tools\n",
      "Error occurred: 'dict' object has no attribute 'choices'\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langsmith import traceable\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, ToolMessage\n",
    "from typing import List, Optional, Dict, Any\n",
    "import json\n",
    "import random\n",
    "\n",
    "mistral_client = ChatMistralAI(model=\"mistral-small-latest\")\n",
    "\n",
    "@traceable(\n",
    "    run_type=\"tool\",\n",
    "    metadata={\n",
    "        \"tool_name\": \"get_current_temperature\",\n",
    "        \"tool_category\": \"weather\",\n",
    "        \"response_format\": \"json\"\n",
    "    }\n",
    ")\n",
    "def get_current_temperature(location: str, unit: str):\n",
    "    \"\"\"Enhanced temperature tool with realistic simulation\"\"\"\n",
    "    print(f\"Getting temperature for {location} in {unit}\")\n",
    "    \n",
    "    # Simulate realistic temperature data\n",
    "    base_temps = {\n",
    "        \"New York\": {\"F\": 68, \"C\": 20},\n",
    "        \"London\": {\"F\": 59, \"C\": 15},\n",
    "        \"Tokyo\": {\"F\": 73, \"C\": 23},\n",
    "        \"Sydney\": {\"F\": 77, \"C\": 25}\n",
    "    }\n",
    "    \n",
    "    # Find closest match or use default\n",
    "    city_key = next((k for k in base_temps.keys() if k.lower() in location.lower()), \"New York\")\n",
    "    unit_key = \"F\" if unit == \"Fahrenheit\" else \"C\"\n",
    "    \n",
    "    # Add some realistic variation\n",
    "    base_temp = base_temps[city_key][unit_key]\n",
    "    actual_temp = base_temp + random.randint(-5, 5)\n",
    "    \n",
    "    return {\n",
    "        \"temperature\": actual_temp,\n",
    "        \"location\": location,\n",
    "        \"unit\": unit,\n",
    "        \"conditions\": random.choice([\"sunny\", \"partly cloudy\", \"overcast\", \"light rain\"]),\n",
    "        \"humidity\": random.randint(40, 80)\n",
    "    }\n",
    "\n",
    "@traceable(\n",
    "    run_type=\"tool\",\n",
    "    metadata={\n",
    "        \"tool_name\": \"get_weather_forecast\",\n",
    "        \"tool_category\": \"weather\",\n",
    "        \"forecast_days\": 3\n",
    "    }\n",
    ")\n",
    "def get_weather_forecast(location: str, days: int = 3):\n",
    "    \"\"\"Additional weather tool for extended forecasting\"\"\"\n",
    "    print(f\"Getting {days}-day forecast for {location}\")\n",
    "    \n",
    "    forecast = []\n",
    "    for day in range(days):\n",
    "        temp_f = random.randint(60, 85)\n",
    "        forecast.append({\n",
    "            \"day\": f\"Day {day + 1}\",\n",
    "            \"temperature_f\": temp_f,\n",
    "            \"temperature_c\": round((temp_f - 32) * 5/9),\n",
    "            \"condition\": random.choice([\"sunny\", \"cloudy\", \"rainy\", \"partly cloudy\"])\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        \"location\": location,\n",
    "        \"forecast\": forecast,\n",
    "        \"generated_at\": \"2025-10-04T12:00:00Z\"\n",
    "    }\n",
    "\n",
    "@traceable(\n",
    "    run_type=\"llm\",\n",
    "    metadata={\n",
    "        \"ls_provider\": \"mistral\",\n",
    "        \"ls_model_name\": \"mistral-small-latest\",\n",
    "        \"supports_tools\": True\n",
    "    }\n",
    ")\n",
    "def call_mistral_with_tools(messages: List[dict], tools: Optional[List[dict]] = None):\n",
    "    \"\"\"Simulate Mistral AI with tool calling capability\"\"\"\n",
    "    print(f\"Calling Mistral AI with {len(messages)} messages and {len(tools) if tools else 0} tools\")\n",
    "    \n",
    "    # Simulate tool calling logic\n",
    "    user_message = next((msg[\"content\"] for msg in messages if msg[\"role\"] == \"user\"), \"\")\n",
    "    \n",
    "    if \"temperature\" in user_message.lower() or \"weather\" in user_message.lower():\n",
    "        # Extract location from user message (simplified)\n",
    "        location = \"New York City\"  # Default location\n",
    "        if \"london\" in user_message.lower():\n",
    "            location = \"London\"\n",
    "        elif \"tokyo\" in user_message.lower():  \n",
    "            location = \"Tokyo\"\n",
    "        elif \"sydney\" in user_message.lower():\n",
    "            location = \"Sydney\"\n",
    "        \n",
    "        # Simulate tool call response\n",
    "        return {\n",
    "            \"choices\": [{\n",
    "                \"message\": {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": None,\n",
    "                    \"tool_calls\": [{\n",
    "                        \"id\": f\"call_{random.randint(1000, 9999)}\",\n",
    "                        \"type\": \"function\",  \n",
    "                        \"function\": {\n",
    "                            \"name\": \"get_current_temperature\",\n",
    "                            \"arguments\": json.dumps({\n",
    "                                \"location\": location,\n",
    "                                \"unit\": \"Fahrenheit\"\n",
    "                            })\n",
    "                        }\n",
    "                    }]\n",
    "                }\n",
    "            }]\n",
    "        }\n",
    "    \n",
    "    # Regular response without tools\n",
    "    return {\n",
    "        \"choices\": [{\n",
    "            \"message\": {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"I'd be happy to help! However, I need more specific information to assist you properly.\"\n",
    "            }\n",
    "        }]\n",
    "    }\n",
    "\n",
    "@traceable(\n",
    "    run_type=\"chain\",\n",
    "    metadata={\n",
    "        \"pipeline_name\": \"weather_assistant\",\n",
    "        \"supports_tools\": True,\n",
    "        \"version\": \"2.0\"\n",
    "    }\n",
    ")\n",
    "def enhanced_weather_assistant(inputs, tools):\n",
    "    \"\"\"Enhanced weather assistant with better error handling and multiple tools\"\"\"\n",
    "    try:\n",
    "        print(\"Starting enhanced weather assistant...\")\n",
    "        \n",
    "        # First call to Mistral AI\n",
    "        response = call_mistral_with_tools(inputs, tools)\n",
    "        \n",
    "        if response.choices[0].message.tool_calls:\n",
    "            # Process tool calls\n",
    "            tool_call = response.choices[0].message.tool_calls[0]\n",
    "            function_name = tool_call.function.name\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            \n",
    "            print(f\"Tool called: {function_name} with args: {function_args}\")\n",
    "            \n",
    "            # Execute the appropriate tool\n",
    "            if function_name == \"get_current_temperature\":\n",
    "                tool_result = get_current_temperature(**function_args)\n",
    "            elif function_name == \"get_weather_forecast\":\n",
    "                tool_result = get_weather_forecast(**function_args)\n",
    "            else:\n",
    "                tool_result = {\"error\": f\"Unknown function: {function_name}\"}\n",
    "            \n",
    "            # Create tool response message\n",
    "            tool_response_message = {\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": json.dumps(tool_result),\n",
    "                \"tool_call_id\": tool_call.id\n",
    "            }\n",
    "            \n",
    "            # Add assistant message and tool response to conversation\n",
    "            inputs.append(response.choices[0].message)\n",
    "            inputs.append(tool_response_message)\n",
    "            \n",
    "            # Second call to generate final response\n",
    "            final_response = call_mistral_with_tools(inputs, None)\n",
    "            \n",
    "            return {\n",
    "                \"final_response\": final_response,\n",
    "                \"tool_used\": function_name,\n",
    "                \"tool_result\": tool_result,\n",
    "                \"conversation_length\": len(inputs)\n",
    "            }\n",
    "        \n",
    "        else:\n",
    "            return {\n",
    "                \"final_response\": response,\n",
    "                \"tool_used\": None,\n",
    "                \"tool_result\": None,\n",
    "                \"conversation_length\": len(inputs)\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"error\": str(e),\n",
    "            \"conversation_length\": len(inputs)\n",
    "        }\n",
    "\n",
    "# Enhanced tools definition\n",
    "enhanced_tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_temperature\",\n",
    "            \"description\": \"Get the current temperature and weather conditions for a specific location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state/country, e.g., San Francisco, CA or London, UK\"\n",
    "                    },\n",
    "                    \"unit\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"Celsius\", \"Fahrenheit\"],\n",
    "                        \"description\": \"The temperature unit to use. Default to Fahrenheit for US locations, Celsius for others.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"location\", \"unit\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\", \n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather_forecast\",\n",
    "            \"description\": \"Get multi-day weather forecast for a location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state/country for the forecast\"\n",
    "                    },\n",
    "                    \"days\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"Number of days to forecast (1-7)\",\n",
    "                        \"minimum\": 1,\n",
    "                        \"maximum\": 7\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"location\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Test with multiple scenarios\n",
    "test_scenarios = [\n",
    "    {\n",
    "        \"name\": \"Current Weather Query\",\n",
    "        \"inputs\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful weather assistant with access to current weather data.\"},\n",
    "            {\"role\": \"user\", \"content\": \"What's the current temperature in Tokyo?\"},\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Detailed Weather Query\", \n",
    "        \"inputs\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful weather assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Can you tell me about the weather in London today? I'm planning outdoor activities.\"},\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "for scenario in test_scenarios:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing: {scenario['name']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    result = enhanced_weather_assistant(scenario[\"inputs\"], enhanced_tools)\n",
    "    \n",
    "    if \"error\" not in result:\n",
    "        print(f\"Tool used: {result['tool_used']}\")\n",
    "        if result['tool_result']:\n",
    "            print(f\"Tool result: {result['tool_result']}\")\n",
    "        print(f\"Conversation length: {result['conversation_length']} messages\")\n",
    "    else:\n",
    "        print(f\"Error occurred: {result['error']}\")\n",
    "    \n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Run Type Experiments\n",
    "\n",
    "Let's explore advanced scenarios with different run types including Prompt, Parser, and Chain runs in production-like settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing ML Evaluation Prompt Generation:\n",
      "==================================================\n",
      "Generated Prompt:\n",
      "You are an expert ML engineer evaluating a Random Forest Classifier model trained on customer_churn_data.\n",
      "\n",
      "    Context: production environment\n",
      "\n",
      "    Please analyze the following metrics and provide insights:\n",
      "    Metrics to evaluate: accuracy, precision, recall, f1-score, auc-roc\n",
      "\n",
      "    Your evaluation should include:\n",
      "    1. Performance assessment\n",
      "    2. Potential issues or concerns  \n",
      "    3. Recommendations for improvement\n",
      "    4. Production readiness assessment\n",
      "\n",
      "    Be specific and actionable in your recommendations.\n",
      "\n",
      "Prompt metadata: {'template_used': 'ml_evaluation_prompt', 'parameters': {'model_type': 'Random Forest Classifier', 'dataset': 'customer_churn_data', 'metrics': ['accuracy', 'precision', 'recall', 'f1-score', 'auc-roc'], 'context': 'production'}, 'prompt_length': 528, 'generated_at': '2025-10-04T17:30:48.960003'}\n",
      "\n",
      "==================================================\n",
      "Testing Data Analysis Prompt Generation:\n",
      "==================================================\n",
      "Generated Prompt:\n",
      "As a senior data scientist, please perform a exploratory analysis on data from sales_transactions_db.\n",
      "\n",
      "    Analysis Objectives:\n",
      "    - Identify seasonal patterns in sales\n",
      "- Analyze customer segmentation opportunities\n",
      "- Detect anomalies in transaction patterns\n",
      "\n",
      "    Please provide:\n",
      "    - Key findings and insights\n",
      "    - Statistical significance of results\n",
      "    - Visualization recommendations\n",
      "    - Next steps for investigation\n",
      "\n",
      "    Focus on actionable insights that can drive business decisions.\n",
      "\n",
      "Parameters used: {'analysis_type': 'exploratory', 'data_source': 'sales_transactions_db', 'objectives_count': 3}\n"
     ]
    }
   ],
   "source": [
    "# Custom Experiment 1: Prompt Run Type with Template Management\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "@traceable(\n",
    "    run_type=\"prompt\",\n",
    "    metadata={\n",
    "        \"prompt_template\": \"ml_evaluation_prompt\",\n",
    "        \"version\": \"1.2\",\n",
    "        \"parameters\": [\"model_type\", \"dataset\", \"metrics\", \"context\"]\n",
    "    }\n",
    ")\n",
    "def create_ml_evaluation_prompt(model_type: str, dataset: str, metrics: list, context: str = \"production\"):\n",
    "    \"\"\"Create specialized prompts for ML model evaluation\"\"\"\n",
    "    \n",
    "    template = \"\"\"\n",
    "    You are an expert ML engineer evaluating a {model_type} model trained on {dataset}.\n",
    "    \n",
    "    Context: {context} environment\n",
    "    \n",
    "    Please analyze the following metrics and provide insights:\n",
    "    Metrics to evaluate: {metrics_list}\n",
    "    \n",
    "    Your evaluation should include:\n",
    "    1. Performance assessment\n",
    "    2. Potential issues or concerns  \n",
    "    3. Recommendations for improvement\n",
    "    4. Production readiness assessment\n",
    "    \n",
    "    Be specific and actionable in your recommendations.\n",
    "    \"\"\"\n",
    "    \n",
    "    metrics_list = \", \".join(metrics) if isinstance(metrics, list) else str(metrics)\n",
    "    \n",
    "    prompt = template.format(\n",
    "        model_type=model_type,\n",
    "        dataset=dataset,\n",
    "        context=context,\n",
    "        metrics_list=metrics_list\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": prompt.strip(),\n",
    "        \"metadata\": {\n",
    "            \"template_used\": \"ml_evaluation_prompt\",\n",
    "            \"parameters\": {\n",
    "                \"model_type\": model_type,\n",
    "                \"dataset\": dataset,\n",
    "                \"metrics\": metrics,\n",
    "                \"context\": context\n",
    "            },\n",
    "            \"prompt_length\": len(prompt),\n",
    "            \"generated_at\": datetime.now().isoformat()\n",
    "        }\n",
    "    }\n",
    "\n",
    "@traceable(\n",
    "    run_type=\"prompt\", \n",
    "    metadata={\n",
    "        \"prompt_template\": \"data_analysis_prompt\",\n",
    "        \"use_case\": \"data_science\"\n",
    "    }\n",
    ")\n",
    "def create_data_analysis_prompt(analysis_type: str, data_source: str, objectives: list):\n",
    "    \"\"\"Create prompts for data analysis tasks\"\"\"\n",
    "    \n",
    "    template = \"\"\"\n",
    "    As a senior data scientist, please perform a {analysis_type} analysis on data from {data_source}.\n",
    "    \n",
    "    Analysis Objectives:\n",
    "    {objectives_list}\n",
    "    \n",
    "    Please provide:\n",
    "    - Key findings and insights\n",
    "    - Statistical significance of results\n",
    "    - Visualization recommendations\n",
    "    - Next steps for investigation\n",
    "    \n",
    "    Focus on actionable insights that can drive business decisions.\n",
    "    \"\"\"\n",
    "    \n",
    "    objectives_list = \"\\n\".join([f\"- {obj}\" for obj in objectives])\n",
    "    \n",
    "    prompt = template.format(\n",
    "        analysis_type=analysis_type,\n",
    "        data_source=data_source,\n",
    "        objectives_list=objectives_list\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": prompt.strip(),\n",
    "        \"parameters_used\": {\n",
    "            \"analysis_type\": analysis_type,\n",
    "            \"data_source\": data_source, \n",
    "            \"objectives_count\": len(objectives)\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Test prompt generation\n",
    "print(\"Testing ML Evaluation Prompt Generation:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "ml_prompt_result = create_ml_evaluation_prompt(\n",
    "    model_type=\"Random Forest Classifier\",\n",
    "    dataset=\"customer_churn_data\",  \n",
    "    metrics=[\"accuracy\", \"precision\", \"recall\", \"f1-score\", \"auc-roc\"],\n",
    "    context=\"production\"\n",
    ")\n",
    "\n",
    "print(\"Generated Prompt:\")\n",
    "print(ml_prompt_result[\"prompt\"])\n",
    "print(f\"\\nPrompt metadata: {ml_prompt_result['metadata']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Testing Data Analysis Prompt Generation:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "data_prompt_result = create_data_analysis_prompt(\n",
    "    analysis_type=\"exploratory\",\n",
    "    data_source=\"sales_transactions_db\",\n",
    "    objectives=[\n",
    "        \"Identify seasonal patterns in sales\",\n",
    "        \"Analyze customer segmentation opportunities\", \n",
    "        \"Detect anomalies in transaction patterns\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Generated Prompt:\")\n",
    "print(data_prompt_result[\"prompt\"])\n",
    "print(f\"\\nParameters used: {data_prompt_result['parameters_used']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing ML Metrics Parser:\n",
      "========================================\n",
      "Parsed ML Metrics:\n",
      "{\n",
      "  \"metrics\": {\n",
      "    \"accuracy\": 0.87,\n",
      "    \"precision\": 0.84,\n",
      "    \"recall\": 0.89,\n",
      "    \"f1_score\": 0.86,\n",
      "    \"auc\": 0.91\n",
      "  },\n",
      "  \"recommendations\": [\n",
      "    \"Consider feature selection to reduce overfitting\",\n",
      "    \"Implement cross\",\n",
      "    \"validation for more robust evaluation\",\n",
      "    \"Monitor model drift in production\"\n",
      "  ],\n",
      "  \"issues\": [\n",
      "    \"Model shows slight bias towards majority class\",\n",
      "    \"Training time is relatively high for large datasets\"\n",
      "  ],\n",
      "  \"production_ready\": null,\n",
      "  \"confidence_score\": 0.8999999999999999\n",
      "}\n",
      "\n",
      "========================================\n",
      "Testing Data Analysis Parser:\n",
      "========================================\n",
      "Parsed Analysis Insights:\n",
      "{\n",
      "  \"key_findings\": [\n",
      "    \"Sales show clear seasonal patterns with\",\n",
      "    \"Customer segments cluster into\",\n",
      "    \"distinct groups based on purchasing behavior\",\n",
      "    \"Transaction anomalies detected in\",\n",
      "    \"% of records, primarily high\"\n",
      "  ],\n",
      "  \"statistical_significance\": {},\n",
      "  \"visualizations\": [\n",
      "    \"Time series plot for seasonal trends\",\n",
      "    \"Cluster scatter plot for customer segmentation\",\n",
      "    \"Histogram of transaction amounts with outlier highlighting\"\n",
      "  ],\n",
      "  \"next_steps\": [],\n",
      "  \"business_impact\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Custom Experiment 2: Parser Run Type for Structured Data Extraction\n",
    "import json\n",
    "import re\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "@traceable(\n",
    "    run_type=\"parser\",\n",
    "    metadata={\n",
    "        \"parser_type\": \"ml_metrics_parser\",\n",
    "        \"output_format\": \"structured_json\",\n",
    "        \"validation\": True\n",
    "    }\n",
    ")\n",
    "def parse_ml_metrics_response(raw_response: str):\n",
    "    \"\"\"Parse ML model evaluation response into structured format\"\"\"\n",
    "    \n",
    "    parsed_data = {\n",
    "        \"metrics\": {},\n",
    "        \"recommendations\": [],\n",
    "        \"issues\": [],\n",
    "        \"production_ready\": None,\n",
    "        \"confidence_score\": 0.0\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Extract metrics using regex patterns\n",
    "        metric_patterns = {\n",
    "            \"accuracy\": r\"accuracy[:\\s]*([0-9.]+)\",\n",
    "            \"precision\": r\"precision[:\\s]*([0-9.]+)\",\n",
    "            \"recall\": r\"recall[:\\s]*([0-9.]+)\",\n",
    "            \"f1_score\": r\"f1[-\\s]?score[:\\s]*([0-9.]+)\",\n",
    "            \"auc\": r\"auc[:\\s]*([0-9.]+)\"\n",
    "        }\n",
    "        \n",
    "        for metric, pattern in metric_patterns.items():\n",
    "            match = re.search(pattern, raw_response.lower())\n",
    "            if match:\n",
    "                parsed_data[\"metrics\"][metric] = float(match.group(1))\n",
    "        \n",
    "        # Extract recommendations\n",
    "        recommendations_section = re.search(\n",
    "            r\"recommendations?[:\\s]*(.*?)(?=\\n\\n|\\n[A-Z]|$)\", \n",
    "            raw_response, \n",
    "            re.IGNORECASE | re.DOTALL\n",
    "        )\n",
    "        if recommendations_section:\n",
    "            recs_text = recommendations_section.group(1)\n",
    "            parsed_data[\"recommendations\"] = [\n",
    "                rec.strip() for rec in re.split(r'[•\\-\\d+\\.]\\s*', recs_text) \n",
    "                if rec.strip() and len(rec.strip()) > 10\n",
    "            ]\n",
    "        \n",
    "        # Extract issues/concerns\n",
    "        issues_section = re.search(\n",
    "            r\"(?:issues?|concerns?|problems?)[:\\s]*(.*?)(?=\\n\\n|\\n[A-Z]|$)\",\n",
    "            raw_response,\n",
    "            re.IGNORECASE | re.DOTALL\n",
    "        )\n",
    "        if issues_section:\n",
    "            issues_text = issues_section.group(1)\n",
    "            parsed_data[\"issues\"] = [\n",
    "                issue.strip() for issue in re.split(r'[•\\-\\d+\\.]\\s*', issues_text)\n",
    "                if issue.strip() and len(issue.strip()) > 10\n",
    "            ]\n",
    "        \n",
    "        # Determine production readiness\n",
    "        production_indicators = {\n",
    "            \"ready\": [\"production ready\", \"ready for deployment\", \"suitable for production\"],\n",
    "            \"not_ready\": [\"not ready\", \"needs improvement\", \"requires optimization\"]\n",
    "        }\n",
    "        \n",
    "        response_lower = raw_response.lower()\n",
    "        ready_score = sum(1 for phrase in production_indicators[\"ready\"] if phrase in response_lower)\n",
    "        not_ready_score = sum(1 for phrase in production_indicators[\"not_ready\"] if phrase in response_lower)\n",
    "        \n",
    "        if ready_score > not_ready_score:\n",
    "            parsed_data[\"production_ready\"] = True\n",
    "        elif not_ready_score > ready_score:\n",
    "            parsed_data[\"production_ready\"] = False\n",
    "        \n",
    "        # Calculate confidence score based on completeness\n",
    "        completeness_score = 0\n",
    "        if parsed_data[\"metrics\"]: completeness_score += 0.4\n",
    "        if parsed_data[\"recommendations\"]: completeness_score += 0.3  \n",
    "        if parsed_data[\"issues\"]: completeness_score += 0.2\n",
    "        if parsed_data[\"production_ready\"] is not None: completeness_score += 0.1\n",
    "        \n",
    "        parsed_data[\"confidence_score\"] = completeness_score\n",
    "        \n",
    "        return parsed_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"error\": str(e),\n",
    "            \"raw_response\": raw_response[:200] + \"...\" if len(raw_response) > 200 else raw_response\n",
    "        }\n",
    "\n",
    "@traceable(\n",
    "    run_type=\"parser\",\n",
    "    metadata={\n",
    "        \"parser_type\": \"data_insights_parser\",\n",
    "        \"structured_output\": True\n",
    "    }\n",
    ")\n",
    "def parse_data_analysis_response(raw_response: str):\n",
    "    \"\"\"Parse data analysis response into actionable insights\"\"\"\n",
    "    \n",
    "    insights = {\n",
    "        \"key_findings\": [],\n",
    "        \"statistical_significance\": {},\n",
    "        \"visualizations\": [],\n",
    "        \"next_steps\": [],\n",
    "        \"business_impact\": None\n",
    "    }\n",
    "    \n",
    "    # Extract key findings\n",
    "    findings_match = re.search(\n",
    "        r\"(?:key findings?|findings?|insights?)[:\\s]*(.*?)(?=statistical|visualization|next steps|$)\",\n",
    "        raw_response,\n",
    "        re.IGNORECASE | re.DOTALL\n",
    "    )\n",
    "    if findings_match:\n",
    "        findings_text = findings_match.group(1)\n",
    "        insights[\"key_findings\"] = [\n",
    "            finding.strip() for finding in re.split(r'[•\\-\\d+\\.]\\s*', findings_text)\n",
    "            if finding.strip() and len(finding.strip()) > 15\n",
    "        ]\n",
    "    \n",
    "    # Extract visualization recommendations\n",
    "    viz_match = re.search(\n",
    "        r\"visualization[s]?[:\\s]*(.*?)(?=next steps|business|$)\",\n",
    "        raw_response,\n",
    "        re.IGNORECASE | re.DOTALL\n",
    "    )\n",
    "    if viz_match:\n",
    "        viz_text = viz_match.group(1) \n",
    "        insights[\"visualizations\"] = [\n",
    "            viz.strip() for viz in re.split(r'[•\\-\\d+\\.]\\s*', viz_text)\n",
    "            if viz.strip() and len(viz.strip()) > 10\n",
    "        ]\n",
    "    \n",
    "    return insights\n",
    "\n",
    "# Test parsing functionality\n",
    "print(\"Testing ML Metrics Parser:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "sample_ml_response = \"\"\"\n",
    "The Random Forest model shows strong performance with the following metrics:\n",
    "- Accuracy: 0.87\n",
    "- Precision: 0.84  \n",
    "- Recall: 0.89\n",
    "- F1-score: 0.86\n",
    "- AUC: 0.91\n",
    "\n",
    "Recommendations:\n",
    "- Consider feature selection to reduce overfitting\n",
    "- Implement cross-validation for more robust evaluation\n",
    "- Monitor model drift in production\n",
    "\n",
    "Issues:\n",
    "- Model shows slight bias towards majority class\n",
    "- Training time is relatively high for large datasets\n",
    "\n",
    "The model appears ready for production deployment with proper monitoring.\n",
    "\"\"\"\n",
    "\n",
    "parsed_ml_result = parse_ml_metrics_response(sample_ml_response)\n",
    "print(\"Parsed ML Metrics:\")\n",
    "print(json.dumps(parsed_ml_result, indent=2))\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"Testing Data Analysis Parser:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "sample_analysis_response = \"\"\"\n",
    "Key Findings:\n",
    "- Sales show clear seasonal patterns with 40% increase in Q4\n",
    "- Customer segments cluster into 3 distinct groups based on purchasing behavior\n",
    "- Transaction anomalies detected in 2.3% of records, primarily high-value purchases\n",
    "\n",
    "Visualizations:\n",
    "- Time series plot for seasonal trends\n",
    "- Cluster scatter plot for customer segmentation  \n",
    "- Histogram of transaction amounts with outlier highlighting\n",
    "\n",
    "Next Steps:\n",
    "- Investigate anomalous transactions for fraud patterns\n",
    "- Develop targeted marketing for each customer segment\n",
    "\"\"\"\n",
    "\n",
    "parsed_analysis_result = parse_data_analysis_response(sample_analysis_response)\n",
    "print(\"Parsed Analysis Insights:\")\n",
    "print(json.dumps(parsed_analysis_result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Complete ML Analysis Pipeline:\n",
      "============================================================\n",
      "Model Info: {'type': 'Gradient Boosting Classifier', 'dataset': 'customer_behavior_analysis', 'accuracy': 0.89, 'precision': 0.87, 'recall': 0.91, 'metrics': ['accuracy', 'precision', 'recall', 'f1-score', 'auc-roc', 'feature_importance']}\n",
      "Evaluation Request: \n",
      "Please evaluate this model for deployment in a customer recommendation system. \n",
      "Focus on production...\n",
      "\n",
      "Executing pipeline...\n",
      "------------------------------------------------------------\n",
      "Stage 1: Retrieving relevant ML documentation...\n",
      "Retrieving documents for query: 'best practices for Gradient Boosting Classifier model evaluation'\n",
      "Retrieved 3 documents\n",
      "Doc 1: Machine learning models require careful evaluation using met...\n",
      "Doc 2: Model deployment involves containerization, API creation, an...\n",
      "Doc 3: Model versioning is crucial for tracking experiments and mai...\n",
      "Stage 2: Generating specialized evaluation prompt...\n",
      "Stage 3: Processing with Mistral AI...\n",
      "Stage 4: Parsing and structuring response...\n",
      "Stage 5: Generating final analysis report...\n",
      "Pipeline Status: SUCCESS\n",
      "Total Execution Time: 0.00s\n",
      "Stages Completed: 5\n",
      "\n",
      "Stage Performance:\n",
      "  document_retrieval: completed (0.00s)\n",
      "  prompt_generation: completed (0.00s)\n",
      "  llm_processing: completed (0.00s)\n",
      "  response_parsing: completed (0.00s)\n",
      "  final_reporting: completed (0.00s)\n",
      "\n",
      "Final Analysis Summary:\n",
      "  Production Ready: None\n",
      "  Recommendations: 5\n",
      "  Issues Identified: 5\n",
      "  Data Completeness: 0.90\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Custom Experiment 3: Complex Chain Run Type - ML Analysis Pipeline\n",
    "from typing import Any, Dict\n",
    "import time\n",
    "\n",
    "@traceable(\n",
    "    run_type=\"chain\",\n",
    "    metadata={\n",
    "        \"chain_name\": \"ml_analysis_pipeline\",\n",
    "        \"version\": \"2.1\",\n",
    "        \"components\": [\"prompt\", \"llm\", \"parser\", \"retriever\"],\n",
    "        \"use_case\": \"automated_ml_evaluation\"\n",
    "    }\n",
    ")\n",
    "def ml_analysis_pipeline(\n",
    "    model_info: Dict[str, Any], \n",
    "    evaluation_request: str,\n",
    "    include_recommendations: bool = True\n",
    "):\n",
    "    \"\"\"Complete ML analysis pipeline orchestrating multiple run types\"\"\"\n",
    "    \n",
    "    pipeline_start = time.time()\n",
    "    results = {\n",
    "        \"pipeline_id\": f\"ml_analysis_{int(time.time())}\",\n",
    "        \"stages\": {},\n",
    "        \"final_output\": {},\n",
    "        \"performance_metrics\": {}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Stage 1: Document Retrieval (Retriever run)\n",
    "        print(\"Stage 1: Retrieving relevant ML documentation...\")\n",
    "        stage1_start = time.time()\n",
    "        \n",
    "        retrieval_query = f\"best practices for {model_info.get('type', 'machine learning')} model evaluation\"\n",
    "        retrieved_docs = retrieve_ml_docs(retrieval_query)\n",
    "        \n",
    "        results[\"stages\"][\"document_retrieval\"] = {\n",
    "            \"status\": \"completed\",\n",
    "            \"documents_count\": len(retrieved_docs),\n",
    "            \"duration\": time.time() - stage1_start\n",
    "        }\n",
    "        \n",
    "        # Stage 2: Prompt Generation (Prompt run)\n",
    "        print(\"Stage 2: Generating specialized evaluation prompt...\")\n",
    "        stage2_start = time.time()\n",
    "        \n",
    "        prompt_result = create_ml_evaluation_prompt(\n",
    "            model_type=model_info.get(\"type\", \"Unknown\"),\n",
    "            dataset=model_info.get(\"dataset\", \"Unknown\"),\n",
    "            metrics=model_info.get(\"metrics\", [\"accuracy\", \"precision\", \"recall\"]),\n",
    "            context=\"automated_analysis\"\n",
    "        )\n",
    "        \n",
    "        results[\"stages\"][\"prompt_generation\"] = {\n",
    "            \"status\": \"completed\", \n",
    "            \"prompt_length\": len(prompt_result[\"prompt\"]),\n",
    "            \"duration\": time.time() - stage2_start\n",
    "        }\n",
    "        \n",
    "        # Stage 3: LLM Processing (LLM run)\n",
    "        print(\"Stage 3: Processing with Mistral AI...\")\n",
    "        stage3_start = time.time()\n",
    "        \n",
    "        # Simulate LLM call with the generated prompt\n",
    "        llm_messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert ML evaluation assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt_result[\"prompt\"] + \"\\n\\nAdditional context: \" + evaluation_request}\n",
    "        ]\n",
    "        \n",
    "        # Simulate realistic ML evaluation response\n",
    "        simulated_response = f\"\"\"\n",
    "        Based on the {model_info.get('type', 'model')} trained on {model_info.get('dataset', 'the dataset')}, here's my evaluation:\n",
    "\n",
    "        Performance Metrics Analysis:\n",
    "        - Accuracy: {model_info.get('accuracy', 0.85):.2f} - Shows good overall performance\n",
    "        - Precision: {model_info.get('precision', 0.82):.2f} - Acceptable for most applications  \n",
    "        - Recall: {model_info.get('recall', 0.88):.2f} - Good coverage of positive cases\n",
    "        - F1-score: 0.85 - Balanced performance between precision and recall\n",
    "\n",
    "        Key Findings:\n",
    "        - Model demonstrates consistent performance across validation sets\n",
    "        - No significant signs of overfitting detected\n",
    "        - Feature importance analysis shows logical patterns\n",
    "\n",
    "        Recommendations:\n",
    "        - Implement real-time monitoring for model drift detection\n",
    "        - Set up automated retraining pipeline for data updates\n",
    "        - Consider ensemble methods to improve robustness\n",
    "        - Establish baseline metrics for production comparison\n",
    "\n",
    "        Issues and Concerns:\n",
    "        - Model inference time may be high for real-time applications\n",
    "        - Limited testing on edge cases and adversarial inputs\n",
    "        - Missing bias analysis for protected attributes\n",
    "\n",
    "        Production Readiness Assessment:\n",
    "        The model appears ready for production deployment with proper monitoring infrastructure.\n",
    "        Confidence level: High (85%)\n",
    "        \"\"\"\n",
    "        \n",
    "        results[\"stages\"][\"llm_processing\"] = {\n",
    "            \"status\": \"completed\",\n",
    "            \"response_length\": len(simulated_response),\n",
    "            \"duration\": time.time() - stage3_start\n",
    "        }\n",
    "        \n",
    "        # Stage 4: Response Parsing (Parser run)\n",
    "        print(\"Stage 4: Parsing and structuring response...\")\n",
    "        stage4_start = time.time()\n",
    "        \n",
    "        parsed_result = parse_ml_metrics_response(simulated_response)\n",
    "        \n",
    "        results[\"stages\"][\"response_parsing\"] = {\n",
    "            \"status\": \"completed\",\n",
    "            \"metrics_extracted\": len(parsed_result.get(\"metrics\", {})),\n",
    "            \"recommendations_count\": len(parsed_result.get(\"recommendations\", [])),\n",
    "            \"confidence_score\": parsed_result.get(\"confidence_score\", 0),\n",
    "            \"duration\": time.time() - stage4_start\n",
    "        }\n",
    "        \n",
    "        # Stage 5: Final Analysis and Reporting (Chain orchestration)\n",
    "        print(\"Stage 5: Generating final analysis report...\")\n",
    "        stage5_start = time.time()\n",
    "        \n",
    "        total_pipeline_time = time.time() - pipeline_start\n",
    "        \n",
    "        final_report = {\n",
    "            \"model_evaluation\": parsed_result,\n",
    "            \"retrieved_context\": {\n",
    "                \"documents_used\": len(retrieved_docs),\n",
    "                \"relevance_scores\": [doc[\"metadata\"][\"relevance_score\"] for doc in retrieved_docs]\n",
    "            },\n",
    "            \"pipeline_performance\": {\n",
    "                \"total_duration\": total_pipeline_time,\n",
    "                \"stages_completed\": len([s for s in results[\"stages\"].values() if s[\"status\"] == \"completed\"]),\n",
    "                \"average_stage_time\": sum(s[\"duration\"] for s in results[\"stages\"].values()) / len(results[\"stages\"])\n",
    "            },\n",
    "            \"quality_assessment\": {\n",
    "                \"data_completeness\": parsed_result.get(\"confidence_score\", 0),\n",
    "                \"recommendation_coverage\": \"high\" if len(parsed_result.get(\"recommendations\", [])) >= 3 else \"medium\",\n",
    "                \"production_readiness\": parsed_result.get(\"production_ready\", False)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        results[\"stages\"][\"final_reporting\"] = {\n",
    "            \"status\": \"completed\",\n",
    "            \"duration\": time.time() - stage5_start\n",
    "        }\n",
    "        \n",
    "        results[\"final_output\"] = final_report\n",
    "        results[\"performance_metrics\"] = {\n",
    "            \"pipeline_success\": True,\n",
    "            \"total_execution_time\": total_pipeline_time,\n",
    "            \"stages_completed\": 5,\n",
    "            \"efficiency_score\": 1.0 / total_pipeline_time if total_pipeline_time > 0 else 0\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        results[\"error\"] = str(e)\n",
    "        results[\"performance_metrics\"] = {\n",
    "            \"pipeline_success\": False,\n",
    "            \"total_execution_time\": time.time() - pipeline_start,\n",
    "            \"error_stage\": len(results[\"stages\"]) + 1\n",
    "        }\n",
    "        return results\n",
    "\n",
    "# Test the complete ML analysis pipeline\n",
    "print(\"Testing Complete ML Analysis Pipeline:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_model_info = {\n",
    "    \"type\": \"Gradient Boosting Classifier\",\n",
    "    \"dataset\": \"customer_behavior_analysis\",\n",
    "    \"accuracy\": 0.89,\n",
    "    \"precision\": 0.87,\n",
    "    \"recall\": 0.91,\n",
    "    \"metrics\": [\"accuracy\", \"precision\", \"recall\", \"f1-score\", \"auc-roc\", \"feature_importance\"]\n",
    "}\n",
    "\n",
    "test_request = \"\"\"\n",
    "Please evaluate this model for deployment in a customer recommendation system. \n",
    "Focus on production readiness, potential biases, and scalability concerns.\n",
    "The system needs to handle 10,000+ predictions per day with <100ms response time.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Model Info: {test_model_info}\")\n",
    "print(f\"Evaluation Request: {test_request[:100]}...\")\n",
    "print(\"\\nExecuting pipeline...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "pipeline_result = ml_analysis_pipeline(test_model_info, test_request)\n",
    "\n",
    "if pipeline_result[\"performance_metrics\"][\"pipeline_success\"]:\n",
    "    print(\"Pipeline Status: SUCCESS\")\n",
    "    print(f\"Total Execution Time: {pipeline_result['performance_metrics']['total_execution_time']:.2f}s\")\n",
    "    print(f\"Stages Completed: {pipeline_result['performance_metrics']['stages_completed']}\")\n",
    "    \n",
    "    print(\"\\nStage Performance:\")\n",
    "    for stage_name, stage_info in pipeline_result[\"stages\"].items():\n",
    "        print(f\"  {stage_name}: {stage_info['status']} ({stage_info['duration']:.2f}s)\")\n",
    "    \n",
    "    print(\"\\nFinal Analysis Summary:\")\n",
    "    final_output = pipeline_result[\"final_output\"]\n",
    "    print(f\"  Production Ready: {final_output['model_evaluation'].get('production_ready', 'Unknown')}\")\n",
    "    print(f\"  Recommendations: {len(final_output['model_evaluation'].get('recommendations', []))}\")\n",
    "    print(f\"  Issues Identified: {len(final_output['model_evaluation'].get('issues', []))}\")\n",
    "    print(f\"  Data Completeness: {final_output['quality_assessment']['data_completeness']:.2f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Pipeline Status: FAILED\")\n",
    "    print(f\"Error: {pipeline_result.get('error', 'Unknown error')}\")\n",
    "    print(f\"Failed at stage: {pipeline_result['performance_metrics'].get('error_stage', 'Unknown')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Learning and Changes\n",
    "\n",
    "### What I Learned\n",
    "Through this notebook, I gained hands-on experience with LangSmith's different run types and their specific use cases. The key learning was understanding how each run type serves a distinct purpose in ML workflows:\n",
    "\n",
    "- **LLM runs** require specific input/output formats and metadata for proper cost tracking and visualization\n",
    "- **Retriever runs** need structured document formats with metadata for effective debugging of retrieval issues  \n",
    "- **Tool runs** enable function calling capabilities and require proper tool definition schemas\n",
    "- **Prompt runs** help manage and version template generation for consistent LLM interactions\n",
    "- **Parser runs** structure unstructured LLM outputs into actionable data formats\n",
    "- **Chain runs** orchestrate multiple components into complete ML analysis pipelines\n",
    "\n",
    "### Changes Made\n",
    "I migrated this notebook from OpenAI to **Mistral AI** and **HuggingFace embeddings**, replacing all OpenAI dependencies. Key enhancements included:\n",
    "\n",
    "1. **Updated all environment variables** from OpenAI to Mistral AI API keys\n",
    "2. **Replaced OpenAI models** with ChatMistralAI throughout all examples\n",
    "3. **Enhanced tool calling examples** with realistic weather tools and better error handling\n",
    "4. **Added custom experiments** including ML evaluation prompts, data analysis parsers, and a complete 5-stage ML analysis pipeline\n",
    "5. **Improved metadata tracking** with detailed run information, performance metrics, and quality assessments\n",
    "6. **Created production-ready examples** with proper error handling, validation, and structured outputs\n",
    "\n",
    "The notebook now demonstrates real-world ML evaluation scenarios using Mistral AI, making it more relevant for modern LLM applications while maintaining all LangSmith tracing capabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mat496",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
