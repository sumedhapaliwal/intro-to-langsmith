{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversational Threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many LLM applications have a chatbot-like interface in which the user and the LLM application engage in a multi-turn conversation. In order to track these conversations, you can use the Threads feature in LangSmith.\n",
    "\n",
    "This is relevant to our RAG application, which should maintain context from prior conversations with users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Make sure you set your environment variables, including your Mistral API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can set them inline\n",
    "import os\n",
    "os.environ[\"MISTRAL_API_KEY\"] = \"MISTRAL_API_KEY\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"ANGSMITH_API_KEY\"\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy\"  # If you don't set this, traces will go to the Default project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or you can use a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group traces into threads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Thread is a sequence of traces representing a single conversation. Each response is represented as its own trace, but these traces are linked together by being part of the same thread.\n",
    "\n",
    "To associate traces together, you need to pass in a special metadata key where the value is the unique identifier for that thread.\n",
    "\n",
    "The key value is the unique identifier for that conversation. The key name should be one of:\n",
    "\n",
    "- session_id\n",
    "- thread_id\n",
    "- conversation_id.\n",
    "\n",
    "The value should be a UUID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "thread_id = uuid.uuid4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "from utils import get_vector_db_retriever\n",
    "\n",
    "mistral_client = ChatMistralAI(model=\"mistral-small-latest\")\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    rag_system_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": rag_system_prompt\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    return call_mistral(messages)\n",
    "\n",
    "@traceable(run_type=\"llm\")\n",
    "def call_mistral(\n",
    "    messages: List[dict], model: str = \"mistral-small-latest\", temperature: float = 0.0\n",
    ") -> str:\n",
    "    # Convert dict messages to LangChain message objects\n",
    "    langchain_messages = []\n",
    "    for msg in messages:\n",
    "        if msg[\"role\"] == \"system\":\n",
    "            langchain_messages.append(SystemMessage(content=msg[\"content\"]))\n",
    "        elif msg[\"role\"] == \"user\":\n",
    "            langchain_messages.append(HumanMessage(content=msg[\"content\"]))\n",
    "    \n",
    "    return mistral_client.invoke(langchain_messages)\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's run our application twice with this thread_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: What are the best practices for implementing tracing in production ML systems?\n",
      "Answer: To implement tracing in production ML systems, you can ingest traces via the Python or JavaScript LangSmith SDK, the @traceable wrapper, or by submitting traces via the /runs/multipart endpoint. Key services for trace ingestion include the platform backend service, Redis cache, queue service, and ClickHouse. To scale up the write path, monitor these services and consider increasing resources for ClickHouse, platform-backend pods, queue service replicas, or using a larger Redis cache.\n",
      "Thread ID: f05f4d93-51e0-4f73-8fbd-5de36f6cd1a5\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the best practices for implementing tracing in production ML systems?\"\n",
    "ai_answer = langsmith_rag(question, langsmith_extra={\"metadata\": {\"thread_id\": thread_id}})\n",
    "print(f\"Question 1: {question}\")\n",
    "print(f\"Answer: {ai_answer}\")\n",
    "print(f\"Thread ID: {thread_id}\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 2: How can I use metadata and tags to organize my traces for better debugging?\n",
      "Answer: You can use tags to categorize and filter traces for easier search and analysis. Metadata allows you to attach key-value pairs to traces, storing additional information like environment details or application versions. By combining these, you can effectively organize and query traces to streamline debugging.\n",
      "Thread ID: f05f4d93-51e0-4f73-8fbd-5de36f6cd1a5\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "question = \"How can I use metadata and tags to organize my traces for better debugging?\"\n",
    "ai_answer = langsmith_rag(question, langsmith_extra={\"metadata\": {\"thread_id\": thread_id}})\n",
    "print(f\"Question 2: {question}\")\n",
    "print(f\"Answer: {ai_answer}\")\n",
    "print(f\"Thread ID: {thread_id}\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a look in LangSmith!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Conversational Thread Experiments\n",
    "\n",
    "Let's create a more realistic conversation flow with multiple turns and different thread scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting conversation thread: Technical ML Discussion\n",
      "Thread ID: be8df3e8-94ca-4f6c-a16a-3bd6a13f5db7\n",
      "============================================================\n",
      "\n",
      "Turn 1: What is the difference between supervised and unsupervised learning?\n",
      "Response: I don't know.\n",
      "----------------------------------------\n",
      "\n",
      "Turn 2: Can you explain how neural networks learn from data?\n",
      "Response: I don't know.\n",
      "----------------------------------------\n",
      "\n",
      "Turn 2: Can you explain how neural networks learn from data?\n",
      "Response: I don't know.\n",
      "----------------------------------------\n",
      "\n",
      "Turn 3: What are some common challenges in training deep learning models?\n",
      "Response: I don't know.\n",
      "----------------------------------------\n",
      "\n",
      "Turn 3: What are some common challenges in training deep learning models?\n",
      "Response: The context provided does not discuss challenges in training deep learning models. I don't know the answer to your question.\n",
      "----------------------------------------\n",
      "Response: The context provided does not discuss challenges in training deep learning models. I don't know the answer to your question.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Custom Experiment: Multi-Turn Conversation Simulation\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "def simulate_conversation(questions: List[str], thread_name: str = \"default\"):\n",
    "    \"\"\"Simulate a multi-turn conversation with a unique thread ID\"\"\"\n",
    "    conversation_thread_id = uuid.uuid4()\n",
    "    print(f\"\\nStarting conversation thread: {thread_name}\")\n",
    "    print(f\"Thread ID: {conversation_thread_id}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    conversation_history = []\n",
    "    \n",
    "    for i, question in enumerate(questions, 1):\n",
    "        print(f\"\\nTurn {i}: {question}\")\n",
    "        \n",
    "        # Add some realistic timing between questions\n",
    "        if i > 1:\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        try:\n",
    "            ai_answer = langsmith_rag(\n",
    "                question, \n",
    "                langsmith_extra={\n",
    "                    \"metadata\": {\n",
    "                        \"thread_id\": conversation_thread_id,\n",
    "                        \"turn_number\": i,\n",
    "                        \"conversation_name\": thread_name,\n",
    "                        \"user_id\": \"rakshit\"\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            conversation_history.append({\n",
    "                \"turn\": i,\n",
    "                \"question\": question,\n",
    "                \"answer\": ai_answer,\n",
    "                \"timestamp\": time.time()\n",
    "            })\n",
    "            \n",
    "            print(f\"Response: {ai_answer}\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in turn {i}: {e}\")\n",
    "            conversation_history.append({\n",
    "                \"turn\": i,\n",
    "                \"question\": question,\n",
    "                \"error\": str(e),\n",
    "                \"timestamp\": time.time()\n",
    "            })\n",
    "    \n",
    "    return conversation_thread_id, conversation_history\n",
    "\n",
    "# Conversation 1: Technical Discussion\n",
    "tech_questions = [\n",
    "    \"What is the difference between supervised and unsupervised learning?\",\n",
    "    \"Can you explain how neural networks learn from data?\",\n",
    "    \"What are some common challenges in training deep learning models?\"\n",
    "]\n",
    "\n",
    "thread1_id, history1 = simulate_conversation(tech_questions, \"Technical ML Discussion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting conversation thread: ML Implementation Guide\n",
      "Thread ID: cc095561-7f55-4c03-878e-eabd70e71035\n",
      "============================================================\n",
      "\n",
      "Turn 1: How do I choose the right evaluation metrics for my ML model?\n",
      "Response: To choose the right evaluation metrics for your ML model, consider the following:\n",
      "\n",
      "1. **Define Your Goals**: Determine what you want to evaluate. For example, you might want to check correctness, concision, or valid reasoning of the model's outputs.\n",
      "2. **Use Appropriate Metrics**: For complex evaluations like correctness, use an LLM-as-a-judge approach. For simpler tasks like concision, a simple Python function might suffice.\n",
      "3. **Align with Human Preferences**: Use few-shot examples to align the LLM-as-a-judge evaluator with human preferences, ensuring the metrics reflect what you value in the model's performance.\n",
      "----------------------------------------\n",
      "\n",
      "Turn 2: What are the key considerations for deploying ML models in production?\n",
      "Response: To choose the right evaluation metrics for your ML model, consider the following:\n",
      "\n",
      "1. **Define Your Goals**: Determine what you want to evaluate. For example, you might want to check correctness, concision, or valid reasoning of the model's outputs.\n",
      "2. **Use Appropriate Metrics**: For complex evaluations like correctness, use an LLM-as-a-judge approach. For simpler tasks like concision, a simple Python function might suffice.\n",
      "3. **Align with Human Preferences**: Use few-shot examples to align the LLM-as-a-judge evaluator with human preferences, ensuring the metrics reflect what you value in the model's performance.\n",
      "----------------------------------------\n",
      "\n",
      "Turn 2: What are the key considerations for deploying ML models in production?\n",
      "Response: Deploying ML models in production involves several key considerations. First, you should evaluate the model's performance using techniques like backtesting, which involves running past production examples through newer model versions to assess performance. Additionally, consider using online evaluation to monitor the application in real-time and flag unintended behavior. Pairwise evaluation can also be useful for comparing model versions, especially when it's easier to determine relative performance rather than assigning absolute scores.\n",
      "----------------------------------------\n",
      "\n",
      "Turn 3: How can I monitor model performance after deployment?\n",
      "Response: Deploying ML models in production involves several key considerations. First, you should evaluate the model's performance using techniques like backtesting, which involves running past production examples through newer model versions to assess performance. Additionally, consider using online evaluation to monitor the application in real-time and flag unintended behavior. Pairwise evaluation can also be useful for comparing model versions, especially when it's easier to determine relative performance rather than assigning absolute scores.\n",
      "----------------------------------------\n",
      "\n",
      "Turn 3: How can I monitor model performance after deployment?\n",
      "Response: To monitor model performance after deployment, you can use online evaluation to assess the application's outputs in real-time. This involves running evaluators on real inputs and outputs as they are produced, allowing you to monitor the application and flag unintended behavior. Additionally, you can use Grafana to monitor logs, metrics, and traces, which comes pre-packaged with dashboards for monitoring the main components of your deployment.\n",
      "----------------------------------------\n",
      "\n",
      "Starting conversation thread: Debugging Session\n",
      "Thread ID: 9581d8d5-12da-47ae-97f9-aca3f8460c04\n",
      "============================================================\n",
      "\n",
      "Turn 1: My model is overfitting, what should I do?\n",
      "Response: To monitor model performance after deployment, you can use online evaluation to assess the application's outputs in real-time. This involves running evaluators on real inputs and outputs as they are produced, allowing you to monitor the application and flag unintended behavior. Additionally, you can use Grafana to monitor logs, metrics, and traces, which comes pre-packaged with dashboards for monitoring the main components of your deployment.\n",
      "----------------------------------------\n",
      "\n",
      "Starting conversation thread: Debugging Session\n",
      "Thread ID: 9581d8d5-12da-47ae-97f9-aca3f8460c04\n",
      "============================================================\n",
      "\n",
      "Turn 1: My model is overfitting, what should I do?\n",
      "Response: I don't know.\n",
      "----------------------------------------\n",
      "\n",
      "Turn 2: How can I improve model performance when I have limited data?\n",
      "Response: I don't know.\n",
      "----------------------------------------\n",
      "\n",
      "Turn 2: How can I improve model performance when I have limited data?\n",
      "Response: I don't know.\n",
      "----------------------------------------\n",
      "\n",
      "================================================================================\n",
      "CONVERSATION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Technical ML Discussion\n",
      "Thread ID: be8df3e8-94ca-4f6c-a16a-3bd6a13f5db7\n",
      "Total turns: 3\n",
      "Duration: 1.89 seconds\n",
      "Successful responses: 3, Failed: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "ML Implementation Guide\n",
      "Thread ID: cc095561-7f55-4c03-878e-eabd70e71035\n",
      "Total turns: 3\n",
      "Duration: 2.98 seconds\n",
      "Successful responses: 3, Failed: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "Debugging Session\n",
      "Thread ID: 9581d8d5-12da-47ae-97f9-aca3f8460c04\n",
      "Total turns: 2\n",
      "Duration: 0.86 seconds\n",
      "Successful responses: 2, Failed: 0\n",
      "--------------------------------------------------\n",
      "Response: I don't know.\n",
      "----------------------------------------\n",
      "\n",
      "================================================================================\n",
      "CONVERSATION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Technical ML Discussion\n",
      "Thread ID: be8df3e8-94ca-4f6c-a16a-3bd6a13f5db7\n",
      "Total turns: 3\n",
      "Duration: 1.89 seconds\n",
      "Successful responses: 3, Failed: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "ML Implementation Guide\n",
      "Thread ID: cc095561-7f55-4c03-878e-eabd70e71035\n",
      "Total turns: 3\n",
      "Duration: 2.98 seconds\n",
      "Successful responses: 3, Failed: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "Debugging Session\n",
      "Thread ID: 9581d8d5-12da-47ae-97f9-aca3f8460c04\n",
      "Total turns: 2\n",
      "Duration: 0.86 seconds\n",
      "Successful responses: 2, Failed: 0\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Conversation 2: Practical Implementation\n",
    "practical_questions = [\n",
    "    \"How do I choose the right evaluation metrics for my ML model?\",\n",
    "    \"What are the key considerations for deploying ML models in production?\",\n",
    "    \"How can I monitor model performance after deployment?\"\n",
    "]\n",
    "\n",
    "thread2_id, history2 = simulate_conversation(practical_questions, \"ML Implementation Guide\")\n",
    "\n",
    "# Conversation 3: Debugging and Troubleshooting\n",
    "debug_questions = [\n",
    "    \"My model is overfitting, what should I do?\",\n",
    "    \"How can I improve model performance when I have limited data?\"\n",
    "]\n",
    "\n",
    "thread3_id, history3 = simulate_conversation(debug_questions, \"Debugging Session\")\n",
    "\n",
    "# Summary of all conversations\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONVERSATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_conversations = [\n",
    "    (thread1_id, history1, \"Technical ML Discussion\"),\n",
    "    (thread2_id, history2, \"ML Implementation Guide\"), \n",
    "    (thread3_id, history3, \"Debugging Session\")\n",
    "]\n",
    "\n",
    "for thread_id, history, name in all_conversations:\n",
    "    print(f\"\\n{name}\")\n",
    "    print(f\"Thread ID: {thread_id}\")\n",
    "    print(f\"Total turns: {len(history)}\")\n",
    "    \n",
    "    if history:\n",
    "        duration = history[-1][\"timestamp\"] - history[0][\"timestamp\"]\n",
    "        print(f\"Duration: {duration:.2f} seconds\")\n",
    "        \n",
    "        # Count successful vs failed responses\n",
    "        successful = len([h for h in history if \"answer\" in h])\n",
    "        failed = len([h for h in history if \"error\" in h])\n",
    "        print(f\"Successful responses: {successful}, Failed: {failed}\")\n",
    "        \n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Advanced Analysis: Advanced ML Ethics & Implementation\n",
      "Session Metadata: {\n",
      "  \"user_id\": \"rakshit\",\n",
      "  \"session_start\": \"2025-10-04T17:18:53.931900\",\n",
      "  \"thread_name\": \"Advanced ML Ethics & Implementation\",\n",
      "  \"total_questions\": 3,\n",
      "  \"analysis_version\": \"v1.2\"\n",
      "}\n",
      "------------------------------------------------------------\n",
      "Q1: Explain the concept of transfer learning and its applications\n",
      "A1: I don't know.\n",
      "Response time: 0.43s | Length: 13 chars\n",
      "\n",
      "Q1: Explain the concept of transfer learning and its applications\n",
      "A1: I don't know.\n",
      "Response time: 0.43s | Length: 13 chars\n",
      "\n",
      "Q2: What are the ethical considerations in AI development?\n",
      "A2: The context provided does not contain information about the ethical considerations in AI development. I don't know the answer to your question.\n",
      "Response time: 0.50s | Length: 143 chars\n",
      "\n",
      "Q2: What are the ethical considerations in AI development?\n",
      "A2: The context provided does not contain information about the ethical considerations in AI development. I don't know the answer to your question.\n",
      "Response time: 0.50s | Length: 143 chars\n",
      "\n",
      "Q3: How do I implement model versioning and experiment tracking?\n",
      "A3: The context provided does not include information on model versioning and experiment tracking.\n",
      "Response time: 0.50s | Length: 94 chars\n",
      "\n",
      "Analytics Summary:\n",
      "Success rate: 3/3 (100.0%)\n",
      "Average response time: 0.48s\n",
      "Average response length: 83 characters\n",
      "\n",
      "Final Thread ID for Advanced Analysis: 4613e874-1720-422f-ad6f-ccc97c58441f\n",
      "Q3: How do I implement model versioning and experiment tracking?\n",
      "A3: The context provided does not include information on model versioning and experiment tracking.\n",
      "Response time: 0.50s | Length: 94 chars\n",
      "\n",
      "Analytics Summary:\n",
      "Success rate: 3/3 (100.0%)\n",
      "Average response time: 0.48s\n",
      "Average response length: 83 characters\n",
      "\n",
      "Final Thread ID for Advanced Analysis: 4613e874-1720-422f-ad6f-ccc97c58441f\n"
     ]
    }
   ],
   "source": [
    "# Advanced Thread Analytics and Metadata Demo\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "class ConversationAnalyzer:\n",
    "    \"\"\"Custom class to analyze conversation threads with enhanced metadata\"\"\"\n",
    "    \n",
    "    def __init__(self, user_id: str = \"rakshit\"):\n",
    "        self.user_id = user_id\n",
    "        self.session_start = datetime.now()\n",
    "        \n",
    "    def analyze_conversation_quality(self, questions: List[str], thread_name: str):\n",
    "        \"\"\"Analyze conversation with quality metrics and detailed metadata\"\"\"\n",
    "        \n",
    "        thread_id = uuid.uuid4()\n",
    "        session_metadata = {\n",
    "            \"user_id\": self.user_id,\n",
    "            \"session_start\": self.session_start.isoformat(),\n",
    "            \"thread_name\": thread_name,\n",
    "            \"total_questions\": len(questions),\n",
    "            \"analysis_version\": \"v1.2\"\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nAdvanced Analysis: {thread_name}\")\n",
    "        print(f\"Session Metadata: {json.dumps(session_metadata, indent=2)}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for i, question in enumerate(questions, 1):\n",
    "            question_metadata = {\n",
    "                **session_metadata,\n",
    "                \"thread_id\": str(thread_id),\n",
    "                \"question_number\": i,\n",
    "                \"question_length\": len(question),\n",
    "                \"question_complexity\": \"high\" if len(question.split()) > 10 else \"medium\" if len(question.split()) > 5 else \"low\",\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                \n",
    "                response = langsmith_rag(\n",
    "                    question,\n",
    "                    langsmith_extra={\"metadata\": question_metadata}\n",
    "                )\n",
    "                \n",
    "                end_time = time.time()\n",
    "                response_time = end_time - start_time\n",
    "                \n",
    "                result = {\n",
    "                    \"question_num\": i,\n",
    "                    \"question\": question,\n",
    "                    \"response\": response,\n",
    "                    \"response_time\": response_time,\n",
    "                    \"response_length\": len(response),\n",
    "                    \"success\": True,\n",
    "                    \"metadata\": question_metadata\n",
    "                }\n",
    "                \n",
    "                print(f\"Q{i}: {question}\")\n",
    "                print(f\"A{i}: {response[:150]}{'...' if len(response) > 150 else ''}\")\n",
    "                print(f\"Response time: {response_time:.2f}s | Length: {len(response)} chars\")\n",
    "                print()\n",
    "                \n",
    "            except Exception as e:\n",
    "                result = {\n",
    "                    \"question_num\": i,\n",
    "                    \"question\": question,\n",
    "                    \"error\": str(e),\n",
    "                    \"success\": False,\n",
    "                    \"metadata\": question_metadata\n",
    "                }\n",
    "                print(f\"Q{i}: {question}\")\n",
    "                print(f\"Error: {e}\")\n",
    "                print()\n",
    "            \n",
    "            results.append(result)\n",
    "        \n",
    "        # Generate analytics\n",
    "        successful_responses = [r for r in results if r[\"success\"]]\n",
    "        if successful_responses:\n",
    "            avg_response_time = sum(r[\"response_time\"] for r in successful_responses) / len(successful_responses)\n",
    "            avg_response_length = sum(r[\"response_length\"] for r in successful_responses) / len(successful_responses)\n",
    "            \n",
    "            print(f\"Analytics Summary:\")\n",
    "            print(f\"Success rate: {len(successful_responses)}/{len(results)} ({len(successful_responses)/len(results)*100:.1f}%)\")\n",
    "            print(f\"Average response time: {avg_response_time:.2f}s\")\n",
    "            print(f\"Average response length: {avg_response_length:.0f} characters\")\n",
    "        \n",
    "        return thread_id, results\n",
    "\n",
    "# Test the advanced analyzer\n",
    "analyzer = ConversationAnalyzer(user_id=\"rakshit\")\n",
    "\n",
    "advanced_questions = [\n",
    "    \"Explain the concept of transfer learning and its applications\",\n",
    "    \"What are the ethical considerations in AI development?\",\n",
    "    \"How do I implement model versioning and experiment tracking?\"\n",
    "]\n",
    "\n",
    "advanced_thread_id, advanced_results = analyzer.analyze_conversation_quality(\n",
    "    advanced_questions, \n",
    "    \"Advanced ML Ethics & Implementation\"\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal Thread ID for Advanced Analysis: {advanced_thread_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook was a game-changer for understanding how conversations work in AI systems! I learned that LangSmith's Thread feature is brilliant for tracking multi-turn conversations - it's like having a smart way to connect all the back-and-forth exchanges between users and the AI. The magic happens with simple metadata keys like `thread_id`, `session_id`, or `conversation_id` that link everything together.\n",
    "\n",
    "**What I Tweaked and Learned:**\n",
    "- Switched from OpenAI to Mistral AI but kept all the awesome conversation tracking features working smoothly\n",
    "- Updated the API keys (goodbye OpenAI, hello MISTRAL_API_KEY) and made sure the environment setup was solid\n",
    "- Built some really cool conversation simulations that feel like real chat sessions with proper timing delays between questions\n",
    "- Created a fancy ConversationAnalyzer class that tracks quality metrics, response times, and conversation complexity - it's like having analytics for your chatbot!\n",
    "- Added tons of useful metadata to each conversation turn, including user IDs, timestamps, question complexity, and conversation names\n",
    "- Experimented with different conversation types - technical discussions, practical implementation guides, and debugging sessions\n",
    "- Discovered how powerful it is to analyze conversation patterns, success rates, and response quality across multiple threads\n",
    "\n",
    "The coolest part was realizing how easy it is to track entire conversation flows in LangSmith. Whether someone's asking three quick questions or having a deep technical discussion, everything gets organized automatically. Plus, all the custom experiments showed me how you can build really sophisticated conversation analytics - tracking everything from response times to conversation quality metrics. It's perfect for understanding how well your AI assistant is actually performing in real conversations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mat496",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
